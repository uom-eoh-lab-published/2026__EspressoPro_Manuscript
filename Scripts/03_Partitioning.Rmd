---
title: "Train/Test/Calibration Split - Complete Pipeline"
author: "Analysis Pipeline"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    code_folding: show
    theme: flatly
    highlight: tango
    df_print: paged
params:
  seed: 123
  train_split: 0.80
  cal_split: 0.90
  min_cells: 50
  use_seurat_sketch: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = TRUE, 
  fig.width = 12, 
  fig.height = 6,
  cache = FALSE,
  dpi = 300
)
```

# 1. Setup and Configuration

```{r}
# Set reticulate to use sceasy environment
# 
```

```{r load-libraries, message=FALSE}
# Core packages
library(here)
library(tidyverse)
library(Seurat)
library(SeuratDisk)
library(sceasy)
library(ComplexHeatmap)
library(patchwork)
library(viridis)
library(scales)
library(SCpubr)
library(grid)
library(cowplot)

# Set working directory
knitr::opts_knit$set(root.dir = here::here())

# Load utility functions
source(here("Scripts/SingleCellUtils/Partitioning.R"))

# Set global seed
set.seed(params$seed)
options(timeout = 3000)

# Suppress warnings globally for cleaner output
options(warn = -1)
```

```{r config}
# Configuration for all atlases
config <- list(
  Zhang = list(
    input = "Data/References/Zhang/Zhang_adata_annotated.h5ad",
    assay = "ADT",
    reduction = "umap",
    merge_labels = c("HSC", "MPP")
  ),
  Hao = list(
    input = "Data/References/Hao/228AB_healthy_donors_PBMNCs_annotated.h5ad",
    assay = "ADT",
    reduction = "wnn.umap",
    merge_labels = c("HSC", "MPP")
  ),
  Triana = list(
    input = "Data/References/Triana/97AB_young_and_old_adult_healthy_donor_BMMNCs_annotated.h5ad",
    assay = "AB",
    reduction = "mofaumap",
    merge_labels = c("HSC", "MPP")
  ),
  Luecken = list(
    input = "Data/References/Luecken/140AB_adult_healthy_donor_BMMNCs_annotated.h5ad",
    assay = "ADT",
    reduction = "umap",
    merge_labels = c("HSC", "MPP")
  )
)

# Annotation depth columns
label_cols <- c(
  broad = "Consensus_annotation_broad_final",
  simplified = "Consensus_annotation_simplified_final",
  detailed = "Consensus_annotation_detailed_final"
)

# Custom color palette matching Python visualization
custom_palette <- c(
  'B Memory' = "#68D827", 'B Naive' = '#1C511D', 'CD14 Mono' = "#D27CE3",
  'CD16 Mono' = "#8D43CD", 'CD4 T Memory' = "#C1AF93", 'CD4 T Naive' = "#C99546",
  'CD8 T Memory' = "#6B3317", 'CD8 T Naive' = "#4D382E", 'ErP' = "#D1235A",
  'Erythroblast' = "#F30A1A", 'GMP' = "#C5E4FF", 'HSC_MPP' = '#0079ea',
  'Immature B' = "#91FF7B", 'LMPP' = "#17BECF", 'MAIT' = "#BCBD22",
  'Myeloid progenitor' = "#AEC7E8", 'NK CD56 bright' = "#F3AC1F",
  'NK CD56 dim' = "#FBEF0D", 'Plasma' = "#9DC012", 'Pro-B' = "#66BB6A", 'DnT' = '#504423',
  'Small' = "#292929", 'cDC1' = "#76A7CB", 'cDC2' = "#16D2E3", 'GdT' = "#EDB416",
  'pDC' = "#69FFCB", 'CD4 CTL' = "#D7D2CB", 'MEP' = "#E364B0", 'Pre-B' = "#2DBD67",
  'Pre-Pro-B' = '#92AC8E', 'EoBaMaP' = "#728245", 'MkP' = "#69424D", 'Mesenchymal' = '#BBBBBB',
  'Stroma' = "#727272", 'Macrophage' = "#5F4761", 'ILC' = "#F7CF94",
  'HSC' = '#0079ea', 'MPP' = '#0079ea', 'Platelet' = '#ec690d', 'Treg' = '#bd9e88'   # Merged labels
)

# Create output directories
dir.create(here("Data/Training_barcodes"), showWarnings = FALSE, recursive = TRUE)
dir.create(here("Data/Testing_barcodes"), showWarnings = FALSE, recursive = TRUE)
dir.create(here("Figures/Partitioning"), showWarnings = FALSE, recursive = TRUE)
```

```{r print-config}
cat("╔═══════════════════════════════════════════════════╗\n")
cat("║     ATLAS PARTITIONING PIPELINE - PARAMETERS      ║\n")
cat("╚═══════════════════════════════════════════════════╝\n\n")

cat("Analysis Parameters:\n")
cat(sprintf("  Random seed: %d\n", params$seed))
cat(sprintf("  Train split: %.1f%%\n", params$train_split * 100))
cat(sprintf("  Calibration split: %.1f%%\n", params$cal_split * 100))
cat(sprintf("  Minimum cells per class: %d\n\n", params$min_cells))

cat("Atlases to process:\n")
walk(names(config), ~cat(sprintf("  ✓ %s\n", .x)))

cat("\nBarcode generation strategy:\n")
cat("  • Training: Balanced (leverage-based downsampling)\n")
cat("  • Testing: Unbalanced (all cells)\n")
cat("  • Calibration: Subset of training data\n\n")

cat("Validation:\n")
cat("  • 0% overlap between Train/Cal/Test partitions\n")
cat("  • Expression correlation analysis\n")
cat("  • UMAP visualization\n\n")

cat("═══════════════════════════════════════════════════\n\n")
```

# 2. Zhang Atlas

```{r zhang-load}
cat("\n╔═══════════════════════════════════════════════════╗\n")
cat("║              PROCESSING ZHANG ATLAS               ║\n")
cat("╚═══════════════════════════════════════════════════╝\n\n")

zhang_config <- config$Zhang

# Load data
zhang_obj <- convertFormat(
  obj = here(zhang_config$input),
  from = "anndata",
  to = "seurat",
  assay = zhang_config$assay,
  main_layer = "counts"
)

cat(sprintf("✓ Loaded: %s cells, %s features\n", 
            comma(ncol(zhang_obj)), 
            comma(nrow(zhang_obj))))

# Preprocess
zhang_obj <- preprocess_object(
  zhang_obj,
  label_col = label_cols["detailed"],
  merge_labels = zhang_config$merge_labels,
  min_cells = params$min_cells
)

cat(sprintf("✓ After filtering: %s cells, %d classes\n", 
            comma(ncol(zhang_obj)), 
            length(unique(zhang_obj[[label_cols["detailed"]]][, 1]))))
```

```{r zhang-split}
# Perform two-stage split
zhang_splits <- two_stage_split(
  zhang_obj,
  label_col = label_cols["detailed"],
  assay = zhang_config$assay,
  train_prop = params$train_split,
  cal_prop = params$cal_split,
  seed = params$seed,
  use_seurat_sketch = params$use_seurat_sketch
)

cat(sprintf("\n✓ Split complete:\n"))
cat(sprintf("  Train: %s cells\n", comma(ncol(zhang_splits$train))))
cat(sprintf("  Cal:   %s cells\n", comma(ncol(zhang_splits$cal))))
cat(sprintf("  Test:  %s cells\n", comma(ncol(zhang_splits$test))))
```

## Partition Overlap Validation

```{r zhang-overlap}
overlap_stats <- validate_partition_overlap(zhang_splits)
knitr::kable(overlap_stats, 
             caption = "Zhang: Partition Overlap Validation",
             digits = 2,
             align = 'c')

if (all(overlap_stats$Overlap_N == 0)) {
  cat("\n✓ No overlap detected between partitions\n")
}
```

## Train/Test Validation

```{r zhang-train-test-umap, fig.width=12, fig.height=6}
p1 <- SCpubr::do_DimPlot(
  sample = zhang_splits$train,
  group.by = label_cols["detailed"],
  reduction = zhang_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Training Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

p2 <- SCpubr::do_DimPlot(
  sample = zhang_splits$test,
  group.by = label_cols["detailed"],
  reduction = zhang_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Test Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

combined_plot <- p1 | p2
print(combined_plot)

ggsave(here("Figures/Partitioning/Zhang_train_test_umap.png"),
       combined_plot, width = 12, height = 6, dpi = 300, bg = "white")
```

```{r zhang-train-test-cor, fig.width=11, fig.height=11}
png(here("Figures/Partitioning/Zhang_train_test_correlation.png"), 
    width = 11, height = 11, units = "in", res = 300, bg = "white")

ht <- validate_split_correlation(
  zhang_splits$train, 
  zhang_splits$test,
  label_cols["detailed"],
  zhang_config$assay,
  title = "Zhang: Train vs Test Expression Correlation"
)

draw(ht, heatmap_legend_side = "bottom")

dev.off()

draw(ht, heatmap_legend_side = "bottom")
```

## Train/Calibration Validation

```{r zhang-train-cal-umap, fig.width=12, fig.height=6}
p1 <- SCpubr::do_DimPlot(
  sample = zhang_splits$train,
  group.by = label_cols["detailed"],
  reduction = zhang_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Training Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

p2 <- SCpubr::do_DimPlot(
  sample = zhang_splits$cal,
  group.by = label_cols["detailed"],
  reduction = zhang_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Calibration Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

combined_plot <- p1 | p2
print(combined_plot)

ggsave(here("Figures/Partitioning/Zhang_train_cal_umap.png"),
       combined_plot, width = 12, height = 6, dpi = 300, bg = "white")
```

```{r zhang-train-cal-cor, fig.width=11, fig.height=11}
png(here("Figures/Partitioning/Zhang_train_cal_correlation.png"), 
    width = 11, height = 11, units = "in", res = 300, bg = "white")

ht <- validate_split_correlation(
  zhang_splits$train, 
  zhang_splits$cal,
  label_cols["detailed"],
  zhang_config$assay,
  title = "Zhang: Train vs Calibration Expression Correlation"
)

draw(ht, heatmap_legend_side = "bottom")

dev.off()

draw(ht, heatmap_legend_side = "bottom")
```

## Save Splits and Generate Barcodes

```{r zhang-save}
# Save h5ad files
save_splits_h5ad(
  zhang_splits,
  "Zhang",
  zhang_config$input,
  here("Data/References/Zhang"),
  zhang_config$assay
)

cat("\n✓ Saved h5ad split files\n")

# Generate and save OVR barcodes for all annotation depths
for (depth_name in names(label_cols)) {
  label_col <- label_cols[depth_name]
  
  cat(sprintf("\n--- Generating barcodes for %s level ---\n", depth_name))
  
  # Training barcodes (balanced)
  cat("\nTraining set (balanced):\n")
  train_barcodes <- generate_ovr_barcodes(
    zhang_splits$train,
    label_col,
    zhang_config$assay,
    is_test = FALSE,
    seed = params$seed,
    use_seurat_sketch = params$use_seurat_sketch
  )
  
  save_ovr_barcodes(
    train_barcodes,
    here("Data/Training_barcodes/Zhang", label_col),
    prefix = "training"
  )
  
  # Test barcodes (unbalanced - use all data)
  cat("\nTest set (unbalanced):\n")
  test_barcodes <- generate_ovr_barcodes(
    zhang_splits$test,
    label_col,
    zhang_config$assay,
    is_test = TRUE,
    seed = params$seed,
    use_seurat_sketch = params$use_seurat_sketch
  )
  
  save_ovr_barcodes(
    test_barcodes,
    here("Data/Testing_barcodes/Zhang", label_col),
    prefix = "testing"
  )
}

cat("\n✓✓✓ Zhang atlas processing complete ✓✓✓\n")
```

# 3. Hao Atlas

```{r hao-load}
cat("\n╔═══════════════════════════════════════════════════╗\n")
cat("║               PROCESSING HAO ATLAS                ║\n")
cat("╚═══════════════════════════════════════════════════╝\n\n")

hao_config <- config$Hao

hao_obj <- convertFormat(
  obj = here(hao_config$input),
  from = "anndata",
  to = "seurat",
  assay = hao_config$assay,
  main_layer = "counts"
)

cat(sprintf("✓ Loaded: %s cells, %s features\n", 
            comma(ncol(hao_obj)), 
            comma(nrow(hao_obj))))

hao_obj <- preprocess_object(
  hao_obj,
  label_col = label_cols["detailed"],
  merge_labels = hao_config$merge_labels,
  min_cells = params$min_cells
)

cat(sprintf("✓ After filtering: %s cells, %d classes\n", 
            comma(ncol(hao_obj)), 
            length(unique(hao_obj[[label_cols["detailed"]]][, 1]))))
```

```{r hao-split}
hao_splits <- two_stage_split(
  hao_obj,
  label_col = label_cols["detailed"],
  assay = hao_config$assay,
  train_prop = params$train_split,
  cal_prop = params$cal_split,
  seed = params$seed,
  use_seurat_sketch = params$use_seurat_sketch
)

cat(sprintf("\n✓ Split complete:\n"))
cat(sprintf("  Train: %s cells\n", comma(ncol(hao_splits$train))))
cat(sprintf("  Cal:   %s cells\n", comma(ncol(hao_splits$cal))))
cat(sprintf("  Test:  %s cells\n", comma(ncol(hao_splits$test))))
```

## Partition Overlap Validation

```{r hao-overlap}
overlap_stats <- validate_partition_overlap(hao_splits)
knitr::kable(overlap_stats, 
             caption = "Hao: Partition Overlap Validation",
             digits = 2,
             align = 'c')

if (all(overlap_stats$Overlap_N == 0)) {
  cat("\n✓ No overlap detected between partitions\n")
}
```

## Train/Test Validation

```{r hao-train-test-umap, fig.width=12, fig.height=6}
p1 <- SCpubr::do_DimPlot(
  sample = hao_splits$train,
  group.by = label_cols["detailed"],
  reduction = hao_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Training Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

p2 <- SCpubr::do_DimPlot(
  sample = hao_splits$test,
  group.by = label_cols["detailed"],
  reduction = hao_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Test Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

combined_plot <- p1 | p2
print(combined_plot)

ggsave(here("Figures/Partitioning/Hao_train_test_umap.png"),
       combined_plot, width = 12, height = 6, dpi = 300, bg = "white")
```

```{r hao-train-test-cor, fig.width=11, fig.height=11}
png(here("Figures/Partitioning/Hao_train_test_correlation.png"), 
    width = 11, height = 11, units = "in", res = 300, bg = "white")

ht <- validate_split_correlation(
  hao_splits$train, 
  hao_splits$test,
  label_cols["detailed"],
  hao_config$assay,
  title = "Hao: Train vs Test Expression Correlation"
)

draw(ht, heatmap_legend_side = "bottom")

dev.off()

draw(ht, heatmap_legend_side = "bottom")
```

## Train/Calibration Validation

```{r hao-train-cal-umap, fig.width=12, fig.height=6}
p1 <- SCpubr::do_DimPlot(
  sample = hao_splits$train,
  group.by = label_cols["detailed"],
  reduction = hao_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Training Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

p2 <- SCpubr::do_DimPlot(
  sample = hao_splits$cal,
  group.by = label_cols["detailed"],
  reduction = hao_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Calibration Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

combined_plot <- p1 | p2
print(combined_plot)

ggsave(here("Figures/Partitioning/Hao_train_cal_umap.png"),
       combined_plot, width = 12, height = 6, dpi = 300, bg = "white")
```

```{r hao-train-cal-cor, fig.width=11, fig.height=11}
png(here("Figures/Partitioning/Hao_train_cal_correlation.png"), 
    width = 11, height = 11, units = "in", res = 300, bg = "white")

ht <- validate_split_correlation(
  hao_splits$train, 
  hao_splits$cal,
  label_cols["detailed"],
  hao_config$assay,
  title = "Hao: Train vs Calibration Expression Correlation"
)

draw(ht, heatmap_legend_side = "bottom")

dev.off()

draw(ht, heatmap_legend_side = "bottom")
```

## Save Splits and Generate Barcodes

```{r hao-save}
save_splits_h5ad(
  hao_splits,
  "Hao",
  hao_config$input,
  here("Data/References/Hao"),
  hao_config$assay
)

cat("\n✓ Saved h5ad split files\n")

for (depth_name in names(label_cols)) {
  label_col <- label_cols[depth_name]
  
  cat(sprintf("\n--- Generating barcodes for %s level ---\n", depth_name))
  
  cat("\nTraining set (balanced):\n")
  train_barcodes <- generate_ovr_barcodes(
    hao_splits$train, label_col, hao_config$assay,
    is_test = FALSE, seed = params$seed,
    use_seurat_sketch = params$use_seurat_sketch
  )
  save_ovr_barcodes(
    train_barcodes,
    here("Data/Training_barcodes/Hao", label_col),
    prefix = "training"
  )
  
  cat("\nTest set (unbalanced):\n")
  test_barcodes <- generate_ovr_barcodes(
    hao_splits$test, label_col, hao_config$assay,
    is_test = TRUE, seed = params$seed,
    use_seurat_sketch = params$use_seurat_sketch
  )
  save_ovr_barcodes(
    test_barcodes,
    here("Data/Testing_barcodes/Hao", label_col),
    prefix = "testing"
  )
}

cat("\n✓✓✓ Hao atlas processing complete ✓✓✓\n")
```

# 4. Triana Atlas

```{r triana-load}
cat("\n╔═══════════════════════════════════════════════════╗\n")
cat("║             PROCESSING TRIANA ATLAS               ║\n")
cat("╚═══════════════════════════════════════════════════╝\n\n")

triana_config <- config$Triana

triana_obj <- convertFormat(
  obj = here(triana_config$input),
  from = "anndata",
  to = "seurat",
  assay = triana_config$assay,
  main_layer = "counts"
)

cat(sprintf("✓ Loaded: %s cells, %s features\n", 
            comma(ncol(triana_obj)), 
            comma(nrow(triana_obj))))

triana_obj <- preprocess_object(
  triana_obj,
  label_col = label_cols["detailed"],
  merge_labels = triana_config$merge_labels,
  min_cells = params$min_cells
)

cat(sprintf("✓ After filtering: %s cells, %d classes\n", 
            comma(ncol(triana_obj)), 
            length(unique(triana_obj[[label_cols["detailed"]]][, 1]))))
```

```{r triana-split}
triana_splits <- two_stage_split(
  triana_obj,
  label_col = label_cols["detailed"],
  assay = triana_config$assay,
  train_prop = params$train_split,
  cal_prop = params$cal_split,
  seed = params$seed,
  use_seurat_sketch = params$use_seurat_sketch
)

cat(sprintf("\n✓ Split complete:\n"))
cat(sprintf("  Train: %s cells\n", comma(ncol(triana_splits$train))))
cat(sprintf("  Cal:   %s cells\n", comma(ncol(triana_splits$cal))))
cat(sprintf("  Test:  %s cells\n", comma(ncol(triana_splits$test))))
```

## Partition Overlap Validation

```{r triana-overlap}
overlap_stats <- validate_partition_overlap(triana_splits)
knitr::kable(overlap_stats, 
             caption = "Triana: Partition Overlap Validation",
             digits = 2,
             align = 'c')

if (all(overlap_stats$Overlap_N == 0)) {
  cat("\n✓ No overlap detected between partitions\n")
}
```

## Train/Test Validation

```{r triana-train-test-umap, fig.width=12, fig.height=6}
p1 <- SCpubr::do_DimPlot(
  sample = triana_splits$train,
  group.by = label_cols["detailed"],
  reduction = triana_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Training Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

p2 <- SCpubr::do_DimPlot(
  sample = triana_splits$test,
  group.by = label_cols["detailed"],
  reduction = triana_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Test Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

combined_plot <- p1 | p2
print(combined_plot)

ggsave(here("Figures/Partitioning/Triana_train_test_umap.png"),
       combined_plot, width = 12, height = 6, dpi = 300, bg = "white")
```

```{r triana-train-test-cor, fig.width=11, fig.height=11}
png(here("Figures/Partitioning/Triana_train_test_correlation.png"), 
    width = 11, height = 11, units = "in", res = 300, bg = "white")

ht <- validate_split_correlation(
  triana_splits$train, 
  triana_splits$test,
  label_cols["detailed"],
  triana_config$assay,
  title = "Triana: Train vs Test Expression Correlation"
)

draw(ht, heatmap_legend_side = "bottom")

dev.off()

draw(ht, heatmap_legend_side = "bottom")
```

## Train/Calibration Validation

```{r triana-train-cal-umap, fig.width=12, fig.height=6}
p1 <- SCpubr::do_DimPlot(
  sample = triana_splits$train,
  group.by = label_cols["detailed"],
  reduction = triana_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Training Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

p2 <- SCpubr::do_DimPlot(
  sample = triana_splits$cal,
  group.by = label_cols["detailed"],
  reduction = triana_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Calibration Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

combined_plot <- p1 | p2
print(combined_plot)

ggsave(here("Figures/Partitioning/Triana_train_cal_umap.png"),
       combined_plot, width = 12, height = 6, dpi = 300, bg = "white")
```

```{r triana-train-cal-cor, fig.width=11, fig.height=11}
png(here("Figures/Partitioning/Triana_train_cal_correlation.png"), 
    width = 11, height = 11, units = "in", res = 300, bg = "white")

ht <- validate_split_correlation(
  triana_splits$train, 
  triana_splits$cal,
  label_cols["detailed"],
  triana_config$assay,
  title = "Triana: Train vs Calibration Expression Correlation"
)

draw(ht, heatmap_legend_side = "bottom")

dev.off()

draw(ht, heatmap_legend_side = "bottom")
```

## Save Splits and Generate Barcodes

```{r triana-save}
save_splits_h5ad(
  triana_splits,
  "Triana",
  triana_config$input,
  here("Data/References/Triana"),
  triana_config$assay
)

cat("\n✓ Saved h5ad split files\n")

for (depth_name in names(label_cols)) {
  label_col <- label_cols[depth_name]
  
  cat(sprintf("\n--- Generating barcodes for %s level ---\n", depth_name))
  
  cat("\nTraining set (balanced):\n")
  train_barcodes <- generate_ovr_barcodes(
    triana_splits$train, label_col, triana_config$assay,
    is_test = FALSE, seed = params$seed,
    use_seurat_sketch = params$use_seurat_sketch
  )
  save_ovr_barcodes(
    train_barcodes,
    here("Data/Training_barcodes/Triana", label_col),
    prefix = "training"
  )
  
  cat("\nTest set (unbalanced):\n")
  test_barcodes <- generate_ovr_barcodes(
    triana_splits$test, label_col, triana_config$assay,
    is_test = TRUE, seed = params$seed,
    use_seurat_sketch = params$use_seurat_sketch
  )
  save_ovr_barcodes(
    test_barcodes,
    here("Data/Testing_barcodes/Triana", label_col),
    prefix = "testing"
  )
}

cat("\n✓✓✓ Triana atlas processing complete ✓✓✓\n")
```

# 5. Luecken Atlas

```{r luecken-load}
cat("\n╔═══════════════════════════════════════════════════╗\n")
cat("║            PROCESSING LUECKEN ATLAS               ║\n")
cat("╚═══════════════════════════════════════════════════╝\n\n")

luecken_config <- config$Luecken

luecken_obj <- convertFormat(
  obj = here(luecken_config$input),
  from = "anndata",
  to = "seurat",
  assay = luecken_config$assay,
  main_layer = "counts"
)

cat(sprintf("✓ Loaded: %s cells, %s features\n", 
            comma(ncol(luecken_obj)), 
            comma(nrow(luecken_obj))))

luecken_obj <- preprocess_object(
  luecken_obj,
  label_col = label_cols["detailed"],
  merge_labels = luecken_config$merge_labels,
  min_cells = params$min_cells
)

cat(sprintf("✓ After filtering: %s cells, %d classes\n", 
            comma(ncol(luecken_obj)), 
            length(unique(luecken_obj[[label_cols["detailed"]]][, 1]))))
```

```{r luecken-split}
luecken_splits <- two_stage_split(
  luecken_obj,
  label_col = label_cols["detailed"],
  assay = luecken_config$assay,
  train_prop = params$train_split,
  cal_prop = params$cal_split,
  seed = params$seed,
  use_seurat_sketch = params$use_seurat_sketch
)

cat(sprintf("\n✓ Split complete:\n"))
cat(sprintf("  Train: %s cells\n", comma(ncol(luecken_splits$train))))
cat(sprintf("  Cal:   %s cells\n", comma(ncol(luecken_splits$cal))))
cat(sprintf("  Test:  %s cells\n", comma(ncol(luecken_splits$test))))
```

## Partition Overlap Validation

```{r luecken-overlap}
overlap_stats <- validate_partition_overlap(luecken_splits)
knitr::kable(overlap_stats, 
             caption = "Luecken: Partition Overlap Validation",
             digits = 2,
             align = 'c')

if (all(overlap_stats$Overlap_N == 0)) {
  cat("\n✓ No overlap detected between partitions\n")
}
```

## Train/Test Validation

```{r luecken-train-test-umap, fig.width=12, fig.height=6}
p1 <- SCpubr::do_DimPlot(
  sample = luecken_splits$train,
  group.by = label_cols["detailed"],
  reduction = luecken_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Training Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

p2 <- SCpubr::do_DimPlot(
  sample = luecken_splits$test,
  group.by = label_cols["detailed"],
  reduction = luecken_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Test Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

combined_plot <- p1 | p2
print(combined_plot)

ggsave(here("Figures/Partitioning/Luecken_train_test_umap.png"),
       combined_plot, width = 12, height = 6, dpi = 300, bg = "white")
```

```{r luecken-train-test-cor, fig.width=11, fig.height=11}
png(here("Figures/Partitioning/Luecken_train_test_correlation.png"), 
    width = 11, height = 11, units = "in", res = 300, bg = "white")

ht <- validate_split_correlation(
  luecken_splits$train, 
  luecken_splits$test,
  label_cols["detailed"],
  luecken_config$assay,
  title = "Luecken: Train vs Test Expression Correlation"
)

draw(ht, heatmap_legend_side = "bottom")

dev.off()

draw(ht, heatmap_legend_side = "bottom")
```

## Train/Calibration Validation

```{r luecken-train-cal-umap, fig.width=12, fig.height=6}
p1 <- SCpubr::do_DimPlot(
  sample = luecken_splits$train,
  group.by = label_cols["detailed"],
  reduction = luecken_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Training Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

p2 <- SCpubr::do_DimPlot(
  sample = luecken_splits$cal,
  group.by = label_cols["detailed"],
  reduction = luecken_config$reduction,
  colors.use = custom_palette,
  pt.size = 0.5,
  legend.position = "none",
  plot.title = "Calibration Set",
  border.size = 5,
  border.density = 1,
  font.size = 14
)

combined_plot <- p1 | p2
print(combined_plot)

ggsave(here("Figures/Partitioning/Luecken_train_cal_umap.png"),
       combined_plot, width = 12, height = 6, dpi = 300, bg = "white")
```

```{r luecken-train-cal-cor, fig.width=11, fig.height=11}
png(here("Figures/Partitioning/Luecken_train_cal_correlation.png"), 
    width = 11, height = 11, units = "in", res = 300, bg = "white")

ht <- validate_split_correlation(
  luecken_splits$train, 
  luecken_splits$cal,
  label_cols["detailed"],
  luecken_config$assay,
  title = "Luecken: Train vs Calibration Expression Correlation"
)

draw(ht, heatmap_legend_side = "bottom")

dev.off()

draw(ht, heatmap_legend_side = "bottom")
```

## Save Splits and Generate Barcodes

```{r luecken-save}
save_splits_h5ad(
  luecken_splits,
  "Luecken",
  luecken_config$input,
  here("Data/References/Luecken"),
  luecken_config$assay
)

cat("\n✓ Saved h5ad split files\n")

for (depth_name in names(label_cols)) {
  label_col <- label_cols[depth_name]
  
  cat(sprintf("\n--- Generating barcodes for %s level ---\n", depth_name))
  
  cat("\nTraining set (balanced):\n")
  train_barcodes <- generate_ovr_barcodes(
    luecken_splits$train, label_col, luecken_config$assay,
    is_test = FALSE, seed = params$seed,
    use_seurat_sketch = params$use_seurat_sketch
  )
  save_ovr_barcodes(
    train_barcodes,
    here("Data/Training_barcodes/Luecken", label_col),
    prefix = "training"
  )
  
  cat("\nTest set (unbalanced):\n")
  test_barcodes <- generate_ovr_barcodes(
    luecken_splits$test, label_col, luecken_config$assay,
    is_test = TRUE, seed = params$seed,
    use_seurat_sketch = params$use_seurat_sketch
  )
  save_ovr_barcodes(
    test_barcodes,
    here("Data/Testing_barcodes/Luecken", label_col),
    prefix = "testing"
  )
}

cat("\n✓✓✓ Luecken atlas processing complete ✓✓✓\n")
```

---

# 6. Summary and Final Validation

## Dataset Split Summary

```{r summary-table}
summary_df <- tibble(
  Atlas = names(config),
  Train_Cells = c(ncol(zhang_splits$train), ncol(hao_splits$train), 
                  ncol(triana_splits$train), ncol(luecken_splits$train)),
  Cal_Cells = c(ncol(zhang_splits$cal), ncol(hao_splits$cal),
                ncol(triana_splits$cal), ncol(luecken_splits$cal)),
  Test_Cells = c(ncol(zhang_splits$test), ncol(hao_splits$test),
                 ncol(triana_splits$test), ncol(luecken_splits$test)),
  Train_Classes = c(
    length(unique(zhang_splits$train[[label_cols["detailed"]]][, 1])),
    length(unique(hao_splits$train[[label_cols["detailed"]]][, 1])),
    length(unique(triana_splits$train[[label_cols["detailed"]]][, 1])),
    length(unique(luecken_splits$train[[label_cols["detailed"]]][, 1]))
  ),
  Cal_Classes = c(
    length(unique(zhang_splits$cal[[label_cols["detailed"]]][, 1])),
    length(unique(hao_splits$cal[[label_cols["detailed"]]][, 1])),
    length(unique(triana_splits$cal[[label_cols["detailed"]]][, 1])),
    length(unique(luecken_splits$cal[[label_cols["detailed"]]][, 1]))
  ),
  Test_Classes = c(
    length(unique(zhang_splits$test[[label_cols["detailed"]]][, 1])),
    length(unique(hao_splits$test[[label_cols["detailed"]]][, 1])),
    length(unique(triana_splits$test[[label_cols["detailed"]]][, 1])),
    length(unique(luecken_splits$test[[label_cols["detailed"]]][, 1]))
  )
) %>%
  mutate(
    Total_Cells = Train_Cells + Cal_Cells + Test_Cells,
    Train_Pct = sprintf("%.1f%%", Train_Cells / Total_Cells * 100),
    Cal_Pct = sprintf("%.1f%%", Cal_Cells / Total_Cells * 100),
    Test_Pct = sprintf("%.1f%%", Test_Cells / Total_Cells * 100)
  )

knitr::kable(summary_df, 
             caption = "Dataset Split Summary (Detailed Annotation Level)",
             format.args = list(big.mark = ","),
             align = 'c')
```

## Partition Size Distribution

```{r partition-viz, fig.width=12, fig.height=8}
# Prepare data for visualization
partition_data <- summary_df %>%
  select(Atlas, Train_Cells, Cal_Cells, Test_Cells) %>%
  pivot_longer(cols = -Atlas, names_to = "Partition", values_to = "Cells") %>%
  mutate(
    Partition = str_remove(Partition, "_Cells"),
    Partition = factor(Partition, levels = c("Train", "Cal", "Test"))
  )

# Stacked bar chart
p1 <- ggplot(partition_data, aes(x = Atlas, y = Cells, fill = Partition)) +
  geom_bar(stat = "identity", color = "black", size = 0.3) +
  scale_fill_manual(values = c("Train" = "#4CAF50", "Cal" = "#FFC107", "Test" = "#2196F3")) +
  labs(title = "Partition Size Distribution",
       y = "Number of Cells",
       x = NULL) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 16),
    legend.position = "top",
    panel.grid.minor = element_blank()
  ) +
  scale_y_continuous(labels = comma)

# Proportional bar chart
p2 <- ggplot(partition_data, aes(x = Atlas, y = Cells, fill = Partition)) +
  geom_bar(stat = "identity", position = "fill", color = "black", size = 0.3) +
  scale_fill_manual(values = c("Train" = "#4CAF50", "Cal" = "#FFC107", "Test" = "#2196F3")) +
  labs(title = "Partition Proportions",
       y = "Proportion",
       x = NULL) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 16),
    legend.position = "top",
    panel.grid.minor = element_blank()
  ) +
  scale_y_continuous(labels = percent)

combined <- p1 / p2
print(combined)

ggsave(here("Figures/Partitioning/Partition_distribution_summary.png"),
       combined, width = 12, height = 8, dpi = 300, bg = "white")
```

## All Partition Overlaps

```{r all-overlaps}
all_overlaps <- bind_rows(
  validate_partition_overlap(zhang_splits) %>% mutate(Atlas = "Zhang"),
  validate_partition_overlap(hao_splits) %>% mutate(Atlas = "Hao"),
  validate_partition_overlap(triana_splits) %>% mutate(Atlas = "Triana"),
  validate_partition_overlap(luecken_splits) %>% mutate(Atlas = "Luecken")
) %>% 
  select(Atlas, everything())

knitr::kable(all_overlaps,
             caption = "Partition Overlap Summary (All Atlases)",
             digits = 2,
             align = 'c')

# Final validation
if (all(all_overlaps$Overlap_N == 0)) {
  cat("\n╔═══════════════════════════════════════════════════╗\n")
  cat("║                                                   ║\n")
  cat("║   ✓✓✓ SUCCESS: ALL PARTITIONS VALIDATED ✓✓✓     ║\n")
  cat("║                                                   ║\n")
  cat("║   • 0% overlap across all atlases                ║\n")
  cat("║   • Train/Test/Cal splits completed              ║\n")
  cat("║   • OVR barcodes generated                       ║\n")
  cat("║   • All files saved successfully                 ║\n")
  cat("║                                                   ║\n")
  cat("╚═══════════════════════════════════════════════════╝\n")
} else {
  cat("\n⚠️  WARNING: OVERLAP DETECTED IN PARTITIONS ⚠️\n")
  cat("Please review the overlap statistics above.\n")
}
```

## File Output Summary

```{r output-summary}
cat("\n╔═══════════════════════════════════════════════════╗\n")
cat("║              OUTPUT FILES GENERATED               ║\n")
cat("╚═══════════════════════════════════════════════════╝\n\n")

cat("H5AD Split Files:\n")
for (atlas in names(config)) {
  cat(sprintf("  %s/\n", atlas))
  cat(sprintf("    ├─ %s_train.h5ad\n", atlas))
  cat(sprintf("    ├─ %s_cal.h5ad\n", atlas))
  cat(sprintf("    └─ %s_test.h5ad\n\n", atlas))
}

cat("OVR Barcode Files (per annotation level):\n")
for (atlas in names(config)) {
  cat(sprintf("  %s/\n", atlas))
  for (depth in names(label_cols)) {
    cat(sprintf("    └─ %s/\n", depth))
    cat("       ├─ training_*.csv (balanced)\n")
    cat("       └─ testing_*.csv (unbalanced)\n")
  }
  cat("\n")
}

cat("Visualization Files:\n")
cat("  Figures/Partitioning/\n")
cat("    ├─ *_train_test_umap.png (no legend)\n")
cat("    ├─ *_train_cal_umap.png (no legend)\n")
cat("    ├─ *_train_test_correlation.png (bottom legend)\n")
cat("    ├─ *_train_cal_correlation.png (bottom legend)\n")
cat("    └─ Partition_distribution_summary.png\n\n")

cat("═══════════════════════════════════════════════════\n")
```

# Session Information

```{r session-info}
# Get current script name (works in RStudio)
script_name <- tryCatch(
  {
    tools::file_path_sans_ext(basename(rstudioapi::getActiveDocumentContext()$path))
  },
  error = function(e) "interactive_session"
)

out_file <- here::here(paste0("Scripts/", script_name, "_sessionInfo.txt"))

sink(out_file)
cat("\n╔═══════════════════════════════════════════════════╗\n")
cat("║              SESSION INFORMATION                  ║\n")
cat("╚═══════════════════════════════════════════════════╝\n\n")
sessionInfo()
sink()

message("Session info saved to: ", out_file)
```

**Analysis Complete:** `r Sys.time()`

**Computational Environment:**
- R version: `r R.version.string`
- Platform: `r R.version$platform`
- Running under: `r R.version$os`