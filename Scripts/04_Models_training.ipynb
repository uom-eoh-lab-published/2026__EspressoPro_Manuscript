{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "script_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "parent_dir = os.path.dirname(script_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import warnings\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc_context\n",
    "\n",
    "# Single-cell analysis\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import harmonypy as hm\n",
    "import espressopro as ep\n",
    "\n",
    "# Machine learning - sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Additional ML utilities\n",
    "from netcal.scaling import TemperatureScaling\n",
    "from scipy.stats import skew, kurtosis, normaltest, shapiro\n",
    "import joblib\n",
    "\n",
    "# Configure multiprocessing\n",
    "num_cores = multiprocessing.cpu_count() - 2\n",
    "print(f\"Total CPU cores to be used: {num_cores}\")\n",
    "\n",
    "# Configure cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading custom scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(parent_dir + '/Scripts/SingleCellUtils')\n",
    "\n",
    "import SCUtils\n",
    "\n",
    "import sys\n",
    "sys.path.append(parent_dir + '/Scripts/ModelTraining')\n",
    "\n",
    "import MLTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def assign_labels(dataset, reduction, n_neighbors, label_input, label_output, frequency_threshold):\n",
    "    \"\"\"Propagate consensus labels within high-resolution clusters.\"\"\"\n",
    "    sc.pp.neighbors(dataset, use_rep=reduction, n_neighbors=n_neighbors)\n",
    "    sc.tl.leiden(dataset, key_added='clusters', resolution=10)\n",
    "    dataset.obs[label_output] = dataset.obs[label_input]\n",
    "    \n",
    "    for cluster in dataset.obs['clusters'].unique():\n",
    "        cluster_labels = dataset.obs.loc[dataset.obs['clusters'] == cluster, label_input]\n",
    "        most_frequent_label = cluster_labels.mode()[0]\n",
    "        frequency = (cluster_labels == most_frequent_label).mean()\n",
    "        \n",
    "        if frequency > frequency_threshold:\n",
    "            dataset.obs.loc[dataset.obs['clusters'] == cluster, label_output] = most_frequent_label\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"Normalize feature names: lowercase, replace spaces/underscores with hyphens.\"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    return (s.str.strip()\n",
    "             .str.lower()\n",
    "             .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "             .str.replace(r\"-+\", \"-\", regex=True)\n",
    "             .str.strip(\"-\"))\n",
    "\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    \"\"\"Add 'Celltype' column from AnnData.obs[field], reindexed to match df.\"\"\"\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    \n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    \n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    \n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        print(f\"[WARN] {out['Celltype'].isna().sum()} rows got NaN Celltype\")\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    \"\"\"Raise error if DataFrame contains non-finite values.\"\"\"\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values in {tag} at {bad}\")\n",
    "\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    \"\"\"Extract base estimator from sklearn wrappers (e.g., CalibratedClassifierCV).\"\"\"\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    \"\"\"Verify all models saw the expected number of features.\"\"\"\n",
    "    for name, est in [(\"NB\", models_dict.get(\"NB\")), (\"XGB\", models_dict.get(\"XGB\")),\n",
    "                       (\"KNN\", models_dict.get(\"KNN\")), (\"MLP\", models_dict.get(\"MLP\")),\n",
    "                       (\"Stacker\", models_dict.get(\"Stacker\"))]:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTHONHASHSEED was set as envinronmental variable to 0 as follows:\n",
    "    \n",
    "    conda env config vars set PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_pythonhashseed(seed=0):\n",
    "    current_seed = os.environ.get(\"PYTHONHASHSEED\")\n",
    "\n",
    "    seed = str(seed)\n",
    "    if current_seed is None or current_seed != seed:\n",
    "        print(f'Setting PYTHONHASHSEED=\"{seed}\"')\n",
    "        os.environ[\"PYTHONHASHSEED\"] = seed\n",
    "        # restart the current process\n",
    "        os.execl(sys.executable, sys.executable, *sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "hash = random.getrandbits(128)\n",
    "\n",
    "print(\"hash value: %032x\" % hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path\n",
    "data_path = parent_dir + \"/Data\"\n",
    "figures_path = parent_dir + \"/Figures/Model_Training\"\n",
    "\n",
    "if not os.path.exists(figures_path):\n",
    "    os.makedirs(figures_path)\n",
    "\n",
    "# Create the folder\n",
    "os.makedirs(data_path + \"/Pre_trained_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_barcodes_path = data_path + \"/Training_barcodes\"\n",
    "train_barcodes_path\n",
    "test_barcodes_path = data_path + \"/Testing_barcodes\"\n",
    "test_barcodes_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Hao Y. et al. (2021) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_dataset_Train = sc.read_h5ad(data_path + \"/References/Hao\" + \"/228AB_healthy_donors_PBMNCs_annotated_Train.h5ad\")\n",
    "Hao_dataset_Test = sc.read_h5ad(data_path + \"/References/Hao\" + \"/228AB_healthy_donors_PBMNCs_annotated_Test.h5ad\")\n",
    "Hao_dataset_Cal = sc.read_h5ad(data_path + \"/References/Hao\" + \"/228AB_healthy_donors_PBMNCs_annotated_Cal.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_dataset_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_dataset_Cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "label_key = 'Consensus_annotation_detailed_final'\n",
    "basis_key = 'X_wnn.umap'\n",
    "color_key = f'{label_key}_colors'\n",
    "\n",
    "# Your custom palette (label -> hex)\n",
    "custom_palette = {\n",
    "    'B Memory': \"#68D827\",\n",
    "    'B Naive': '#1C511D',\n",
    "    'CD14 Mono': \"#D27CE3\",\n",
    "    'CD16 Mono': \"#8D43CD\",\n",
    "    'CD4 T Memory': \"#C1AF93\",\n",
    "    'CD4 T Naive': \"#C99546\",\n",
    "    'CD8 T Memory': \"#6B3317\",\n",
    "    'CD8 T Naive': \"#4D382E\",\n",
    "    'ErP': \"#D1235A\",\n",
    "    'Erythroblast': \"#F30A1A\",\n",
    "    'GMP': \"#C5E4FF\",\n",
    "    'HSC_MPP': '#0079ea',\n",
    "    'Immature B': \"#91FF7B\",\n",
    "    'LMPP': \"#17BECF\",\n",
    "    'MAIT': \"#BCBD22\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'NK CD56 bright': \"#F3AC1F\",\n",
    "    'NK CD56 dim': \"#FBEF0D\",\n",
    "    'Plasma': \"#9DC012\",\n",
    "    'Pro-B': \"#66BB6A\",\n",
    "    'Small': \"#292929\",\n",
    "    'cDC1': \"#76A7CB\",\n",
    "    'cDC2': \"#16D2E3\",\n",
    "    'GdT': \"#EDB416\",\n",
    "    'Mesenchymal': '#BBBBBB',\n",
    "    'pDC': \"#69FFCB\",\n",
    "    'CD4 CTL': \"#D7D2CB\",\n",
    "    'MEP': \"#E364B0\",\n",
    "    'Pre-B': \"#2DBD67\",\n",
    "    'Pre-Pro-B': '#92AC8E',\n",
    "    'EoBaMaP': \"#728245\",\n",
    "    'MkP': \"#69424D\",\n",
    "    'Stroma': \"#727272\",\n",
    "    'Macrophage': \"#5F4761\",\n",
    "    'ILC': \"#F7CF94\",\n",
    "    'DnT': \"#504423\",\n",
    "    'Treg': \"#6E6C37\",\n",
    "    'Platelet': \"#FF39A6\",\n",
    "}\n",
    "\n",
    "# --- Ensure categorical dtype ---\n",
    "if not pd.api.types.is_categorical_dtype(Hao_dataset_Train.obs[label_key]):\n",
    "    Hao_dataset_Train.obs[label_key] = Hao_dataset_Train.obs[label_key].astype('category')\n",
    "\n",
    "cats = list(Hao_dataset_Train.obs[label_key].cat.categories)\n",
    "\n",
    "# --- Sanity checks: missing/extra labels ---\n",
    "labels_in_palette = set(custom_palette.keys())\n",
    "labels_in_data = set(cats)\n",
    "\n",
    "missing_in_palette = [c for c in cats if c not in labels_in_palette]\n",
    "extra_in_palette   = [c for c in custom_palette.keys() if c not in labels_in_data]\n",
    "\n",
    "if missing_in_palette:\n",
    "    print(\"[WARN] Missing colors for:\", missing_in_palette, \"-> will use light grey (#cccccc).\")\n",
    "if extra_in_palette:\n",
    "    print(\"[INFO] Palette has unused entries:\", extra_in_palette)\n",
    "\n",
    "# --- Build palette list in *category order* ---\n",
    "fallback = '#cccccc'\n",
    "palette_list = [custom_palette.get(c, fallback) for c in cats]\n",
    "\n",
    "# --- Save onto .uns so Scanpy uses it consistently elsewhere ---\n",
    "Hao_dataset_Train.uns[color_key] = palette_list\n",
    "\n",
    "# --- Plot with Scanpy using built-in outlines (no clustering) ---\n",
    "with rc_context({\"figure.figsize\": (5.2, 4.5)}):\n",
    "    sc.pl.embedding(\n",
    "        Hao_dataset_Train,\n",
    "        basis=basis_key,\n",
    "        color=label_key,\n",
    "        palette=palette_list,\n",
    "        legend_loc='on data',\n",
    "        legend_fontsize=10,\n",
    "        legend_fontoutline=1.5,\n",
    "        size=10,\n",
    "        add_outline=True,   # built-in group outlines\n",
    "        frameon=False,\n",
    "        title='Hao',\n",
    "        show=False,\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_path + \"/Hao_final_annotation.png\", \n",
    "        dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_labels = Hao_dataset_Train.obs['Consensus_annotation_detailed_final'].values\n",
    "unique_pops = np.unique(pop_labels)\n",
    "print(f\"Found {len(unique_pops)} populations:\", unique_pops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep.Normalise_protein_data(Hao_dataset_Train, inplace=True, axis=1, flavor=\"seurat\")\n",
    "ep.Normalise_protein_data(Hao_dataset_Cal,   inplace=True, axis=1, flavor=\"seurat\")\n",
    "ep.Normalise_protein_data(Hao_dataset_Test,  inplace=True, axis=1, flavor=\"seurat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(Hao_dataset_Train, 'Consensus_annotation_detailed_final', method='wilcoxon')\n",
    "sc.pl.rank_genes_groups(Hao_dataset_Train, n_genes=10, sharey=False, ncols = 3, fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.rank_genes_groups_matrixplot(Hao_dataset_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_dataset_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Hao_dataset_Train, keys='CD56', groupby='celltype.l2', rotation=90, use_raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Hao_dataset_Train, keys='CD56', groupby='Consensus_annotation_detailed_final', rotation=90, use_raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_data_Train = pd.DataFrame.sparse.from_spmatrix(Hao_dataset_Train.X, index=Hao_dataset_Train.obs_names, columns=Hao_dataset_Train.var_names)\n",
    "Hao_data_Test = pd.DataFrame.sparse.from_spmatrix(Hao_dataset_Test.X, index=Hao_dataset_Test.obs_names, columns=Hao_dataset_Test.var_names)\n",
    "Hao_data_Cal = pd.DataFrame.sparse.from_spmatrix(Hao_dataset_Cal.X, index=Hao_dataset_Cal.obs_names, columns=Hao_dataset_Cal.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these are the columns to be dropped\n",
    "columns_to_drop = [\"IgD\",\"IgM\", \"Rag-IgG2c\"]\n",
    "Hao_data_Train = Hao_data_Train.drop(columns=columns_to_drop, errors='ignore')\n",
    "Hao_data_Test = Hao_data_Test.drop(columns=columns_to_drop, errors='ignore')\n",
    "Hao_data_Cal = Hao_data_Cal.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Zhang X. et al. (2024) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zhang_dataset_Train = sc.read_h5ad(data_path + \"/References/Zhang\" + \"/Zhang_adata_annotated_Train.h5ad\")\n",
    "Zhang_dataset_Test = sc.read_h5ad(data_path + \"/References/Zhang\" + \"/Zhang_adata_annotated_Test.h5ad\")\n",
    "Zhang_dataset_Cal = sc.read_h5ad(data_path + \"/References/Zhang\" + \"/Zhang_adata_annotated_Cal.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zhang_dataset_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "label_key = 'Consensus_annotation_detailed_final'\n",
    "basis_key = 'X_umap'\n",
    "color_key = f'{label_key}_colors'\n",
    "\n",
    "# Your custom palette (label -> hex)\n",
    "custom_palette = {\n",
    "    'B Memory': \"#68D827\",\n",
    "    'B Naive': '#1C511D',\n",
    "    'CD14 Mono': \"#D27CE3\",\n",
    "    'CD16 Mono': \"#8D43CD\",\n",
    "    'CD4 T Memory': \"#C1AF93\",\n",
    "    'CD4 T Naive': \"#C99546\",\n",
    "    'CD8 T Memory': \"#6B3317\",\n",
    "    'CD8 T Naive': \"#4D382E\",\n",
    "    'ErP': \"#D1235A\",\n",
    "    'Erythroblast': \"#F30A1A\",\n",
    "    'GMP': \"#C5E4FF\",\n",
    "    'HSC_MPP': '#0079ea',\n",
    "    'Immature B': \"#91FF7B\",\n",
    "    'LMPP': \"#17BECF\",\n",
    "    'MAIT': \"#BCBD22\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'NK CD56 bright': \"#F3AC1F\",\n",
    "    'NK CD56 dim': \"#FBEF0D\",\n",
    "    'Plasma': \"#9DC012\",\n",
    "    'Pro-B': \"#66BB6A\",\n",
    "    'Small': \"#292929\",\n",
    "    'cDC1': \"#76A7CB\",\n",
    "    'cDC2': \"#16D2E3\",\n",
    "    'GdT': \"#EDB416\",\n",
    "    'Mesenchymal': '#BBBBBB',\n",
    "    'pDC': \"#69FFCB\",\n",
    "    'CD4 CTL': \"#D7D2CB\",\n",
    "    'MEP': \"#E364B0\",\n",
    "    'Pre-B': \"#2DBD67\",\n",
    "    'Pre-Pro-B': '#92AC8E',\n",
    "    'EoBaMaP': \"#728245\",\n",
    "    'MkP': \"#69424D\",\n",
    "    'Stroma': \"#727272\",\n",
    "    'Macrophage': \"#5F4761\",\n",
    "    'ILC': \"#F7CF94\",\n",
    "    'DnT': \"#504423\",\n",
    "}\n",
    "\n",
    "# --- Ensure categorical dtype ---\n",
    "if not pd.api.types.is_categorical_dtype(Zhang_dataset_Train.obs[label_key]):\n",
    "    Zhang_dataset_Train.obs[label_key] = Zhang_dataset_Train.obs[label_key].astype('category')\n",
    "\n",
    "cats = list(Zhang_dataset_Train.obs[label_key].cat.categories)\n",
    "\n",
    "# --- Sanity checks: missing/extra labels ---\n",
    "labels_in_palette = set(custom_palette.keys())\n",
    "labels_in_data = set(cats)\n",
    "\n",
    "missing_in_palette = [c for c in cats if c not in labels_in_palette]\n",
    "extra_in_palette   = [c for c in custom_palette.keys() if c not in labels_in_data]\n",
    "\n",
    "if missing_in_palette:\n",
    "    print(\"[WARN] Missing colors for:\", missing_in_palette, \"-> will use light grey (#cccccc).\")\n",
    "if extra_in_palette:\n",
    "    print(\"[INFO] Palette has unused entries:\", extra_in_palette)\n",
    "\n",
    "# --- Build palette list in *category order* ---\n",
    "fallback = '#cccccc'\n",
    "palette_list = [custom_palette.get(c, fallback) for c in cats]\n",
    "\n",
    "# --- Save onto .uns so Scanpy uses it consistently elsewhere ---\n",
    "Zhang_dataset_Train.uns[color_key] = palette_list\n",
    "\n",
    "# --- Plot with Scanpy using built-in outlines (no clustering) ---\n",
    "with rc_context({\"figure.figsize\": (5.5, 4.5)}):\n",
    "    sc.pl.embedding(\n",
    "        Zhang_dataset_Train,\n",
    "        basis=basis_key,\n",
    "        color=label_key,\n",
    "        palette=palette_list,\n",
    "        legend_loc='on data',\n",
    "        legend_fontsize=10,\n",
    "        legend_fontoutline=1.5,\n",
    "        size=10,\n",
    "        add_outline=True,   # built-in group outlines\n",
    "        frameon=False,\n",
    "        title='Zhang',\n",
    "        show=False,\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_path + \"/Zhang_final_annotation.png\", \n",
    "        dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep.Normalise_protein_data(Zhang_dataset_Train, inplace=True, axis=1, flavor=\"seurat\")\n",
    "ep.Normalise_protein_data(Zhang_dataset_Cal,   inplace=True, axis=1, flavor=\"seurat\")\n",
    "ep.Normalise_protein_data(Zhang_dataset_Test,  inplace=True, axis=1, flavor=\"seurat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(Zhang_dataset_Train, 'Consensus_annotation_detailed_final', method='wilcoxon')\n",
    "sc.pl.rank_genes_groups(Zhang_dataset_Train, n_genes=10, sharey=False, ncols = 3, fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Zhang_dataset_Train, keys='CD123', groupby='Consensus_annotation_broad_final', rotation=90, use_raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zhang_data_Train = pd.DataFrame.sparse.from_spmatrix(Zhang_dataset_Train.X, index=Zhang_dataset_Train.obs_names, columns=Zhang_dataset_Train.var_names)\n",
    "Zhang_data_Test = pd.DataFrame.sparse.from_spmatrix(Zhang_dataset_Test.X, index=Zhang_dataset_Test.obs_names, columns=Zhang_dataset_Test.var_names)\n",
    "Zhang_data_Cal = pd.DataFrame.sparse.from_spmatrix(Zhang_dataset_Cal.X, index=Zhang_dataset_Cal.obs_names, columns=Zhang_dataset_Cal.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these are the columns to be dropped\n",
    "columns_to_drop = [\"IgG.Fc\", \"Isotype_G0114F7\", \"Isotype_HTK888\",\n",
    "                   \"Isotype_MOPC.173\", \"Isotype_MOPC.21\", \"Isotype_MPC.11\",\n",
    "                   \"Isotype_RTK2071\", \"Isotype_RTK2758\", \"Isotype_RTK4174\",\n",
    "                   \"Isotype_RTK4530\"]\n",
    "Zhang_data_Train = Zhang_data_Train.drop(columns=columns_to_drop, errors='ignore')\n",
    "Zhang_data_Test = Zhang_data_Test.drop(columns=columns_to_drop, errors='ignore')\n",
    "Zhang_data_Cal = Zhang_data_Cal.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Triana S. et al. (2021) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triana_dataset_Train = sc.read_h5ad(data_path + \"/References/Triana\" + \"/97AB_young_and_old_adult_healthy_donor_BMMNCs_annotated_Train.h5ad\")\n",
    "Triana_dataset_Test = sc.read_h5ad(data_path + \"/References/Triana\" + \"/97AB_young_and_old_adult_healthy_donor_BMMNCs_annotated_Test.h5ad\")\n",
    "Triana_dataset_Cal = sc.read_h5ad(data_path + \"/References/Triana\" + \"/97AB_young_and_old_adult_healthy_donor_BMMNCs_annotated_Cal.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triana_dataset_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "label_key = 'Consensus_annotation_detailed_final'\n",
    "basis_key = 'X_mofaumap'\n",
    "color_key = f'{label_key}_colors'\n",
    "\n",
    "# Your custom palette (label -> hex)\n",
    "custom_palette = {\n",
    "    'B Memory': \"#68D827\",\n",
    "    'B Naive': '#1C511D',\n",
    "    'CD14 Mono': \"#D27CE3\",\n",
    "    'CD16 Mono': \"#8D43CD\",\n",
    "    'CD4 T Memory': \"#C1AF93\",\n",
    "    'CD4 T Naive': \"#C99546\",\n",
    "    'CD8 T Memory': \"#6B3317\",\n",
    "    'CD8 T Naive': \"#4D382E\",\n",
    "    'ErP': \"#D1235A\",\n",
    "    'Erythroblast': \"#F30A1A\",\n",
    "    'GMP': \"#C5E4FF\",\n",
    "    'HSC_MPP': '#0079ea',\n",
    "    'Immature B': \"#91FF7B\",\n",
    "    'LMPP': \"#17BECF\",\n",
    "    'MAIT': \"#BCBD22\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'NK CD56 bright': \"#F3AC1F\",\n",
    "    'NK CD56 dim': \"#FBEF0D\",\n",
    "    'Plasma': \"#9DC012\",\n",
    "    'Pro-B': \"#66BB6A\",\n",
    "    'Small': \"#292929\",\n",
    "    'cDC1': \"#76A7CB\",\n",
    "    'cDC2': \"#16D2E3\",\n",
    "    'GdT': \"#EDB416\",\n",
    "    'Mesenchymal': '#BBBBBB',\n",
    "    'pDC': \"#69FFCB\",\n",
    "    'CD4 CTL': \"#D7D2CB\",\n",
    "    'MEP': \"#E364B0\",\n",
    "    'Pre-B': \"#2DBD67\",\n",
    "    'Pre-Pro-B': '#92AC8E',\n",
    "    'EoBaMaP': \"#728245\",\n",
    "    'MkP': \"#69424D\",\n",
    "    'Stroma': \"#727272\",\n",
    "    'Macrophage': \"#5F4761\",\n",
    "    'ILC': \"#F7CF94\",\n",
    "    'DnT': \"#504423\",\n",
    "    'Treg': \"#6E6C37\",\n",
    "    'Platelet': \"#FF39A6\",\n",
    "}\n",
    "\n",
    "# --- Ensure categorical dtype ---\n",
    "if not pd.api.types.is_categorical_dtype(Triana_dataset_Train.obs[label_key]):\n",
    "    Triana_dataset_Train.obs[label_key] = Triana_dataset_Train.obs[label_key].astype('category')\n",
    "\n",
    "cats = list(Triana_dataset_Train.obs[label_key].cat.categories)\n",
    "\n",
    "# --- Sanity checks: missing/extra labels ---\n",
    "labels_in_palette = set(custom_palette.keys())\n",
    "labels_in_data = set(cats)\n",
    "\n",
    "missing_in_palette = [c for c in cats if c not in labels_in_palette]\n",
    "extra_in_palette   = [c for c in custom_palette.keys() if c not in labels_in_data]\n",
    "\n",
    "if missing_in_palette:\n",
    "    print(\"[WARN] Missing colors for:\", missing_in_palette, \"-> will use light grey (#cccccc).\")\n",
    "if extra_in_palette:\n",
    "    print(\"[INFO] Palette has unused entries:\", extra_in_palette)\n",
    "\n",
    "# --- Build palette list in *category order* ---\n",
    "fallback = '#cccccc'\n",
    "palette_list = [custom_palette.get(c, fallback) for c in cats]\n",
    "\n",
    "# --- Save onto .uns so Scanpy uses it consistently elsewhere ---\n",
    "Triana_dataset_Train.uns[color_key] = palette_list\n",
    "\n",
    "# --- Plot with Scanpy using built-in outlines (no clustering) ---\n",
    "with rc_context({\"figure.figsize\": (5.25, 4.5)}):\n",
    "    sc.pl.embedding(\n",
    "        Triana_dataset_Train,\n",
    "        basis=basis_key,\n",
    "        color=label_key,\n",
    "        palette=palette_list,\n",
    "        legend_loc='on data',\n",
    "        legend_fontsize=10,\n",
    "        legend_fontoutline=1.5,\n",
    "        size=10,\n",
    "        add_outline=True,   # built-in group outlines\n",
    "        frameon=False,\n",
    "        title='Triana',\n",
    "        show=False,\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_path + \"/Triana_final_annotation.png\", \n",
    "        dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep.Normalise_protein_data(Triana_dataset_Train, inplace=True, axis=1, flavor=\"seurat\")\n",
    "ep.Normalise_protein_data(Triana_dataset_Cal,   inplace=True, axis=1, flavor=\"seurat\")\n",
    "ep.Normalise_protein_data(Triana_dataset_Test,  inplace=True, axis=1, flavor=\"seurat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(Triana_dataset_Train, 'Consensus_annotation_simplified_final', method='wilcoxon')\n",
    "sc.pl.rank_genes_groups(Triana_dataset_Train, n_genes=10, sharey=False, ncols = 3, fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Triana_dataset_Train, keys='CD133', groupby='Consensus_annotation_detailed_final', rotation=90, use_raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triana_data_Train = pd.DataFrame.sparse.from_spmatrix(Triana_dataset_Train.X, index=Triana_dataset_Train.obs_names, columns=Triana_dataset_Train.var_names)\n",
    "Triana_data_Test = pd.DataFrame.sparse.from_spmatrix(Triana_dataset_Test.X, index=Triana_dataset_Test.obs_names, columns=Triana_dataset_Test.var_names)\n",
    "Triana_data_Cal = pd.DataFrame.sparse.from_spmatrix(Triana_dataset_Cal.X, index=Triana_dataset_Cal.obs_names, columns=Triana_dataset_Cal.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these are the columns to be dropped\n",
    "columns_to_drop = [\"IgG\", \"IgD\"]\n",
    "Triana_data_Train = Triana_data_Train.drop(columns=columns_to_drop, errors='ignore')\n",
    "Triana_data_Test = Triana_data_Test.drop(columns=columns_to_drop, errors='ignore')\n",
    "Triana_data_Cal = Triana_data_Cal.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Luecken M.D. et al. (2021) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luecken_dataset_Train = sc.read_h5ad(data_path + \"/References/Luecken\" + \"/140AB_adult_healthy_donor_BMMNCs_annotated_Train.h5ad\")\n",
    "Luecken_dataset_Test = sc.read_h5ad(data_path + \"/References/Luecken\" + \"/140AB_adult_healthy_donor_BMMNCs_annotated_Test.h5ad\")\n",
    "Luecken_dataset_Cal = sc.read_h5ad(data_path + \"/References/Luecken\" + \"/140AB_adult_healthy_donor_BMMNCs_annotated_Cal.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luecken_dataset_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "label_key = 'Consensus_annotation_detailed_final'\n",
    "basis_key = 'X_umap'\n",
    "color_key = f'{label_key}_colors'\n",
    "\n",
    "# Your custom palette (label -> hex)\n",
    "custom_palette = {\n",
    "    'B Memory': \"#68D827\",\n",
    "    'B Naive': '#1C511D',\n",
    "    'CD14 Mono': \"#D27CE3\",\n",
    "    'CD16 Mono': \"#8D43CD\",\n",
    "    'CD4 T Memory': \"#C1AF93\",\n",
    "    'CD4 T Naive': \"#C99546\",\n",
    "    'CD8 T Memory': \"#6B3317\",\n",
    "    'CD8 T Naive': \"#4D382E\",\n",
    "    'ErP': \"#D1235A\",\n",
    "    'Erythroblast': \"#F30A1A\",\n",
    "    'GMP': \"#C5E4FF\",\n",
    "    'HSC_MPP': '#0079ea',\n",
    "    'Immature B': \"#91FF7B\",\n",
    "    'LMPP': \"#17BECF\",\n",
    "    'MAIT': \"#BCBD22\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'NK CD56 bright': \"#F3AC1F\",\n",
    "    'NK CD56 dim': \"#FBEF0D\",\n",
    "    'Plasma': \"#9DC012\",\n",
    "    'Pro-B': \"#66BB6A\",\n",
    "    'Small': \"#292929\",\n",
    "    'cDC1': \"#76A7CB\",\n",
    "    'cDC2': \"#16D2E3\",\n",
    "    'GdT': \"#EDB416\",\n",
    "    'Mesenchymal': '#BBBBBB',\n",
    "    'pDC': \"#69FFCB\",\n",
    "    'CD4 CTL': \"#D7D2CB\",\n",
    "    'MEP': \"#E364B0\",\n",
    "    'Pre-B': \"#2DBD67\",\n",
    "    'Pre-Pro-B': '#92AC8E',\n",
    "    'EoBaMaP': \"#728245\",\n",
    "    'MkP': \"#69424D\",\n",
    "    'Stroma': \"#727272\",\n",
    "    'Macrophage': \"#5F4761\",\n",
    "    'ILC': \"#F7CF94\",\n",
    "    'DnT': \"#504423\",\n",
    "    'Treg': \"#6E6C37\",\n",
    "    'Platelet': \"#FF39A6\",\n",
    "}\n",
    "\n",
    "# --- Ensure categorical dtype ---\n",
    "if not pd.api.types.is_categorical_dtype(Luecken_dataset_Train.obs[label_key]):\n",
    "    Luecken_dataset_Train.obs[label_key] = Luecken_dataset_Train.obs[label_key].astype('category')\n",
    "\n",
    "cats = list(Luecken_dataset_Train.obs[label_key].cat.categories)\n",
    "\n",
    "# --- Sanity checks: missing/extra labels ---\n",
    "labels_in_palette = set(custom_palette.keys())\n",
    "labels_in_data = set(cats)\n",
    "\n",
    "missing_in_palette = [c for c in cats if c not in labels_in_palette]\n",
    "extra_in_palette   = [c for c in custom_palette.keys() if c not in labels_in_data]\n",
    "\n",
    "if missing_in_palette:\n",
    "    print(\"[WARN] Missing colors for:\", missing_in_palette, \"-> will use light grey (#cccccc).\")\n",
    "if extra_in_palette:\n",
    "    print(\"[INFO] Palette has unused entries:\", extra_in_palette)\n",
    "\n",
    "# --- Build palette list in *category order* ---\n",
    "fallback = '#cccccc'\n",
    "palette_list = [custom_palette.get(c, fallback) for c in cats]\n",
    "\n",
    "# --- Save onto .uns so Scanpy uses it consistently elsewhere ---\n",
    "Luecken_dataset_Train.uns[color_key] = palette_list\n",
    "\n",
    "# --- Plot with Scanpy using built-in outlines (no clustering) ---\n",
    "with rc_context({\"figure.figsize\": (5, 4.5)}):\n",
    "    sc.pl.embedding(\n",
    "        Luecken_dataset_Train,\n",
    "        basis=basis_key,\n",
    "        color=label_key,\n",
    "        palette=palette_list,\n",
    "        legend_loc='on data',\n",
    "        legend_fontsize=10,\n",
    "        legend_fontoutline=1.5,\n",
    "        size=10,\n",
    "        add_outline=True,   # built-in group outlines\n",
    "        frameon=False,\n",
    "        title='Luecken',\n",
    "        show=False,\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_path + \"/Luecken_final_annotation.png\", \n",
    "        dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep.Normalise_protein_data(Luecken_dataset_Train, inplace=True, axis=1, flavor=\"seurat\")\n",
    "ep.Normalise_protein_data(Luecken_dataset_Cal,   inplace=True, axis=1, flavor=\"seurat\")\n",
    "ep.Normalise_protein_data(Luecken_dataset_Test,  inplace=True, axis=1, flavor=\"seurat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(Luecken_dataset_Train, 'Consensus_annotation_detailed_final', method='wilcoxon')\n",
    "sc.pl.rank_genes_groups(Luecken_dataset_Train, n_genes=10, sharey=False, ncols = 3, fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Luecken_dataset_Train, keys='CD49b', groupby='Consensus_annotation_detailed_final', rotation=90, use_raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luecken_data_Train = pd.DataFrame.sparse.from_spmatrix(Luecken_dataset_Train.X, index=Luecken_dataset_Train.obs_names, columns=Luecken_dataset_Train.var_names)\n",
    "Luecken_data_Test = pd.DataFrame.sparse.from_spmatrix(Luecken_dataset_Test.X, index=Luecken_dataset_Test.obs_names, columns=Luecken_dataset_Test.var_names)\n",
    "Luecken_data_Cal = pd.DataFrame.sparse.from_spmatrix(Luecken_dataset_Cal.X, index=Luecken_dataset_Cal.obs_names, columns=Luecken_dataset_Cal.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these are the columns to be dropped\n",
    "columns_to_drop = [\"IgG\", \"IgM\", \"IgD\"]\n",
    "Luecken_data_Train = Luecken_data_Train.drop(columns=columns_to_drop, errors='ignore')\n",
    "Luecken_data_Test = Luecken_data_Test.drop(columns=columns_to_drop, errors='ignore')\n",
    "Luecken_data_Cal = Luecken_data_Cal.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luecken_dataset_Train.obs['Consensus_annotation_detailed_final'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of cells per partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- CONFIG -------\n",
    "label_col = \"Consensus_annotation_detailed_final\"\n",
    "\n",
    "partition_colors = {\n",
    "    \"Train\": \"#023047\",  # dark blue\n",
    "    \"Test\":  \"#ffb703\",  # amber\n",
    "    \"Cal\":   \"#edede9\",  # light gray\n",
    "}\n",
    "\n",
    "custom_palette = {\n",
    "    'B Memory': \"#68D827\", 'B Naive': '#1C511D', 'CD14 Mono': \"#D27CE3\",\n",
    "    'CD16 Mono': \"#8D43CD\", 'CD4 T Memory': \"#C1AF93\", 'CD4 T Naive': \"#C99546\",\n",
    "    'CD8 T Memory': \"#6B3317\", 'CD8 T Naive': \"#4D382E\", 'ErP': \"#D1235A\",\n",
    "    'Erythroblast': \"#F30A1A\", 'GMP': \"#C5E4FF\", 'HSC': '#0079ea', 'MPP': \"#79b6ac\",\n",
    "    'Immature B': \"#91FF7B\", 'LMPP': \"#17BECF\", 'MAIT': \"#BCBD22\", 'HSC_MPP': '#0079ea',\n",
    "    'Myeloid progenitor': \"#AEC7E8\", 'NK CD56 bright': \"#F3AC1F\",\n",
    "    'NK CD56 dim': \"#FBEF0D\", 'Plasma': \"#9DC012\", 'Pro-B': \"#66BB6A\",\n",
    "    'Small': \"#292929\", 'cDC1': \"#76A7CB\", 'cDC2': \"#16D2E3\", 'GdT': \"#EDB416\",\n",
    "    'pDC': \"#69FFCB\", 'CD4 CTL': \"#D7D2CB\", 'MEP': \"#E364B0\", 'Pre-B': \"#2DBD67\",\n",
    "    'Pre-Pro-B': '#92AC8E', 'EoBaMaP': \"#728245\", 'MkP': \"#69424D\",\n",
    "    'Stroma': \"#727272\", 'Macrophage': \"#5F4761\", 'ILC': \"#F7CF94\", 'DnT': \"#504423\",\n",
    "    'GdT_DnT': \"#B07A2A\",\n",
    "}\n",
    "\n",
    "# Ensure figures_path exists\n",
    "try:\n",
    "    os.makedirs(figures_path, exist_ok=True)\n",
    "except NameError:\n",
    "    figures_path = \".\"\n",
    "    os.makedirs(figures_path, exist_ok=True)\n",
    "\n",
    "# ---- FIXED OUTPUT SIZE (Option B) ----\n",
    "OUT_W_IN, OUT_H_IN = 3.6, 7.2   # inches -> identical pixel size across datasets\n",
    "OUT_DPI = 300                   # export dpi\n",
    "# fixed axes rectangle [left, bottom, width, height] in figure fraction\n",
    "# Increase 'left' if y labels are long; decrease if you want more plot width.\n",
    "AX_RECT = [0.42, 0.06, 0.56, 0.90]\n",
    "\n",
    "def _obs_to_df(ad, part_name):\n",
    "    s = ad.obs[label_col].astype(str).fillna(\"Unknown\")\n",
    "    return pd.DataFrame({\"Partition\": part_name, label_col: s})\n",
    "\n",
    "def plot_partition_counts_hstack_log10_with_dots(ad_train, ad_test, ad_cal, dataset_name: str):\n",
    "    # sanity\n",
    "    for ad in (ad_train, ad_test, ad_cal):\n",
    "        if label_col not in ad.obs:\n",
    "            raise KeyError(f\"'{label_col}' missing in one of the AnnData objects for {dataset_name}\")\n",
    "\n",
    "    # tidy df\n",
    "    df = pd.concat([\n",
    "        _obs_to_df(ad_train, \"Train\"),\n",
    "        _obs_to_df(ad_test,  \"Test\"),\n",
    "        _obs_to_df(ad_cal,   \"Cal\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # counts: rows = cell type, cols = partition\n",
    "    count_table = (\n",
    "        df.groupby([label_col, \"Partition\"])\n",
    "          .size()\n",
    "          .unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    # order by TRAIN counts, then by total (stable-ish ordering)\n",
    "    train_counts = count_table.get(\"Train\", pd.Series(0, index=count_table.index))\n",
    "    total_counts = count_table.sum(axis=1)\n",
    "\n",
    "    # primary sort: train counts desc\n",
    "    count_table = count_table.loc[train_counts.sort_values(ascending=False).index]\n",
    "\n",
    "    # secondary tie-break: total counts desc (keeps stability for equal train counts)\n",
    "    # use mergesort for stability\n",
    "    tmp = pd.DataFrame({\n",
    "        \"train\": train_counts.reindex(count_table.index),\n",
    "        \"total\": total_counts.reindex(count_table.index),\n",
    "    }, index=count_table.index).sort_values([\"train\", \"total\"], ascending=[False, False], kind=\"mergesort\")\n",
    "    count_table = count_table.loc[tmp.index]\n",
    "\n",
    "    # --- correct log10 stacking ---\n",
    "    totals = count_table.sum(axis=1).to_numpy(dtype=float)\n",
    "    log_totals = np.zeros_like(totals, dtype=float)\n",
    "    mask = totals > 0\n",
    "    log_totals[mask] = np.log10(totals[mask])\n",
    "\n",
    "    labels = count_table.index.to_list()\n",
    "\n",
    "    # ---- FIXED GEOMETRY FIGURE (Option B) ----\n",
    "    fig = plt.figure(figsize=(OUT_W_IN, OUT_H_IN), dpi=OUT_DPI)\n",
    "    ax = fig.add_axes(AX_RECT)  # fixed axes area -> consistent geometry across datasets\n",
    "\n",
    "    left = np.zeros_like(log_totals)\n",
    "\n",
    "    for part in [\"Train\", \"Test\", \"Cal\"]:\n",
    "        counts = count_table[part].to_numpy(dtype=float) if part in count_table.columns else np.zeros_like(totals)\n",
    "        frac = np.divide(counts, totals, out=np.zeros_like(counts), where=mask)\n",
    "        seg = log_totals * frac  # segment width proportional to composition\n",
    "        ax.barh(\n",
    "            labels,\n",
    "            seg,\n",
    "            left=left,\n",
    "            color=partition_colors[part],\n",
    "            label=part,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.8,\n",
    "        )\n",
    "        left += seg\n",
    "\n",
    "    # style\n",
    "    ax.set_xlabel(\"log₁₀(cells)\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # x-axis ticks and vertical guides\n",
    "    tick_vals = [1, 2, 3, 4, 5]\n",
    "    xmax_data = float(np.nanmax(log_totals)) if len(log_totals) else 0.0\n",
    "    xmax = max(xmax_data, max(tick_vals))\n",
    "    ax.set_xlim(0, xmax * 1.05 if xmax > 0 else 1)\n",
    "\n",
    "    ax.set_xticks(tick_vals)\n",
    "    ax.set_xticklabels([str(t) for t in tick_vals])\n",
    "\n",
    "    for xv in tick_vals:\n",
    "        ax.axvline(x=xv, linestyle=\"--\", linewidth=0.6, color=\"#BDBDBD\", alpha=0.8, zorder=0)\n",
    "\n",
    "    # dots between label text and bars, but placed consistently using axes coords for x\n",
    "    dot_x_axes = 0.02  # 2% inside the axes from the left\n",
    "    y_transform = ax.get_yaxis_transform()  # x in axes coords, y in data coords\n",
    "    for y, label in enumerate(labels):\n",
    "        color = custom_palette.get(label, \"#BBBBBB\")\n",
    "        ax.scatter(\n",
    "            dot_x_axes, y,\n",
    "            s=22,\n",
    "            color=color,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.3,\n",
    "            zorder=5,\n",
    "            clip_on=False,\n",
    "            transform=y_transform\n",
    "        )\n",
    "\n",
    "    # legend bottom-right\n",
    "    ax.legend(\n",
    "        loc=\"lower right\",\n",
    "        frameon=False,\n",
    "        fontsize=7,\n",
    "        ncol=1,\n",
    "        handlelength=1.2,\n",
    "        handleheight=0.8,\n",
    "        labelspacing=0.3,\n",
    "        borderaxespad=0.5\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: do NOT use tight_layout or bbox_inches='tight' if you want identical output size\n",
    "    out_png = os.path.join(figures_path, f\"N_celltypes_{dataset_name}.png\")\n",
    "    fig.savefig(out_png, dpi=OUT_DPI)  # fixed pixel size\n",
    "    plt.close(fig)\n",
    "    print(f\"[INFO] Saved: {out_png}\")\n",
    "\n",
    "# -------- RUN FOR THE FOUR DATASETS (already loaded) --------\n",
    "plot_partition_counts_hstack_log10_with_dots(\n",
    "    Luecken_dataset_Train, Triana_dataset_Test if False else Luecken_dataset_Test,\n",
    "    Luecken_dataset_Cal, dataset_name=\"Luecken\"\n",
    ")\n",
    "plot_partition_counts_hstack_log10_with_dots(\n",
    "    Triana_dataset_Train, Triana_dataset_Test, Triana_dataset_Cal, dataset_name=\"Triana\"\n",
    ")\n",
    "plot_partition_counts_hstack_log10_with_dots(\n",
    "    Hao_dataset_Train, Hao_dataset_Test, Hao_dataset_Cal, dataset_name=\"Hao\"\n",
    ")\n",
    "plot_partition_counts_hstack_log10_with_dots(\n",
    "    Zhang_dataset_Train, Zhang_dataset_Test, Zhang_dataset_Cal, dataset_name=\"Zhang\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading antibodies panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalSeqD_Heme_Oncology_CAT399906 = pd.read_csv(data_path + \"/Antibodies_panels/TotalSeqD_Heme_Oncology_CAT399906.csv\", index_col=0).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TotalSeqD Heme Oncology CAT399906 Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_path + '/Pre_trained_models/TotalSeqD_Heme_Oncology_CAT399906'\n",
    "os.makedirs(data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hao Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "os.makedirs(data_path + \"/Hao\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Hao/Dev\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Hao/Release\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Hao/Dev/Models\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Hao/Release/Models\", exist_ok=True)\n",
    "\n",
    "models_output = data_path + \"/Hao\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_Models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broad annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# NOTE:\n",
    "#   Many helper functions are now provided by MLTraining.py.\n",
    "#   This script should primarily orchestrate: data prep -> model training loop -> exports.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\n",
    "    'Immature': \"#0079ea\", \n",
    "    'Mature': \"#AF3434\"\n",
    "}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    'HSC_MPP':            '#0079ea',\n",
    "    'LMPP':               \"#17BECF\",\n",
    "    'GMP':                \"#C5E4FF\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'Monocyte':           \"#D27CE3\",\n",
    "    'CD14 Mono':         \"#D27CE3\",\n",
    "    'CD16 Mono':         \"#8D43CD\",\n",
    "    'Erythroblast':      \"#F30A1A\",\n",
    "    'ErP':               \"#D1235A\",\n",
    "    'MEP':               \"#E364B0\",\n",
    "    'CD4 T Naive':       \"#C99546\",\n",
    "    'CD4 T Memory':      \"#C1AF93\",\n",
    "    'CD8 T Naive':       \"#4D382E\",\n",
    "    'CD8 T Memory':      \"#6B3317\",\n",
    "    'Other_T':           \"#EDB416\",\n",
    "    'Treg':              \"#6E6C37\",\n",
    "    'B Naive':          '#1C511D',\n",
    "    'B Memory':         \"#68D827\",\n",
    "    'Pro-B':            \"#66BB6A\",\n",
    "    'Pre-B':            \"#2DBD67\",\n",
    "    'Immature B':      \"#91FF7B\",\n",
    "    'Plasma':           \"#9DC012\",\n",
    "    'cDC1':             \"#76A7CB\",\n",
    "    'cDC2':             \"#16D2E3\",\n",
    "    'pDC':              \"#69FFCB\",\n",
    "    'NK CD56 bright':  \"#F3AC1F\",\n",
    "    'NK CD56 dim':     \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True    # set False to disable Release outputs\n",
    "EXPORT_DEV     = False   # set True to enable Dev outputs\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Broad\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Choose where to read the 2D embedding from.\n",
    "# Supported:\n",
    "#   - \"adata_obsm\": read from adata_train.obsm[obsm_key]\n",
    "#   - \"adata_obs\":  read from adata_train.obs[[obs_x, obs_y]]\n",
    "#   - \"train_df\":   read from train_df[[df_x, df_y]] (e.g., Hao_data_Train has UMAP columns)\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"adata_obsm\"\n",
    "EMBEDDING_OBSM_KEY = \"X_wnn.umap\"     # e.g. \"X_umap\", \"X_pca\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"adata_obs\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"train_df\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "hao_root = Path(models_output)\n",
    "\n",
    "dev_root     = hao_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir    = models_root / \"heads\"\n",
    "metrics_dir  = reports_root / \"metrics\"\n",
    "probs_dir    = reports_root / \"probabilities\"\n",
    "fig_percls   = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root     = hao_root / \"Release\"\n",
    "release_models   = release_root / name_target_class / \"Models\"\n",
    "release_reports  = release_root / name_target_class / \"Reports\"\n",
    "release_metrics  = release_reports / \"Metrics\"\n",
    "release_probs    = release_reports / \"Probabilities\"\n",
    "release_imps     = release_reports / \"Importances\"\n",
    "release_figs     = release_root / name_target_class / \"Figures\"\n",
    "release_single   = release_figs / \"Single_classes\"\n",
    "\n",
    "# Create directories conditionally\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] DEV Models:  {models_root}\")\n",
    "    print(f\"[INFO] DEV Reports: {reports_root}\")\n",
    "    print(f\"[INFO] DEV Figures: {fig_root}\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Hao_data_Train = MLTraining.attach_celltype(Hao_data_Train, Hao_dataset_Train, consensus_field)\n",
    "Hao_data_Test  = MLTraining.attach_celltype(Hao_data_Test,  Hao_dataset_Test,  consensus_field)\n",
    "Hao_data_Cal   = MLTraining.attach_celltype(Hao_data_Cal,   Hao_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Hao_data_Train = panel_intersection(rename_data_to_panel(Hao_data_Train))\n",
    "Hao_data_Test  = panel_intersection(rename_data_to_panel(Hao_data_Test))\n",
    "Hao_data_Cal   = panel_intersection(rename_data_to_panel(Hao_data_Cal))\n",
    "\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Hao_data_Cal_lbl = Hao_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Cal.columns]\n",
    "\n",
    "Hao_data_Train_Sub = Hao_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Hao_data_Test_Sub  = Hao_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Hao_data_Cal_Sub   = Hao_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Hao_data_Train_Sub.columns)\n",
    "if list(Hao_data_Test_Sub.columns) != cols_train or list(Hao_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Hao_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Hao_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Hao_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "class_names  = sorted(pd.Series(Hao_data_Train[\"Celltype\"]).dropna().unique())\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes\")\n",
    "\n",
    "s_cal = Hao_data_Cal_lbl[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Hao_data_Cal_lbl.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Hao_data_Test[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Hao_data_Test.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "X_cal_all_df = Hao_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Hao_data_Test_Sub.copy()\n",
    "test_index   = Hao_data_Test_Sub.index\n",
    "\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "# Importances collectors\n",
    "xgb_shap_rows = []       # mean_abs + corr (later filtered top10/class)\n",
    "lr_contrib_rows = []     # LR base learner contributions (from stacker_raw)\n",
    "platt_metrics_rows = []  # per-class logloss/brier pre vs post platt\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Hao/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Hao_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Hao_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN UMAP (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=PALETTE_BROAD,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Hao_dataset_Train,\n",
    "            train_df=Hao_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=(str(celltype) == \"Mature\"),\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Hao/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Hao_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Hao_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Hao_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # CAL split for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Hao_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Hao\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions: keep your existing helper for now if not moved\n",
    "    # If you have moved this helper into MLTraining.py, replace call accordingly.\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)  # existing in notebook\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Hao\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "# SHAP export (Top10/class; keep corr_feature_value_vs_shap)\n",
    "shap_df = None\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "\n",
    "    keep_cols = [\n",
    "        \"depth\", \"class_name\", \"dataset\",\n",
    "        \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\",\n",
    "        \"rank_within_class\",\n",
    "    ]\n",
    "    shap_df = shap_df[keep_cols]\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "# LR export\n",
    "lr_df = None\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Hao_data_Test[\"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "\n",
    "    probs_dev_path = probs_dir / \"probabilities_before_after_TEST.csv\"\n",
    "    probs_dev.to_csv(probs_dev_path, index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Hao_data_Test[\"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "\n",
    "    release_probs_path = release_probs / \"Multiclass_models_probabilities_on_test.csv\"\n",
    "    probs_release.to_csv(release_probs_path, index=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    "report_df = pd.DataFrame(report).T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(\n",
    "    cm_mc,\n",
    "    index=pd.Index(class_names, name=\"true\"),\n",
    "    columns=pd.Index(class_names, name=\"pred\"),\n",
    ")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        idx = pd.Index([\"True=Other\", f\"True={cls}\"], name=\"true\")\n",
    "        cols = pd.Index([\"Pred=Other\", f\"Pred={cls}\"], name=\"pred\")\n",
    "        pd.DataFrame(cm_raw, index=idx, columns=cols).to_csv(metrics_dir / f\"{cls_safe}_binary_confmat_TEST_ARGMAX_RAW.csv\")\n",
    "        pd.DataFrame(cm_cal, index=idx, columns=cols).to_csv(metrics_dir / f\"{cls_safe}_binary_confmat_TEST_ARGMAX_CAL.csv\")\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(\n",
    "        y_true_bin,\n",
    "        score_raw,\n",
    "        f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\",\n",
    "        dev_out,\n",
    "        rel_out,\n",
    "    )\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(\n",
    "        y_true_bin,\n",
    "        score_cal,\n",
    "        f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\",\n",
    "        dev_out,\n",
    "        rel_out,\n",
    "    )\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw)\n",
    "    m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw)\n",
    "    per_class_rows.append(m_raw)\n",
    "\n",
    "    m_cal = _metrics_from_cm(cm_cal)\n",
    "    m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal)\n",
    "    per_class_rows.append(m_cal)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    print(f\"  ✓ Saved per-class plots (DEV) → {fig_percls}\")\n",
    "if EXPORT_RELEASE:\n",
    "    print(f\"  ✓ Saved per-class plots (RELEASE) → {release_single}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    dev_metrics_path = metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\"\n",
    "    per_class_df.to_csv(dev_metrics_path, index=False)\n",
    "    print(f\"  ✓ Saved DEV per-class metrics → {dev_metrics_path}\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "    print(f\"  ✓ Saved RELEASE per-class metrics → {out_single}\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "    print(f\"  ✓ Appended DEV binary-head metrics → {dev_root / 'stacker_metrics.csv'}\")\n",
    "\n",
    "print(\"\\n✅ BROAD PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# PATCHES ADDED (to address “plots missing / skipped” symptoms):\n",
    "#   (A) Optional DEBUG_DIAGNOSTICS: prints output paths + CAL class balance + confirms file writes.\n",
    "#   (B) Hard traceback on failures (instead of silent warnings) to surface root cause.\n",
    "#   (C) SHAP beeswarm robustification: optional subsample of TRAIN to avoid memory/time failures.\n",
    "#   (D) Optional SAFE_SINGLE_THREAD: mitigates fork/thread/numba/TBB instability during SHAP/plotting.\n",
    "#   (E) Explicit existence checks after savefig (so “saved but not where expected” is obvious).\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 0: IMPORTS + CONFIG\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\"Immature\": \"#0079ea\", \"Mature\": \"#AF3434\"}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    \"HSC_MPP\":            \"#0079ea\",\n",
    "    \"LMPP\":               \"#17BECF\",\n",
    "    \"GMP\":                \"#C5E4FF\",\n",
    "    \"Myeloid progenitor\": \"#AEC7E8\",\n",
    "    \"Monocyte\":           \"#D27CE3\",\n",
    "    \"CD14 Mono\":          \"#D27CE3\",\n",
    "    \"CD16 Mono\":          \"#8D43CD\",\n",
    "    \"Erythroblast\":       \"#F30A1A\",\n",
    "    \"ErP\":                \"#D1235A\",\n",
    "    \"MEP\":                \"#E364B0\",\n",
    "    \"CD4 T Naive\":        \"#C99546\",\n",
    "    \"CD4 T Memory\":       \"#C1AF93\",\n",
    "    \"CD8 T Naive\":        \"#4D382E\",\n",
    "    \"CD8 T Memory\":       \"#6B3317\",\n",
    "    \"Other_T\":            \"#EDB416\",\n",
    "    \"Treg\":               \"#6E6C37\",\n",
    "    \"B Naive\":            \"#1C511D\",\n",
    "    \"B Memory\":           \"#68D827\",\n",
    "    \"Pro-B\":              \"#66BB6A\",\n",
    "    \"Pre-B\":              \"#2DBD67\",\n",
    "    \"Immature B\":         \"#91FF7B\",\n",
    "    \"Plasma\":             \"#9DC012\",\n",
    "    \"cDC1\":               \"#76A7CB\",\n",
    "    \"cDC2\":               \"#16D2E3\",\n",
    "    \"pDC\":                \"#69FFCB\",\n",
    "    \"NK CD56 bright\":     \"#F3AC1F\",\n",
    "    \"NK CD56 dim\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_BY_DEPTH = {\n",
    "    \"Broad\": PALETTE_BROAD,\n",
    "    \"Simplified\": PALETTE_SIMPLIFIED,\n",
    "    \"Detailed\": PALETTE_DETAILED,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True\n",
    "EXPORT_DEV     = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Simplified\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "EXCLUDE_CLASSES = {}\n",
    "\n",
    "custom_palette = PALETTE_BY_DEPTH.get(name_target_class, {})\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DIAGNOSTICS / ROBUSTIFICATION SWITCHES (PATCH)\n",
    "# -----------------------------------------------------------------------------\n",
    "DEBUG_DIAGNOSTICS = True\n",
    "HARD_TRACEBACKS   = True   # if True: prints stack traces when plot/SHAP fails\n",
    "SHAP_TRAIN_SUBSAMPLE_MAX_N = 5000  # set None to disable subsampling\n",
    "SAFE_SINGLE_THREAD = False  # set True if you see Numba/TBB fork/thread warnings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "EMBEDDING_OBSM_KEY = \"X_wnn.umap\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "hao_root = Path(models_output)\n",
    "\n",
    "dev_root     = hao_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir       = models_root / \"heads\"\n",
    "metrics_dir     = reports_root / \"metrics\"\n",
    "probs_dir       = reports_root / \"probabilities\"\n",
    "fig_percls      = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root    = hao_root / \"Release\"\n",
    "release_models  = release_root / name_target_class / \"Models\"\n",
    "release_reports = release_root / name_target_class / \"Reports\"\n",
    "release_metrics = release_reports / \"Metrics\"\n",
    "release_probs   = release_reports / \"Probabilities\"\n",
    "release_imps    = release_reports / \"Importances\"\n",
    "release_figs    = release_root / name_target_class / \"Figures\"\n",
    "release_single  = release_figs / \"Single_classes\"\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "if DEBUG_DIAGNOSTICS:\n",
    "    print(f\"[DEBUG] HAS_SHAP={HAS_SHAP} EXPORT_RELEASE={EXPORT_RELEASE} EXPORT_DEV={EXPORT_DEV}\")\n",
    "    print(f\"[DEBUG] release_single={release_single}\")\n",
    "    print(f\"[DEBUG] release_imps={release_imps}\")\n",
    "    print(f\"[DEBUG] SAFE_SINGLE_THREAD={SAFE_SINGLE_THREAD} SHAP_SUBSAMPLE_MAX_N={SHAP_TRAIN_SUBSAMPLE_MAX_N}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "Hao_data_Train = MLTraining.attach_celltype(Hao_data_Train, Hao_dataset_Train, consensus_field)\n",
    "Hao_data_Test  = MLTraining.attach_celltype(Hao_data_Test,  Hao_dataset_Test,  consensus_field)\n",
    "Hao_data_Cal   = MLTraining.attach_celltype(Hao_data_Cal,   Hao_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Hao_data_Train = panel_intersection(rename_data_to_panel(Hao_data_Train))\n",
    "Hao_data_Test  = panel_intersection(rename_data_to_panel(Hao_data_Test))\n",
    "Hao_data_Cal   = panel_intersection(rename_data_to_panel(Hao_data_Cal))\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS (WITH CAL/TEST ROW FILTERING)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Hao_data_Cal_lbl = Hao_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Cal.columns]\n",
    "\n",
    "Hao_data_Train_Sub = Hao_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Hao_data_Test_Sub  = Hao_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Hao_data_Cal_Sub   = Hao_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Hao_data_Train_Sub.columns)\n",
    "if list(Hao_data_Test_Sub.columns) != cols_train or list(Hao_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Hao_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Hao_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Hao_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "# classes learned from TRAIN, excluding user-specified\n",
    "all_classes = sorted(pd.Series(Hao_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if str(c) not in EXCLUDE_CLASSES]\n",
    "\n",
    "excluded_present = sorted(set(all_classes).intersection(EXCLUDE_CLASSES))\n",
    "if excluded_present:\n",
    "    print(f\"  [INFO] Excluding {len(excluded_present)} classes: {excluded_present}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes after exclusions\")\n",
    "\n",
    "# ---- critical: filter CAL/TEST rows to those classes ----\n",
    "keep_set = set(map(str, class_names))\n",
    "\n",
    "cal_keep_mask  = Hao_data_Cal_lbl[\"Celltype\"].astype(str).isin(keep_set)\n",
    "test_keep_mask = Hao_data_Test[\"Celltype\"].astype(str).isin(keep_set)\n",
    "\n",
    "n_cal_drop  = int((~cal_keep_mask).sum())\n",
    "n_test_drop = int((~test_keep_mask).sum())\n",
    "\n",
    "if n_cal_drop > 0:\n",
    "    dropped = sorted(Hao_data_Cal_lbl.loc[~cal_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_cal_drop} CAL rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "if n_test_drop > 0:\n",
    "    dropped = sorted(Hao_data_Test.loc[~test_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_test_drop} TEST rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "# filtered label frames\n",
    "Hao_data_Cal_lbl_f  = Hao_data_Cal_lbl.loc[cal_keep_mask].copy()\n",
    "Hao_data_Test_lbl_f = Hao_data_Test.loc[test_keep_mask, [\"Celltype\"]].copy()\n",
    "\n",
    "# filtered feature frames (must align by index)\n",
    "X_cal_all_df = Hao_data_Cal_Sub.loc[Hao_data_Cal_lbl_f.index].copy()\n",
    "X_te_all_df  = Hao_data_Test_Sub.loc[Hao_data_Test_lbl_f.index].copy()\n",
    "test_index   = X_te_all_df.index\n",
    "\n",
    "# map filtered labels\n",
    "s_cal = Hao_data_Cal_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Hao_data_Cal_lbl_f.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in CAL after filtering: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Hao_data_Test_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Hao_data_Test_lbl_f.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in TEST after filtering: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# probability matrices sized to filtered CAL/TEST\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "xgb_shap_rows      = []\n",
    "lr_contrib_rows    = []\n",
    "platt_metrics_rows = []\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Hao/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Hao_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Hao_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN embedding (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=custom_palette,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Hao_dataset_Train,\n",
    "            train_df=Hao_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Hao/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Hao_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Hao_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST (filtered) for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Hao_data_Test.loc[X_te_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # CAL split (filtered) for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Hao_data_Cal.loc[X_cal_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(scaler.transform(df.values), index=df.index, columns=cols_train)\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Hao\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions (unchanged)\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Hao\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "    shap_df = shap_df[\n",
    "        [\"depth\", \"class_name\", \"dataset\", \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\", \"rank_within_class\"]\n",
    "    ]\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Hao_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "    probs_dev.to_csv(probs_dir / \"probabilities_before_after_TEST.csv\", index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Hao_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "    probs_release.to_csv(release_probs / \"Multiclass_models_probabilities_on_test.csv\", index=True)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report_df = pd.DataFrame(\n",
    "    classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    ").T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(cm_mc, index=pd.Index(class_names, name=\"true\"), columns=pd.Index(class_names, name=\"pred\"))\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(y_true_bin, score_raw, f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(y_true_bin, score_cal, f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw); m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw); per_class_rows.append(m_raw)\n",
    "    m_cal = _metrics_from_cm(cm_cal); m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal); per_class_rows.append(m_cal)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    per_class_df.to_csv(metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\", index=False)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\n✅ SIMPLIFIED PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# PATCHES ADDED (to address “plots missing / skipped” symptoms):\n",
    "#   (A) Optional DEBUG_DIAGNOSTICS: prints output paths + CAL class balance + confirms file writes.\n",
    "#   (B) Hard traceback on failures (instead of silent warnings) to surface root cause.\n",
    "#   (C) SHAP beeswarm robustification: optional subsample of TRAIN to avoid memory/time failures.\n",
    "#   (D) Optional SAFE_SINGLE_THREAD: mitigates fork/thread/numba/TBB instability during SHAP/plotting.\n",
    "#   (E) Explicit existence checks after savefig (so “saved but not where expected” is obvious).\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 0: IMPORTS + CONFIG\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\"Immature\": \"#0079ea\", \"Mature\": \"#AF3434\"}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    \"HSC_MPP\":            \"#0079ea\",\n",
    "    \"LMPP\":               \"#17BECF\",\n",
    "    \"GMP\":                \"#C5E4FF\",\n",
    "    \"Myeloid progenitor\": \"#AEC7E8\",\n",
    "    \"Monocyte\":           \"#D27CE3\",\n",
    "    \"CD14 Mono\":          \"#D27CE3\",\n",
    "    \"CD16 Mono\":          \"#8D43CD\",\n",
    "    \"Erythroblast\":       \"#F30A1A\",\n",
    "    \"ErP\":                \"#D1235A\",\n",
    "    \"MEP\":                \"#E364B0\",\n",
    "    \"CD4 T Naive\":        \"#C99546\",\n",
    "    \"CD4 T Memory\":       \"#C1AF93\",\n",
    "    \"CD8 T Naive\":        \"#4D382E\",\n",
    "    \"CD8 T Memory\":       \"#6B3317\",\n",
    "    \"Other_T\":            \"#EDB416\",\n",
    "    \"Treg\":               \"#6E6C37\",\n",
    "    \"B Naive\":            \"#1C511D\",\n",
    "    \"B Memory\":           \"#68D827\",\n",
    "    \"Pro-B\":              \"#66BB6A\",\n",
    "    \"Pre-B\":              \"#2DBD67\",\n",
    "    \"Immature B\":         \"#91FF7B\",\n",
    "    \"Plasma\":             \"#9DC012\",\n",
    "    \"cDC1\":               \"#76A7CB\",\n",
    "    \"cDC2\":               \"#16D2E3\",\n",
    "    \"pDC\":                \"#69FFCB\",\n",
    "    \"NK CD56 bright\":     \"#F3AC1F\",\n",
    "    \"NK CD56 dim\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_BY_DEPTH = {\n",
    "    \"Broad\": PALETTE_BROAD,\n",
    "    \"Simplified\": PALETTE_SIMPLIFIED,\n",
    "    \"Detailed\": PALETTE_DETAILED,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True\n",
    "EXPORT_DEV     = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Detailed\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "EXCLUDE_CLASSES = {}\n",
    "\n",
    "custom_palette = PALETTE_BY_DEPTH.get(name_target_class, {})\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DIAGNOSTICS / ROBUSTIFICATION SWITCHES (PATCH)\n",
    "# -----------------------------------------------------------------------------\n",
    "DEBUG_DIAGNOSTICS = True\n",
    "HARD_TRACEBACKS   = True   # if True: prints stack traces when plot/SHAP fails\n",
    "SHAP_TRAIN_SUBSAMPLE_MAX_N = 5000  # set None to disable subsampling\n",
    "SAFE_SINGLE_THREAD = False  # set True if you see Numba/TBB fork/thread warnings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "EMBEDDING_OBSM_KEY = \"X_wnn.umap\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "hao_root = Path(models_output)\n",
    "\n",
    "dev_root     = hao_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir       = models_root / \"heads\"\n",
    "metrics_dir     = reports_root / \"metrics\"\n",
    "probs_dir       = reports_root / \"probabilities\"\n",
    "fig_percls      = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root    = hao_root / \"Release\"\n",
    "release_models  = release_root / name_target_class / \"Models\"\n",
    "release_reports = release_root / name_target_class / \"Reports\"\n",
    "release_metrics = release_reports / \"Metrics\"\n",
    "release_probs   = release_reports / \"Probabilities\"\n",
    "release_imps    = release_reports / \"Importances\"\n",
    "release_figs    = release_root / name_target_class / \"Figures\"\n",
    "release_single  = release_figs / \"Single_classes\"\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "if DEBUG_DIAGNOSTICS:\n",
    "    print(f\"[DEBUG] HAS_SHAP={HAS_SHAP} EXPORT_RELEASE={EXPORT_RELEASE} EXPORT_DEV={EXPORT_DEV}\")\n",
    "    print(f\"[DEBUG] release_single={release_single}\")\n",
    "    print(f\"[DEBUG] release_imps={release_imps}\")\n",
    "    print(f\"[DEBUG] SAFE_SINGLE_THREAD={SAFE_SINGLE_THREAD} SHAP_SUBSAMPLE_MAX_N={SHAP_TRAIN_SUBSAMPLE_MAX_N}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "Hao_data_Train = MLTraining.attach_celltype(Hao_data_Train, Hao_dataset_Train, consensus_field)\n",
    "Hao_data_Test  = MLTraining.attach_celltype(Hao_data_Test,  Hao_dataset_Test,  consensus_field)\n",
    "Hao_data_Cal   = MLTraining.attach_celltype(Hao_data_Cal,   Hao_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Hao_data_Train = panel_intersection(rename_data_to_panel(Hao_data_Train))\n",
    "Hao_data_Test  = panel_intersection(rename_data_to_panel(Hao_data_Test))\n",
    "Hao_data_Cal   = panel_intersection(rename_data_to_panel(Hao_data_Cal))\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS (WITH CAL/TEST ROW FILTERING)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Hao_data_Cal_lbl = Hao_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Cal.columns]\n",
    "\n",
    "Hao_data_Train_Sub = Hao_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Hao_data_Test_Sub  = Hao_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Hao_data_Cal_Sub   = Hao_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Hao_data_Train_Sub.columns)\n",
    "if list(Hao_data_Test_Sub.columns) != cols_train or list(Hao_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Hao_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Hao_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Hao_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "# classes learned from TRAIN, excluding user-specified\n",
    "all_classes = sorted(pd.Series(Hao_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if str(c) not in EXCLUDE_CLASSES]\n",
    "\n",
    "excluded_present = sorted(set(all_classes).intersection(EXCLUDE_CLASSES))\n",
    "if excluded_present:\n",
    "    print(f\"  [INFO] Excluding {len(excluded_present)} classes: {excluded_present}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes after exclusions\")\n",
    "\n",
    "# ---- critical: filter CAL/TEST rows to those classes ----\n",
    "keep_set = set(map(str, class_names))\n",
    "\n",
    "cal_keep_mask  = Hao_data_Cal_lbl[\"Celltype\"].astype(str).isin(keep_set)\n",
    "test_keep_mask = Hao_data_Test[\"Celltype\"].astype(str).isin(keep_set)\n",
    "\n",
    "n_cal_drop  = int((~cal_keep_mask).sum())\n",
    "n_test_drop = int((~test_keep_mask).sum())\n",
    "\n",
    "if n_cal_drop > 0:\n",
    "    dropped = sorted(Hao_data_Cal_lbl.loc[~cal_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_cal_drop} CAL rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "if n_test_drop > 0:\n",
    "    dropped = sorted(Hao_data_Test.loc[~test_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_test_drop} TEST rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "# filtered label frames\n",
    "Hao_data_Cal_lbl_f  = Hao_data_Cal_lbl.loc[cal_keep_mask].copy()\n",
    "Hao_data_Test_lbl_f = Hao_data_Test.loc[test_keep_mask, [\"Celltype\"]].copy()\n",
    "\n",
    "# filtered feature frames (must align by index)\n",
    "X_cal_all_df = Hao_data_Cal_Sub.loc[Hao_data_Cal_lbl_f.index].copy()\n",
    "X_te_all_df  = Hao_data_Test_Sub.loc[Hao_data_Test_lbl_f.index].copy()\n",
    "test_index   = X_te_all_df.index\n",
    "\n",
    "# map filtered labels\n",
    "s_cal = Hao_data_Cal_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Hao_data_Cal_lbl_f.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in CAL after filtering: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Hao_data_Test_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Hao_data_Test_lbl_f.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in TEST after filtering: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# probability matrices sized to filtered CAL/TEST\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "xgb_shap_rows      = []\n",
    "lr_contrib_rows    = []\n",
    "platt_metrics_rows = []\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Hao/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Hao_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Hao_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN embedding (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=custom_palette,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Hao_dataset_Train,\n",
    "            train_df=Hao_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Hao/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Hao_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Hao_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST (filtered) for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Hao_data_Test.loc[X_te_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # CAL split (filtered) for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Hao_data_Cal.loc[X_cal_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(scaler.transform(df.values), index=df.index, columns=cols_train)\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Hao\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions (unchanged)\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Hao\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "    shap_df = shap_df[\n",
    "        [\"depth\", \"class_name\", \"dataset\", \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\", \"rank_within_class\"]\n",
    "    ]\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Hao_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "    probs_dev.to_csv(probs_dir / \"probabilities_before_after_TEST.csv\", index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Hao_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "    probs_release.to_csv(release_probs / \"Multiclass_models_probabilities_on_test.csv\", index=True)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report_df = pd.DataFrame(\n",
    "    classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    ").T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(cm_mc, index=pd.Index(class_names, name=\"true\"), columns=pd.Index(class_names, name=\"pred\"))\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(y_true_bin, score_raw, f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(y_true_bin, score_cal, f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw); m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw); per_class_rows.append(m_raw)\n",
    "    m_cal = _metrics_from_cm(cm_cal); m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal); per_class_rows.append(m_cal)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    per_class_df.to_csv(metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\", index=False)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\n✅ DETAILED PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zhang Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "os.makedirs(data_path + \"/Zhang\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Zhang/Dev\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Zhang/Release\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Zhang/Dev/Models\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Zhang/Release/Models\", exist_ok=True)\n",
    "\n",
    "models_output = data_path + \"/Zhang\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zhang_Models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broad annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# NOTE:\n",
    "#   Many helper functions are now provided by MLTraining.py.\n",
    "#   This script should primarily orchestrate: data prep -> model training loop -> exports.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\n",
    "    'Immature': \"#0079ea\", \n",
    "    'Mature': \"#AF3434\"\n",
    "}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    'HSC_MPP':            '#0079ea',\n",
    "    'LMPP':               \"#17BECF\",\n",
    "    'GMP':                \"#C5E4FF\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'Monocyte':           \"#D27CE3\",\n",
    "    'CD14 Mono':         \"#D27CE3\",\n",
    "    'CD16 Mono':         \"#8D43CD\",\n",
    "    'Erythroblast':      \"#F30A1A\",\n",
    "    'ErP':               \"#D1235A\",\n",
    "    'MEP':               \"#E364B0\",\n",
    "    'CD4 T Naive':       \"#C99546\",\n",
    "    'CD4 T Memory':      \"#C1AF93\",\n",
    "    'CD8 T Naive':       \"#4D382E\",\n",
    "    'CD8 T Memory':      \"#6B3317\",\n",
    "    'Other_T':           \"#EDB416\",\n",
    "    'Treg':              \"#6E6C37\",\n",
    "    'B Naive':          '#1C511D',\n",
    "    'B Memory':         \"#68D827\",\n",
    "    'Pro-B':            \"#66BB6A\",\n",
    "    'Pre-B':            \"#2DBD67\",\n",
    "    'Immature B':      \"#91FF7B\",\n",
    "    'Plasma':           \"#9DC012\",\n",
    "    'cDC1':             \"#76A7CB\",\n",
    "    'cDC2':             \"#16D2E3\",\n",
    "    'pDC':              \"#69FFCB\",\n",
    "    'NK CD56 bright':  \"#F3AC1F\",\n",
    "    'NK CD56 dim':     \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True    # set False to disable Release outputs\n",
    "EXPORT_DEV     = False   # set True to enable Dev outputs\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Broad\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Choose where to read the 2D embedding from.\n",
    "# Supported:\n",
    "#   - \"adata_obsm\": read from adata_train.obsm[obsm_key]\n",
    "#   - \"adata_obs\":  read from adata_train.obs[[obs_x, obs_y]]\n",
    "#   - \"train_df\":   read from train_df[[df_x, df_y]] (e.g., Zhang_data_Train has UMAP columns)\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"adata_obsm\"\n",
    "EMBEDDING_OBSM_KEY = \"X_umap\"     # e.g. \"X_umap\", \"X_pca\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"adata_obs\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"train_df\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "Zhang_root = Path(models_output)\n",
    "\n",
    "dev_root     = Zhang_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir    = models_root / \"heads\"\n",
    "metrics_dir  = reports_root / \"metrics\"\n",
    "probs_dir    = reports_root / \"probabilities\"\n",
    "fig_percls   = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root     = Zhang_root / \"Release\"\n",
    "release_models   = release_root / name_target_class / \"Models\"\n",
    "release_reports  = release_root / name_target_class / \"Reports\"\n",
    "release_metrics  = release_reports / \"Metrics\"\n",
    "release_probs    = release_reports / \"Probabilities\"\n",
    "release_imps     = release_reports / \"Importances\"\n",
    "release_figs     = release_root / name_target_class / \"Figures\"\n",
    "release_single   = release_figs / \"Single_classes\"\n",
    "\n",
    "# Create directories conditionally\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] DEV Models:  {models_root}\")\n",
    "    print(f\"[INFO] DEV Reports: {reports_root}\")\n",
    "    print(f\"[INFO] DEV Figures: {fig_root}\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Zhang_data_Train = MLTraining.attach_celltype(Zhang_data_Train, Zhang_dataset_Train, consensus_field)\n",
    "Zhang_data_Test  = MLTraining.attach_celltype(Zhang_data_Test,  Zhang_dataset_Test,  consensus_field)\n",
    "Zhang_data_Cal   = MLTraining.attach_celltype(Zhang_data_Cal,   Zhang_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Zhang_data_Train = panel_intersection(rename_data_to_panel(Zhang_data_Train))\n",
    "Zhang_data_Test  = panel_intersection(rename_data_to_panel(Zhang_data_Test))\n",
    "Zhang_data_Cal   = panel_intersection(rename_data_to_panel(Zhang_data_Cal))\n",
    "\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Zhang_data_Cal_lbl = Zhang_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Cal.columns]\n",
    "\n",
    "Zhang_data_Train_Sub = Zhang_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Zhang_data_Test_Sub  = Zhang_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Zhang_data_Cal_Sub   = Zhang_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Zhang_data_Train_Sub.columns)\n",
    "if list(Zhang_data_Test_Sub.columns) != cols_train or list(Zhang_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Zhang_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Zhang_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Zhang_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "class_names  = sorted(pd.Series(Zhang_data_Train[\"Celltype\"]).dropna().unique())\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes\")\n",
    "\n",
    "s_cal = Zhang_data_Cal_lbl[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Zhang_data_Cal_lbl.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Zhang_data_Test[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Zhang_data_Test.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "X_cal_all_df = Zhang_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Zhang_data_Test_Sub.copy()\n",
    "test_index   = Zhang_data_Test_Sub.index\n",
    "\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "# Importances collectors\n",
    "xgb_shap_rows = []       # mean_abs + corr (later filtered top10/class)\n",
    "lr_contrib_rows = []     # LR base learner contributions (from stacker_raw)\n",
    "platt_metrics_rows = []  # per-class logloss/brier pre vs post platt\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Zhang/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Zhang_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Zhang_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN UMAP (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=PALETTE_BROAD,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Zhang_dataset_Train,\n",
    "            train_df=Zhang_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=(str(celltype) == \"Mature\"),\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Zhang/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Zhang_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Zhang_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Zhang_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # CAL split for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Zhang_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Zhang\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions: keep your existing helper for now if not moved\n",
    "    # If you have moved this helper into MLTraining.py, replace call accordingly.\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)  # existing in notebook\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Zhang\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "# SHAP export (Top10/class; keep corr_feature_value_vs_shap)\n",
    "shap_df = None\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "\n",
    "    keep_cols = [\n",
    "        \"depth\", \"class_name\", \"dataset\",\n",
    "        \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\",\n",
    "        \"rank_within_class\",\n",
    "    ]\n",
    "    shap_df = shap_df[keep_cols]\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "# LR export\n",
    "lr_df = None\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Zhang_data_Test[\"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "\n",
    "    probs_dev_path = probs_dir / \"probabilities_before_after_TEST.csv\"\n",
    "    probs_dev.to_csv(probs_dev_path, index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Zhang_data_Test[\"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "\n",
    "    release_probs_path = release_probs / \"Multiclass_models_probabilities_on_test.csv\"\n",
    "    probs_release.to_csv(release_probs_path, index=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    "report_df = pd.DataFrame(report).T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(\n",
    "    cm_mc,\n",
    "    index=pd.Index(class_names, name=\"true\"),\n",
    "    columns=pd.Index(class_names, name=\"pred\"),\n",
    ")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        idx = pd.Index([\"True=Other\", f\"True={cls}\"], name=\"true\")\n",
    "        cols = pd.Index([\"Pred=Other\", f\"Pred={cls}\"], name=\"pred\")\n",
    "        pd.DataFrame(cm_raw, index=idx, columns=cols).to_csv(metrics_dir / f\"{cls_safe}_binary_confmat_TEST_ARGMAX_RAW.csv\")\n",
    "        pd.DataFrame(cm_cal, index=idx, columns=cols).to_csv(metrics_dir / f\"{cls_safe}_binary_confmat_TEST_ARGMAX_CAL.csv\")\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(\n",
    "        y_true_bin,\n",
    "        score_raw,\n",
    "        f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\",\n",
    "        dev_out,\n",
    "        rel_out,\n",
    "    )\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(\n",
    "        y_true_bin,\n",
    "        score_cal,\n",
    "        f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\",\n",
    "        dev_out,\n",
    "        rel_out,\n",
    "    )\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw)\n",
    "    m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw)\n",
    "    per_class_rows.append(m_raw)\n",
    "\n",
    "    m_cal = _metrics_from_cm(cm_cal)\n",
    "    m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal)\n",
    "    per_class_rows.append(m_cal)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    print(f\"  ✓ Saved per-class plots (DEV) → {fig_percls}\")\n",
    "if EXPORT_RELEASE:\n",
    "    print(f\"  ✓ Saved per-class plots (RELEASE) → {release_single}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    dev_metrics_path = metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\"\n",
    "    per_class_df.to_csv(dev_metrics_path, index=False)\n",
    "    print(f\"  ✓ Saved DEV per-class metrics → {dev_metrics_path}\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "    print(f\"  ✓ Saved RELEASE per-class metrics → {out_single}\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "    print(f\"  ✓ Appended DEV binary-head metrics → {dev_root / 'stacker_metrics.csv'}\")\n",
    "\n",
    "print(\"\\n✅ BROAD PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# PATCHES ADDED (to address “plots missing / skipped” symptoms):\n",
    "#   (A) Optional DEBUG_DIAGNOSTICS: prints output paths + CAL class balance + confirms file writes.\n",
    "#   (B) Hard traceback on failures (instead of silent warnings) to surface root cause.\n",
    "#   (C) SHAP beeswarm robustification: optional subsample of TRAIN to avoid memory/time failures.\n",
    "#   (D) Optional SAFE_SINGLE_THREAD: mitigates fork/thread/numba/TBB instability during SHAP/plotting.\n",
    "#   (E) Explicit existence checks after savefig (so “saved but not where expected” is obvious).\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 0: IMPORTS + CONFIG\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\"Immature\": \"#0079ea\", \"Mature\": \"#AF3434\"}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    \"HSC_MPP\":            \"#0079ea\",\n",
    "    \"LMPP\":               \"#17BECF\",\n",
    "    \"GMP\":                \"#C5E4FF\",\n",
    "    \"Myeloid progenitor\": \"#AEC7E8\",\n",
    "    \"Monocyte\":           \"#D27CE3\",\n",
    "    \"CD14 Mono\":          \"#D27CE3\",\n",
    "    \"CD16 Mono\":          \"#8D43CD\",\n",
    "    \"Erythroblast\":       \"#F30A1A\",\n",
    "    \"ErP\":                \"#D1235A\",\n",
    "    \"MEP\":                \"#E364B0\",\n",
    "    \"CD4 T Naive\":        \"#C99546\",\n",
    "    \"CD4 T Memory\":       \"#C1AF93\",\n",
    "    \"CD8 T Naive\":        \"#4D382E\",\n",
    "    \"CD8 T Memory\":       \"#6B3317\",\n",
    "    \"Other_T\":            \"#EDB416\",\n",
    "    \"Treg\":               \"#6E6C37\",\n",
    "    \"B Naive\":            \"#1C511D\",\n",
    "    \"B Memory\":           \"#68D827\",\n",
    "    \"Pro-B\":              \"#66BB6A\",\n",
    "    \"Pre-B\":              \"#2DBD67\",\n",
    "    \"Immature B\":         \"#91FF7B\",\n",
    "    \"Plasma\":             \"#9DC012\",\n",
    "    \"cDC1\":               \"#76A7CB\",\n",
    "    \"cDC2\":               \"#16D2E3\",\n",
    "    \"pDC\":                \"#69FFCB\",\n",
    "    \"NK CD56 bright\":     \"#F3AC1F\",\n",
    "    \"NK CD56 dim\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_BY_DEPTH = {\n",
    "    \"Broad\": PALETTE_BROAD,\n",
    "    \"Simplified\": PALETTE_SIMPLIFIED,\n",
    "    \"Detailed\": PALETTE_DETAILED,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True\n",
    "EXPORT_DEV     = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Simplified\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "EXCLUDE_CLASSES = {}\n",
    "\n",
    "custom_palette = PALETTE_BY_DEPTH.get(name_target_class, {})\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DIAGNOSTICS / ROBUSTIFICATION SWITCHES (PATCH)\n",
    "# -----------------------------------------------------------------------------\n",
    "DEBUG_DIAGNOSTICS = True\n",
    "HARD_TRACEBACKS   = True   # if True: prints stack traces when plot/SHAP fails\n",
    "SHAP_TRAIN_SUBSAMPLE_MAX_N = 5000  # set None to disable subsampling\n",
    "SAFE_SINGLE_THREAD = False  # set True if you see Numba/TBB fork/thread warnings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "EMBEDDING_OBSM_KEY = \"X_umap\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "Zhang_root = Path(models_output)\n",
    "\n",
    "dev_root     = Zhang_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir       = models_root / \"heads\"\n",
    "metrics_dir     = reports_root / \"metrics\"\n",
    "probs_dir       = reports_root / \"probabilities\"\n",
    "fig_percls      = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root    = Zhang_root / \"Release\"\n",
    "release_models  = release_root / name_target_class / \"Models\"\n",
    "release_reports = release_root / name_target_class / \"Reports\"\n",
    "release_metrics = release_reports / \"Metrics\"\n",
    "release_probs   = release_reports / \"Probabilities\"\n",
    "release_imps    = release_reports / \"Importances\"\n",
    "release_figs    = release_root / name_target_class / \"Figures\"\n",
    "release_single  = release_figs / \"Single_classes\"\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "if DEBUG_DIAGNOSTICS:\n",
    "    print(f\"[DEBUG] HAS_SHAP={HAS_SHAP} EXPORT_RELEASE={EXPORT_RELEASE} EXPORT_DEV={EXPORT_DEV}\")\n",
    "    print(f\"[DEBUG] release_single={release_single}\")\n",
    "    print(f\"[DEBUG] release_imps={release_imps}\")\n",
    "    print(f\"[DEBUG] SAFE_SINGLE_THREAD={SAFE_SINGLE_THREAD} SHAP_SUBSAMPLE_MAX_N={SHAP_TRAIN_SUBSAMPLE_MAX_N}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "Zhang_data_Train = MLTraining.attach_celltype(Zhang_data_Train, Zhang_dataset_Train, consensus_field)\n",
    "Zhang_data_Test  = MLTraining.attach_celltype(Zhang_data_Test,  Zhang_dataset_Test,  consensus_field)\n",
    "Zhang_data_Cal   = MLTraining.attach_celltype(Zhang_data_Cal,   Zhang_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Zhang_data_Train = panel_intersection(rename_data_to_panel(Zhang_data_Train))\n",
    "Zhang_data_Test  = panel_intersection(rename_data_to_panel(Zhang_data_Test))\n",
    "Zhang_data_Cal   = panel_intersection(rename_data_to_panel(Zhang_data_Cal))\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS (WITH CAL/TEST ROW FILTERING)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Zhang_data_Cal_lbl = Zhang_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Cal.columns]\n",
    "\n",
    "Zhang_data_Train_Sub = Zhang_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Zhang_data_Test_Sub  = Zhang_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Zhang_data_Cal_Sub   = Zhang_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Zhang_data_Train_Sub.columns)\n",
    "if list(Zhang_data_Test_Sub.columns) != cols_train or list(Zhang_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Zhang_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Zhang_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Zhang_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "# classes learned from TRAIN, excluding user-specified\n",
    "all_classes = sorted(pd.Series(Zhang_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if str(c) not in EXCLUDE_CLASSES]\n",
    "\n",
    "excluded_present = sorted(set(all_classes).intersection(EXCLUDE_CLASSES))\n",
    "if excluded_present:\n",
    "    print(f\"  [INFO] Excluding {len(excluded_present)} classes: {excluded_present}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes after exclusions\")\n",
    "\n",
    "# ---- critical: filter CAL/TEST rows to those classes ----\n",
    "keep_set = set(map(str, class_names))\n",
    "\n",
    "cal_keep_mask  = Zhang_data_Cal_lbl[\"Celltype\"].astype(str).isin(keep_set)\n",
    "test_keep_mask = Zhang_data_Test[\"Celltype\"].astype(str).isin(keep_set)\n",
    "\n",
    "n_cal_drop  = int((~cal_keep_mask).sum())\n",
    "n_test_drop = int((~test_keep_mask).sum())\n",
    "\n",
    "if n_cal_drop > 0:\n",
    "    dropped = sorted(Zhang_data_Cal_lbl.loc[~cal_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_cal_drop} CAL rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "if n_test_drop > 0:\n",
    "    dropped = sorted(Zhang_data_Test.loc[~test_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_test_drop} TEST rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "# filtered label frames\n",
    "Zhang_data_Cal_lbl_f  = Zhang_data_Cal_lbl.loc[cal_keep_mask].copy()\n",
    "Zhang_data_Test_lbl_f = Zhang_data_Test.loc[test_keep_mask, [\"Celltype\"]].copy()\n",
    "\n",
    "# filtered feature frames (must align by index)\n",
    "X_cal_all_df = Zhang_data_Cal_Sub.loc[Zhang_data_Cal_lbl_f.index].copy()\n",
    "X_te_all_df  = Zhang_data_Test_Sub.loc[Zhang_data_Test_lbl_f.index].copy()\n",
    "test_index   = X_te_all_df.index\n",
    "\n",
    "# map filtered labels\n",
    "s_cal = Zhang_data_Cal_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Zhang_data_Cal_lbl_f.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in CAL after filtering: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Zhang_data_Test_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Zhang_data_Test_lbl_f.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in TEST after filtering: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# probability matrices sized to filtered CAL/TEST\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "xgb_shap_rows      = []\n",
    "lr_contrib_rows    = []\n",
    "platt_metrics_rows = []\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Zhang/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Zhang_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Zhang_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN embedding (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=custom_palette,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Zhang_dataset_Train,\n",
    "            train_df=Zhang_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Zhang/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Zhang_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Zhang_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST (filtered) for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Zhang_data_Test.loc[X_te_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # CAL split (filtered) for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Zhang_data_Cal.loc[X_cal_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(scaler.transform(df.values), index=df.index, columns=cols_train)\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Zhang\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions (unchanged)\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Zhang\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "    shap_df = shap_df[\n",
    "        [\"depth\", \"class_name\", \"dataset\", \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\", \"rank_within_class\"]\n",
    "    ]\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Zhang_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "    probs_dev.to_csv(probs_dir / \"probabilities_before_after_TEST.csv\", index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Zhang_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "    probs_release.to_csv(release_probs / \"Multiclass_models_probabilities_on_test.csv\", index=True)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report_df = pd.DataFrame(\n",
    "    classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    ").T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(cm_mc, index=pd.Index(class_names, name=\"true\"), columns=pd.Index(class_names, name=\"pred\"))\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(y_true_bin, score_raw, f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(y_true_bin, score_cal, f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw); m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw); per_class_rows.append(m_raw)\n",
    "    m_cal = _metrics_from_cm(cm_cal); m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal); per_class_rows.append(m_cal)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    per_class_df.to_csv(metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\", index=False)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\n✅ SIMPLIFIED PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# PATCHES ADDED (to address “plots missing / skipped” symptoms):\n",
    "#   (A) Optional DEBUG_DIAGNOSTICS: prints output paths + CAL class balance + confirms file writes.\n",
    "#   (B) Hard traceback on failures (instead of silent warnings) to surface root cause.\n",
    "#   (C) SHAP beeswarm robustification: optional subsample of TRAIN to avoid memory/time failures.\n",
    "#   (D) Optional SAFE_SINGLE_THREAD: mitigates fork/thread/numba/TBB instability during SHAP/plotting.\n",
    "#   (E) Explicit existence checks after savefig (so “saved but not where expected” is obvious).\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 0: IMPORTS + CONFIG\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\"Immature\": \"#0079ea\", \"Mature\": \"#AF3434\"}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    \"HSC_MPP\":            \"#0079ea\",\n",
    "    \"LMPP\":               \"#17BECF\",\n",
    "    \"GMP\":                \"#C5E4FF\",\n",
    "    \"Myeloid progenitor\": \"#AEC7E8\",\n",
    "    \"Monocyte\":           \"#D27CE3\",\n",
    "    \"CD14 Mono\":          \"#D27CE3\",\n",
    "    \"CD16 Mono\":          \"#8D43CD\",\n",
    "    \"Erythroblast\":       \"#F30A1A\",\n",
    "    \"ErP\":                \"#D1235A\",\n",
    "    \"MEP\":                \"#E364B0\",\n",
    "    \"CD4 T Naive\":        \"#C99546\",\n",
    "    \"CD4 T Memory\":       \"#C1AF93\",\n",
    "    \"CD8 T Naive\":        \"#4D382E\",\n",
    "    \"CD8 T Memory\":       \"#6B3317\",\n",
    "    \"Other_T\":            \"#EDB416\",\n",
    "    \"Treg\":               \"#6E6C37\",\n",
    "    \"B Naive\":            \"#1C511D\",\n",
    "    \"B Memory\":           \"#68D827\",\n",
    "    \"Pro-B\":              \"#66BB6A\",\n",
    "    \"Pre-B\":              \"#2DBD67\",\n",
    "    \"Immature B\":         \"#91FF7B\",\n",
    "    \"Plasma\":             \"#9DC012\",\n",
    "    \"cDC1\":               \"#76A7CB\",\n",
    "    \"cDC2\":               \"#16D2E3\",\n",
    "    \"pDC\":                \"#69FFCB\",\n",
    "    \"NK CD56 bright\":     \"#F3AC1F\",\n",
    "    \"NK CD56 dim\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_BY_DEPTH = {\n",
    "    \"Broad\": PALETTE_BROAD,\n",
    "    \"Simplified\": PALETTE_SIMPLIFIED,\n",
    "    \"Detailed\": PALETTE_DETAILED,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True\n",
    "EXPORT_DEV     = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Detailed\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "EXCLUDE_CLASSES = {}\n",
    "\n",
    "custom_palette = PALETTE_BY_DEPTH.get(name_target_class, {})\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DIAGNOSTICS / ROBUSTIFICATION SWITCHES (PATCH)\n",
    "# -----------------------------------------------------------------------------\n",
    "DEBUG_DIAGNOSTICS = True\n",
    "HARD_TRACEBACKS   = True   # if True: prints stack traces when plot/SHAP fails\n",
    "SHAP_TRAIN_SUBSAMPLE_MAX_N = 5000  # set None to disable subsampling\n",
    "SAFE_SINGLE_THREAD = False  # set True if you see Numba/TBB fork/thread warnings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "EMBEDDING_OBSM_KEY = \"X_umap\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "Zhang_root = Path(models_output)\n",
    "\n",
    "dev_root     = Zhang_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir       = models_root / \"heads\"\n",
    "metrics_dir     = reports_root / \"metrics\"\n",
    "probs_dir       = reports_root / \"probabilities\"\n",
    "fig_percls      = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root    = Zhang_root / \"Release\"\n",
    "release_models  = release_root / name_target_class / \"Models\"\n",
    "release_reports = release_root / name_target_class / \"Reports\"\n",
    "release_metrics = release_reports / \"Metrics\"\n",
    "release_probs   = release_reports / \"Probabilities\"\n",
    "release_imps    = release_reports / \"Importances\"\n",
    "release_figs    = release_root / name_target_class / \"Figures\"\n",
    "release_single  = release_figs / \"Single_classes\"\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "if DEBUG_DIAGNOSTICS:\n",
    "    print(f\"[DEBUG] HAS_SHAP={HAS_SHAP} EXPORT_RELEASE={EXPORT_RELEASE} EXPORT_DEV={EXPORT_DEV}\")\n",
    "    print(f\"[DEBUG] release_single={release_single}\")\n",
    "    print(f\"[DEBUG] release_imps={release_imps}\")\n",
    "    print(f\"[DEBUG] SAFE_SINGLE_THREAD={SAFE_SINGLE_THREAD} SHAP_SUBSAMPLE_MAX_N={SHAP_TRAIN_SUBSAMPLE_MAX_N}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "Zhang_data_Train = MLTraining.attach_celltype(Zhang_data_Train, Zhang_dataset_Train, consensus_field)\n",
    "Zhang_data_Test  = MLTraining.attach_celltype(Zhang_data_Test,  Zhang_dataset_Test,  consensus_field)\n",
    "Zhang_data_Cal   = MLTraining.attach_celltype(Zhang_data_Cal,   Zhang_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Zhang_data_Train = panel_intersection(rename_data_to_panel(Zhang_data_Train))\n",
    "Zhang_data_Test  = panel_intersection(rename_data_to_panel(Zhang_data_Test))\n",
    "Zhang_data_Cal   = panel_intersection(rename_data_to_panel(Zhang_data_Cal))\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS (WITH CAL/TEST ROW FILTERING)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Zhang_data_Cal_lbl = Zhang_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Cal.columns]\n",
    "\n",
    "Zhang_data_Train_Sub = Zhang_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Zhang_data_Test_Sub  = Zhang_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Zhang_data_Cal_Sub   = Zhang_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Zhang_data_Train_Sub.columns)\n",
    "if list(Zhang_data_Test_Sub.columns) != cols_train or list(Zhang_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Zhang_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Zhang_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Zhang_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "# classes learned from TRAIN, excluding user-specified\n",
    "all_classes = sorted(pd.Series(Zhang_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if str(c) not in EXCLUDE_CLASSES]\n",
    "\n",
    "excluded_present = sorted(set(all_classes).intersection(EXCLUDE_CLASSES))\n",
    "if excluded_present:\n",
    "    print(f\"  [INFO] Excluding {len(excluded_present)} classes: {excluded_present}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes after exclusions\")\n",
    "\n",
    "# ---- critical: filter CAL/TEST rows to those classes ----\n",
    "keep_set = set(map(str, class_names))\n",
    "\n",
    "cal_keep_mask  = Zhang_data_Cal_lbl[\"Celltype\"].astype(str).isin(keep_set)\n",
    "test_keep_mask = Zhang_data_Test[\"Celltype\"].astype(str).isin(keep_set)\n",
    "\n",
    "n_cal_drop  = int((~cal_keep_mask).sum())\n",
    "n_test_drop = int((~test_keep_mask).sum())\n",
    "\n",
    "if n_cal_drop > 0:\n",
    "    dropped = sorted(Zhang_data_Cal_lbl.loc[~cal_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_cal_drop} CAL rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "if n_test_drop > 0:\n",
    "    dropped = sorted(Zhang_data_Test.loc[~test_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_test_drop} TEST rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "# filtered label frames\n",
    "Zhang_data_Cal_lbl_f  = Zhang_data_Cal_lbl.loc[cal_keep_mask].copy()\n",
    "Zhang_data_Test_lbl_f = Zhang_data_Test.loc[test_keep_mask, [\"Celltype\"]].copy()\n",
    "\n",
    "# filtered feature frames (must align by index)\n",
    "X_cal_all_df = Zhang_data_Cal_Sub.loc[Zhang_data_Cal_lbl_f.index].copy()\n",
    "X_te_all_df  = Zhang_data_Test_Sub.loc[Zhang_data_Test_lbl_f.index].copy()\n",
    "test_index   = X_te_all_df.index\n",
    "\n",
    "# map filtered labels\n",
    "s_cal = Zhang_data_Cal_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Zhang_data_Cal_lbl_f.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in CAL after filtering: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Zhang_data_Test_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Zhang_data_Test_lbl_f.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in TEST after filtering: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# probability matrices sized to filtered CAL/TEST\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "xgb_shap_rows      = []\n",
    "lr_contrib_rows    = []\n",
    "platt_metrics_rows = []\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Zhang/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Zhang_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Zhang_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN embedding (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=custom_palette,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Zhang_dataset_Train,\n",
    "            train_df=Zhang_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Zhang/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Zhang_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Zhang_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST (filtered) for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Zhang_data_Test.loc[X_te_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # CAL split (filtered) for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Zhang_data_Cal.loc[X_cal_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(scaler.transform(df.values), index=df.index, columns=cols_train)\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Zhang\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions (unchanged)\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Zhang\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "    shap_df = shap_df[\n",
    "        [\"depth\", \"class_name\", \"dataset\", \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\", \"rank_within_class\"]\n",
    "    ]\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Zhang_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "    probs_dev.to_csv(probs_dir / \"probabilities_before_after_TEST.csv\", index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Zhang_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "    probs_release.to_csv(release_probs / \"Multiclass_models_probabilities_on_test.csv\", index=True)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report_df = pd.DataFrame(\n",
    "    classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    ").T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(cm_mc, index=pd.Index(class_names, name=\"true\"), columns=pd.Index(class_names, name=\"pred\"))\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(y_true_bin, score_raw, f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(y_true_bin, score_cal, f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw); m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw); per_class_rows.append(m_raw)\n",
    "    m_cal = _metrics_from_cm(cm_cal); m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal); per_class_rows.append(m_cal)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    per_class_df.to_csv(metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\", index=False)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\n✅ DETAILED PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triana Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "os.makedirs(data_path + \"/Triana\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Triana/Dev\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Triana/Release\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Triana/Dev/Models\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Triana/Release/Models\", exist_ok=True)\n",
    "\n",
    "models_output = data_path + \"/Triana\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triana_Models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broad annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# NOTE:\n",
    "#   Many helper functions are now provided by MLTraining.py.\n",
    "#   This script should primarily orchestrate: data prep -> model training loop -> exports.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\n",
    "    'Immature': \"#0079ea\", \n",
    "    'Mature': \"#AF3434\"\n",
    "}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    'HSC_MPP':            '#0079ea',\n",
    "    'LMPP':               \"#17BECF\",\n",
    "    'GMP':                \"#C5E4FF\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'Monocyte':           \"#D27CE3\",\n",
    "    'CD14 Mono':         \"#D27CE3\",\n",
    "    'CD16 Mono':         \"#8D43CD\",\n",
    "    'Erythroblast':      \"#F30A1A\",\n",
    "    'ErP':               \"#D1235A\",\n",
    "    'MEP':               \"#E364B0\",\n",
    "    'CD4 T Naive':       \"#C99546\",\n",
    "    'CD4 T Memory':      \"#C1AF93\",\n",
    "    'CD8 T Naive':       \"#4D382E\",\n",
    "    'CD8 T Memory':      \"#6B3317\",\n",
    "    'Other_T':           \"#EDB416\",\n",
    "    'Treg':              \"#6E6C37\",\n",
    "    'B Naive':          '#1C511D',\n",
    "    'B Memory':         \"#68D827\",\n",
    "    'Pro-B':            \"#66BB6A\",\n",
    "    'Pre-B':            \"#2DBD67\",\n",
    "    'Immature B':      \"#91FF7B\",\n",
    "    'Plasma':           \"#9DC012\",\n",
    "    'cDC1':             \"#76A7CB\",\n",
    "    'cDC2':             \"#16D2E3\",\n",
    "    'pDC':              \"#69FFCB\",\n",
    "    'NK CD56 bright':  \"#F3AC1F\",\n",
    "    'NK CD56 dim':     \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True    # set False to disable Release outputs\n",
    "EXPORT_DEV     = False   # set True to enable Dev outputs\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Broad\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Choose where to read the 2D embedding from.\n",
    "# Supported:\n",
    "#   - \"adata_obsm\": read from adata_train.obsm[obsm_key]\n",
    "#   - \"adata_obs\":  read from adata_train.obs[[obs_x, obs_y]]\n",
    "#   - \"train_df\":   read from train_df[[df_x, df_y]] (e.g., Triana_data_Train has UMAP columns)\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"adata_obsm\"\n",
    "EMBEDDING_OBSM_KEY = \"X_mofaumap\"     # e.g. \"X_umap\", \"X_pca\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"adata_obs\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"train_df\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "Triana_root = Path(models_output)\n",
    "\n",
    "dev_root     = Triana_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir    = models_root / \"heads\"\n",
    "metrics_dir  = reports_root / \"metrics\"\n",
    "probs_dir    = reports_root / \"probabilities\"\n",
    "fig_percls   = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root     = Triana_root / \"Release\"\n",
    "release_models   = release_root / name_target_class / \"Models\"\n",
    "release_reports  = release_root / name_target_class / \"Reports\"\n",
    "release_metrics  = release_reports / \"Metrics\"\n",
    "release_probs    = release_reports / \"Probabilities\"\n",
    "release_imps     = release_reports / \"Importances\"\n",
    "release_figs     = release_root / name_target_class / \"Figures\"\n",
    "release_single   = release_figs / \"Single_classes\"\n",
    "\n",
    "# Create directories conditionally\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] DEV Models:  {models_root}\")\n",
    "    print(f\"[INFO] DEV Reports: {reports_root}\")\n",
    "    print(f\"[INFO] DEV Figures: {fig_root}\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Triana_data_Train = MLTraining.attach_celltype(Triana_data_Train, Triana_dataset_Train, consensus_field)\n",
    "Triana_data_Test  = MLTraining.attach_celltype(Triana_data_Test,  Triana_dataset_Test,  consensus_field)\n",
    "Triana_data_Cal   = MLTraining.attach_celltype(Triana_data_Cal,   Triana_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Triana_data_Train = panel_intersection(rename_data_to_panel(Triana_data_Train))\n",
    "Triana_data_Test  = panel_intersection(rename_data_to_panel(Triana_data_Test))\n",
    "Triana_data_Cal   = panel_intersection(rename_data_to_panel(Triana_data_Cal))\n",
    "\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Triana_data_Cal_lbl = Triana_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Cal.columns]\n",
    "\n",
    "Triana_data_Train_Sub = Triana_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Triana_data_Test_Sub  = Triana_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Triana_data_Cal_Sub   = Triana_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Triana_data_Train_Sub.columns)\n",
    "if list(Triana_data_Test_Sub.columns) != cols_train or list(Triana_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Triana_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Triana_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Triana_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "class_names  = sorted(pd.Series(Triana_data_Train[\"Celltype\"]).dropna().unique())\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes\")\n",
    "\n",
    "s_cal = Triana_data_Cal_lbl[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Triana_data_Cal_lbl.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Triana_data_Test[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Triana_data_Test.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "X_cal_all_df = Triana_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Triana_data_Test_Sub.copy()\n",
    "test_index   = Triana_data_Test_Sub.index\n",
    "\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "# Importances collectors\n",
    "xgb_shap_rows = []       # mean_abs + corr (later filtered top10/class)\n",
    "lr_contrib_rows = []     # LR base learner contributions (from stacker_raw)\n",
    "platt_metrics_rows = []  # per-class logloss/brier pre vs post platt\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Triana/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Triana_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Triana_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN UMAP (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=PALETTE_BROAD,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Triana_dataset_Train,\n",
    "            train_df=Triana_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=(str(celltype) == \"Mature\"),\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Triana/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Triana_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Triana_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Triana_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # CAL split for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Triana_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Triana\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions: keep your existing helper for now if not moved\n",
    "    # If you have moved this helper into MLTraining.py, replace call accordingly.\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)  # existing in notebook\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Triana\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "# SHAP export (Top10/class; keep corr_feature_value_vs_shap)\n",
    "shap_df = None\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "\n",
    "    keep_cols = [\n",
    "        \"depth\", \"class_name\", \"dataset\",\n",
    "        \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\",\n",
    "        \"rank_within_class\",\n",
    "    ]\n",
    "    shap_df = shap_df[keep_cols]\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "# LR export\n",
    "lr_df = None\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Triana_data_Test[\"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "\n",
    "    probs_dev_path = probs_dir / \"probabilities_before_after_TEST.csv\"\n",
    "    probs_dev.to_csv(probs_dev_path, index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Triana_data_Test[\"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "\n",
    "    release_probs_path = release_probs / \"Multiclass_models_probabilities_on_test.csv\"\n",
    "    probs_release.to_csv(release_probs_path, index=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    "report_df = pd.DataFrame(report).T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(\n",
    "    cm_mc,\n",
    "    index=pd.Index(class_names, name=\"true\"),\n",
    "    columns=pd.Index(class_names, name=\"pred\"),\n",
    ")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        idx = pd.Index([\"True=Other\", f\"True={cls}\"], name=\"true\")\n",
    "        cols = pd.Index([\"Pred=Other\", f\"Pred={cls}\"], name=\"pred\")\n",
    "        pd.DataFrame(cm_raw, index=idx, columns=cols).to_csv(metrics_dir / f\"{cls_safe}_binary_confmat_TEST_ARGMAX_RAW.csv\")\n",
    "        pd.DataFrame(cm_cal, index=idx, columns=cols).to_csv(metrics_dir / f\"{cls_safe}_binary_confmat_TEST_ARGMAX_CAL.csv\")\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(\n",
    "        y_true_bin,\n",
    "        score_raw,\n",
    "        f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\",\n",
    "        dev_out,\n",
    "        rel_out,\n",
    "    )\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(\n",
    "        y_true_bin,\n",
    "        score_cal,\n",
    "        f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\",\n",
    "        dev_out,\n",
    "        rel_out,\n",
    "    )\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw)\n",
    "    m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw)\n",
    "    per_class_rows.append(m_raw)\n",
    "\n",
    "    m_cal = _metrics_from_cm(cm_cal)\n",
    "    m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal)\n",
    "    per_class_rows.append(m_cal)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    print(f\"  ✓ Saved per-class plots (DEV) → {fig_percls}\")\n",
    "if EXPORT_RELEASE:\n",
    "    print(f\"  ✓ Saved per-class plots (RELEASE) → {release_single}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    dev_metrics_path = metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\"\n",
    "    per_class_df.to_csv(dev_metrics_path, index=False)\n",
    "    print(f\"  ✓ Saved DEV per-class metrics → {dev_metrics_path}\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "    print(f\"  ✓ Saved RELEASE per-class metrics → {out_single}\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "    print(f\"  ✓ Appended DEV binary-head metrics → {dev_root / 'stacker_metrics.csv'}\")\n",
    "\n",
    "print(\"\\n✅ BROAD PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# PATCHES ADDED (to address “plots missing / skipped” symptoms):\n",
    "#   (A) Optional DEBUG_DIAGNOSTICS: prints output paths + CAL class balance + confirms file writes.\n",
    "#   (B) Hard traceback on failures (instead of silent warnings) to surface root cause.\n",
    "#   (C) SHAP beeswarm robustification: optional subsample of TRAIN to avoid memory/time failures.\n",
    "#   (D) Optional SAFE_SINGLE_THREAD: mitigates fork/thread/numba/TBB instability during SHAP/plotting.\n",
    "#   (E) Explicit existence checks after savefig (so “saved but not where expected” is obvious).\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 0: IMPORTS + CONFIG\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\"Immature\": \"#0079ea\", \"Mature\": \"#AF3434\"}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    \"HSC_MPP\":            \"#0079ea\",\n",
    "    \"LMPP\":               \"#17BECF\",\n",
    "    \"GMP\":                \"#C5E4FF\",\n",
    "    \"Myeloid progenitor\": \"#AEC7E8\",\n",
    "    \"Monocyte\":           \"#D27CE3\",\n",
    "    \"CD14 Mono\":          \"#D27CE3\",\n",
    "    \"CD16 Mono\":          \"#8D43CD\",\n",
    "    \"Erythroblast\":       \"#F30A1A\",\n",
    "    \"ErP\":                \"#D1235A\",\n",
    "    \"MEP\":                \"#E364B0\",\n",
    "    \"CD4 T Naive\":        \"#C99546\",\n",
    "    \"CD4 T Memory\":       \"#C1AF93\",\n",
    "    \"CD8 T Naive\":        \"#4D382E\",\n",
    "    \"CD8 T Memory\":       \"#6B3317\",\n",
    "    \"Other_T\":            \"#EDB416\",\n",
    "    \"Treg\":               \"#6E6C37\",\n",
    "    \"B Naive\":            \"#1C511D\",\n",
    "    \"B Memory\":           \"#68D827\",\n",
    "    \"Pro-B\":              \"#66BB6A\",\n",
    "    \"Pre-B\":              \"#2DBD67\",\n",
    "    \"Immature B\":         \"#91FF7B\",\n",
    "    \"Plasma\":             \"#9DC012\",\n",
    "    \"cDC1\":               \"#76A7CB\",\n",
    "    \"cDC2\":               \"#16D2E3\",\n",
    "    \"pDC\":                \"#69FFCB\",\n",
    "    \"NK CD56 bright\":     \"#F3AC1F\",\n",
    "    \"NK CD56 dim\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_BY_DEPTH = {\n",
    "    \"Broad\": PALETTE_BROAD,\n",
    "    \"Simplified\": PALETTE_SIMPLIFIED,\n",
    "    \"Detailed\": PALETTE_DETAILED,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True\n",
    "EXPORT_DEV     = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Simplified\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "EXCLUDE_CLASSES = {}\n",
    "\n",
    "custom_palette = PALETTE_BY_DEPTH.get(name_target_class, {})\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DIAGNOSTICS / ROBUSTIFICATION SWITCHES (PATCH)\n",
    "# -----------------------------------------------------------------------------\n",
    "DEBUG_DIAGNOSTICS = True\n",
    "HARD_TRACEBACKS   = True   # if True: prints stack traces when plot/SHAP fails\n",
    "SHAP_TRAIN_SUBSAMPLE_MAX_N = 5000  # set None to disable subsampling\n",
    "SAFE_SINGLE_THREAD = False  # set True if you see Numba/TBB fork/thread warnings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "EMBEDDING_OBSM_KEY = \"X_mofaumap\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "Triana_root = Path(models_output)\n",
    "\n",
    "dev_root     = Triana_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir       = models_root / \"heads\"\n",
    "metrics_dir     = reports_root / \"metrics\"\n",
    "probs_dir       = reports_root / \"probabilities\"\n",
    "fig_percls      = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root    = Triana_root / \"Release\"\n",
    "release_models  = release_root / name_target_class / \"Models\"\n",
    "release_reports = release_root / name_target_class / \"Reports\"\n",
    "release_metrics = release_reports / \"Metrics\"\n",
    "release_probs   = release_reports / \"Probabilities\"\n",
    "release_imps    = release_reports / \"Importances\"\n",
    "release_figs    = release_root / name_target_class / \"Figures\"\n",
    "release_single  = release_figs / \"Single_classes\"\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "if DEBUG_DIAGNOSTICS:\n",
    "    print(f\"[DEBUG] HAS_SHAP={HAS_SHAP} EXPORT_RELEASE={EXPORT_RELEASE} EXPORT_DEV={EXPORT_DEV}\")\n",
    "    print(f\"[DEBUG] release_single={release_single}\")\n",
    "    print(f\"[DEBUG] release_imps={release_imps}\")\n",
    "    print(f\"[DEBUG] SAFE_SINGLE_THREAD={SAFE_SINGLE_THREAD} SHAP_SUBSAMPLE_MAX_N={SHAP_TRAIN_SUBSAMPLE_MAX_N}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "Triana_data_Train = MLTraining.attach_celltype(Triana_data_Train, Triana_dataset_Train, consensus_field)\n",
    "Triana_data_Test  = MLTraining.attach_celltype(Triana_data_Test,  Triana_dataset_Test,  consensus_field)\n",
    "Triana_data_Cal   = MLTraining.attach_celltype(Triana_data_Cal,   Triana_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Triana_data_Train = panel_intersection(rename_data_to_panel(Triana_data_Train))\n",
    "Triana_data_Test  = panel_intersection(rename_data_to_panel(Triana_data_Test))\n",
    "Triana_data_Cal   = panel_intersection(rename_data_to_panel(Triana_data_Cal))\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS (WITH CAL/TEST ROW FILTERING)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Triana_data_Cal_lbl = Triana_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Cal.columns]\n",
    "\n",
    "Triana_data_Train_Sub = Triana_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Triana_data_Test_Sub  = Triana_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Triana_data_Cal_Sub   = Triana_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Triana_data_Train_Sub.columns)\n",
    "if list(Triana_data_Test_Sub.columns) != cols_train or list(Triana_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Triana_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Triana_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Triana_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "# classes learned from TRAIN, excluding user-specified\n",
    "all_classes = sorted(pd.Series(Triana_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if str(c) not in EXCLUDE_CLASSES]\n",
    "\n",
    "excluded_present = sorted(set(all_classes).intersection(EXCLUDE_CLASSES))\n",
    "if excluded_present:\n",
    "    print(f\"  [INFO] Excluding {len(excluded_present)} classes: {excluded_present}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes after exclusions\")\n",
    "\n",
    "# ---- critical: filter CAL/TEST rows to those classes ----\n",
    "keep_set = set(map(str, class_names))\n",
    "\n",
    "cal_keep_mask  = Triana_data_Cal_lbl[\"Celltype\"].astype(str).isin(keep_set)\n",
    "test_keep_mask = Triana_data_Test[\"Celltype\"].astype(str).isin(keep_set)\n",
    "\n",
    "n_cal_drop  = int((~cal_keep_mask).sum())\n",
    "n_test_drop = int((~test_keep_mask).sum())\n",
    "\n",
    "if n_cal_drop > 0:\n",
    "    dropped = sorted(Triana_data_Cal_lbl.loc[~cal_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_cal_drop} CAL rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "if n_test_drop > 0:\n",
    "    dropped = sorted(Triana_data_Test.loc[~test_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_test_drop} TEST rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "# filtered label frames\n",
    "Triana_data_Cal_lbl_f  = Triana_data_Cal_lbl.loc[cal_keep_mask].copy()\n",
    "Triana_data_Test_lbl_f = Triana_data_Test.loc[test_keep_mask, [\"Celltype\"]].copy()\n",
    "\n",
    "# filtered feature frames (must align by index)\n",
    "X_cal_all_df = Triana_data_Cal_Sub.loc[Triana_data_Cal_lbl_f.index].copy()\n",
    "X_te_all_df  = Triana_data_Test_Sub.loc[Triana_data_Test_lbl_f.index].copy()\n",
    "test_index   = X_te_all_df.index\n",
    "\n",
    "# map filtered labels\n",
    "s_cal = Triana_data_Cal_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Triana_data_Cal_lbl_f.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in CAL after filtering: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Triana_data_Test_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Triana_data_Test_lbl_f.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in TEST after filtering: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# probability matrices sized to filtered CAL/TEST\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "xgb_shap_rows      = []\n",
    "lr_contrib_rows    = []\n",
    "platt_metrics_rows = []\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Triana/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Triana_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Triana_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN embedding (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=custom_palette,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Triana_dataset_Train,\n",
    "            train_df=Triana_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Triana/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Triana_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Triana_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST (filtered) for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Triana_data_Test.loc[X_te_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # CAL split (filtered) for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Triana_data_Cal.loc[X_cal_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(scaler.transform(df.values), index=df.index, columns=cols_train)\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Triana\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions (unchanged)\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Triana\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "    shap_df = shap_df[\n",
    "        [\"depth\", \"class_name\", \"dataset\", \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\", \"rank_within_class\"]\n",
    "    ]\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Triana_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "    probs_dev.to_csv(probs_dir / \"probabilities_before_after_TEST.csv\", index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Triana_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "    probs_release.to_csv(release_probs / \"Multiclass_models_probabilities_on_test.csv\", index=True)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report_df = pd.DataFrame(\n",
    "    classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    ").T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(cm_mc, index=pd.Index(class_names, name=\"true\"), columns=pd.Index(class_names, name=\"pred\"))\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(y_true_bin, score_raw, f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(y_true_bin, score_cal, f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw); m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw); per_class_rows.append(m_raw)\n",
    "    m_cal = _metrics_from_cm(cm_cal); m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal); per_class_rows.append(m_cal)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    per_class_df.to_csv(metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\", index=False)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\n✅ SIMPLIFIED PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc_context\n",
    "\n",
    "# --------- 1) Collect training barcodes (union across classes or a single class) ---------\n",
    "def collect_training_barcodes(train_barcodes_path: str | Path,\n",
    "                              atlas: str = \"Triana\",\n",
    "                              depth: str = \"simplified\",\n",
    "                              class_name: str | None = None) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Reads CSVs like:\n",
    "      {train_barcodes_path}/{atlas}/Consensus_annotation_{depth}_final/Barcodes_training_class_{Class}.csv\n",
    "    and returns the union of 'Positive' and 'Negative' barcodes.\n",
    "    \"\"\"\n",
    "    base = Path(train_barcodes_path) / atlas / f\"Consensus_annotation_{depth.lower()}_final\"\n",
    "\n",
    "    if class_name is None:\n",
    "        files = sorted(base.glob(\"Barcodes_training_class_*.csv\"))\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No training CSVs found in: {base}\")\n",
    "    else:\n",
    "        files = [base / f\"Barcodes_training_class_{str(class_name).replace(' ', '_')}.csv\"]\n",
    "        if not files[0].exists():\n",
    "            raise FileNotFoundError(f\"Training CSV not found: {files[0]}\")\n",
    "\n",
    "    barcodes = []\n",
    "    for fp in files:\n",
    "        df = pd.read_csv(fp, index_col=0)\n",
    "        for col in (\"Positive\", \"Negative\"):\n",
    "            if col in df.columns:\n",
    "                barcodes.extend(df[col].dropna().astype(str).tolist())\n",
    "\n",
    "    return pd.Index(pd.unique(barcodes))\n",
    "\n",
    "\n",
    "# --------- 2) Plot UMAP for the subset of training barcodes ---------\n",
    "def plot_training_subset_umap(\n",
    "    adata,                                 # Triana_dataset_Train (AnnData)\n",
    "    train_barcodes_path: str | Path,\n",
    "    *,\n",
    "    atlas: str = \"Triana\",\n",
    "    depth: str = \"simplified\",             # matches your CSV folder name\n",
    "    class_name: str | None = None,         # None -> all classes; or \"B Memory\", etc.\n",
    "    label_key: str = \"Consensus_annotation_detailed_final\",\n",
    "    basis_key: str = \"X_mofaumap\",\n",
    "    custom_palette: dict | None = None,    # your dict {label: \"#hex\"}\n",
    "    point_size: float = 10.0,\n",
    "    title: str | None = None,\n",
    "):\n",
    "    # 1) get training barcodes\n",
    "    keep_barcodes = collect_training_barcodes(train_barcodes_path, atlas=atlas, depth=depth, class_name=class_name)\n",
    "\n",
    "    # 2) subset AnnData\n",
    "    n_before = adata.n_obs\n",
    "    mask = adata.obs_names.isin(keep_barcodes)\n",
    "    n_after = int(mask.sum())\n",
    "    if n_after == 0:\n",
    "        raise ValueError(\"No training barcodes overlapped with adata.obs_names.\")\n",
    "    ad_sub = adata[mask].copy()\n",
    "    print(f\"[subset] kept {n_after}/{n_before} cells for plotting \"\n",
    "          f\"({'ALL classes' if class_name is None else f'class={class_name}'})\")\n",
    "\n",
    "    # 3) ensure categorical for coloring\n",
    "    if not pd.api.types.is_categorical_dtype(ad_sub.obs[label_key]):\n",
    "        ad_sub.obs[label_key] = ad_sub.obs[label_key].astype(\"category\")\n",
    "    cats = list(ad_sub.obs[label_key].cat.categories)\n",
    "\n",
    "    # 4) palette handling (use your custom palette where available; fallback to grey)\n",
    "    fallback = \"#cccccc\"\n",
    "    if custom_palette is None:\n",
    "        palette_list = None  # let scanpy pick\n",
    "    else:\n",
    "        labels_in_palette = set(custom_palette.keys())\n",
    "        labels_in_data = set(cats)\n",
    "        missing_in_palette = [c for c in cats if c not in labels_in_palette]\n",
    "        extra_in_palette   = [c for c in custom_palette.keys() if c not in labels_in_data]\n",
    "        if missing_in_palette:\n",
    "            print(\"[WARN] Missing colors for:\", missing_in_palette, \"-> using light grey (#cccccc).\")\n",
    "        if extra_in_palette:\n",
    "            print(\"[INFO] Palette has unused entries:\", extra_in_palette)\n",
    "\n",
    "        palette_list = [custom_palette.get(c, fallback) for c in cats]\n",
    "        # stash into .uns so scanpy reuses it\n",
    "        ad_sub.uns[f\"{label_key}_colors\"] = palette_list\n",
    "\n",
    "    # 5) plot\n",
    "    with rc_context({\"figure.figsize\": (5.75, 4.75)}):\n",
    "        sc.pl.embedding(\n",
    "            ad_sub,\n",
    "            basis=basis_key,\n",
    "            color=label_key,\n",
    "            palette=palette_list,\n",
    "            legend_loc=\"on data\",\n",
    "            legend_fontsize=10,\n",
    "            legend_fontoutline=1.5,\n",
    "            size=point_size,\n",
    "            add_outline=True,\n",
    "            frameon=False,\n",
    "            title=title or f\"Triana — training subset ({'all classes' if class_name is None else class_name})\",\n",
    "            show=True,\n",
    "        )\n",
    "\n",
    "\n",
    "# ---------------------- EXAMPLES ----------------------\n",
    "# 1) Plot ALL training barcodes (union of every class) with your detailed labels + palette\n",
    "# plot_training_subset_umap(\n",
    "#     Triana_dataset_Train,\n",
    "#     train_barcodes_path=train_barcodes_path,\n",
    "#     atlas=\"Triana\",\n",
    "#     depth=\"simplified\",\n",
    "#     class_name=None,\n",
    "#     label_key=\"Consensus_annotation_detailed_final\",\n",
    "#     basis_key=\"X_mofaumap\",\n",
    "#     custom_palette=custom_palette,  # pass the dict you defined\n",
    "#     point_size=10,\n",
    "#     title=\"Triana — all training cells\",\n",
    "# )\n",
    "\n",
    "# 2) Plot only one class’s training barcodes (e.g., \"B Memory\")\n",
    "plot_training_subset_umap(\n",
    "    Triana_dataset_Train,\n",
    "    train_barcodes_path=train_barcodes_path,\n",
    "    atlas=\"Triana\",\n",
    "    depth=\"simplified\",\n",
    "    class_name=\"Erythroid\",\n",
    "    label_key=\"Consensus_annotation_detailed_final\",\n",
    "    basis_key=\"X_mofaumap\",\n",
    "    custom_palette=custom_palette,\n",
    "    point_size=10,\n",
    "    title=\"Triana — training cells: B Memory\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Triana_data_Train, Triana_data_Test, Triana_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Triana_dataset_Train, Triana_dataset_Test, Triana_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Simplified\"   # \"simplified\" | \"Simplified\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Triana_data_Train = attach_celltype(Triana_data_Train, Triana_dataset_Train, consensus_field)\n",
    "Triana_data_Test  = attach_celltype(Triana_data_Test,  Triana_dataset_Test,  consensus_field)\n",
    "Triana_data_Cal   = attach_celltype(Triana_data_Cal,   Triana_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns → same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Triana_data_Train = _rename_data_to_panel(Triana_data_Train)\n",
    "Triana_data_Test  = _rename_data_to_panel(Triana_data_Test)\n",
    "Triana_data_Cal   = _rename_data_to_panel(Triana_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Triana_data_Train = _panel_intersection(Triana_data_Train)\n",
    "Triana_data_Test  = _panel_intersection(Triana_data_Test)\n",
    "Triana_data_Cal   = _panel_intersection(Triana_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Triana_data_Cal_lbl = Triana_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Cal.columns]\n",
    "\n",
    "Triana_data_Train_Sub = Triana_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Triana_data_Test_Sub  = Triana_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Triana_data_Cal_Sub   = Triana_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Triana_data_Train_Sub.columns)\n",
    "if list(Triana_data_Test_Sub.columns) != cols_train or list(Triana_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Triana_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Triana_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Triana_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# Consistent class order\n",
    "class_names  = sorted(pd.Series(Triana_data_Train[\"Celltype\"]).dropna().unique())\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# Multiclass labels arrays\n",
    "s_cal = Triana_data_Cal_lbl[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Triana_data_Cal_lbl.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Triana_data_Test[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Triana_data_Test.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Triana_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Triana_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Triana_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Triana/Consensus_annotation_simplified_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Triana_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Triana_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Triana/Consensus_annotation_simplified_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Triana_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Triana_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Triana_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Triana_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration – {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) – {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Triana\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal, y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te,    index=test_index, columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc, index=test_index, columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Triana_data_Test[\"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te.argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc.argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS):\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation\n",
    "y_pred_mc = P_te_mc.argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# PATCHES ADDED (to address “plots missing / skipped” symptoms):\n",
    "#   (A) Optional DEBUG_DIAGNOSTICS: prints output paths + CAL class balance + confirms file writes.\n",
    "#   (B) Hard traceback on failures (instead of silent warnings) to surface root cause.\n",
    "#   (C) SHAP beeswarm robustification: optional subsample of TRAIN to avoid memory/time failures.\n",
    "#   (D) Optional SAFE_SINGLE_THREAD: mitigates fork/thread/numba/TBB instability during SHAP/plotting.\n",
    "#   (E) Explicit existence checks after savefig (so “saved but not where expected” is obvious).\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 0: IMPORTS + CONFIG\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\"Immature\": \"#0079ea\", \"Mature\": \"#AF3434\"}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    \"HSC_MPP\":            \"#0079ea\",\n",
    "    \"LMPP\":               \"#17BECF\",\n",
    "    \"GMP\":                \"#C5E4FF\",\n",
    "    \"Myeloid progenitor\": \"#AEC7E8\",\n",
    "    \"Monocyte\":           \"#D27CE3\",\n",
    "    \"CD14 Mono\":          \"#D27CE3\",\n",
    "    \"CD16 Mono\":          \"#8D43CD\",\n",
    "    \"Erythroblast\":       \"#F30A1A\",\n",
    "    \"ErP\":                \"#D1235A\",\n",
    "    \"MEP\":                \"#E364B0\",\n",
    "    \"CD4 T Naive\":        \"#C99546\",\n",
    "    \"CD4 T Memory\":       \"#C1AF93\",\n",
    "    \"CD8 T Naive\":        \"#4D382E\",\n",
    "    \"CD8 T Memory\":       \"#6B3317\",\n",
    "    \"Other_T\":            \"#EDB416\",\n",
    "    \"Treg\":               \"#6E6C37\",\n",
    "    \"B Naive\":            \"#1C511D\",\n",
    "    \"B Memory\":           \"#68D827\",\n",
    "    \"Pro-B\":              \"#66BB6A\",\n",
    "    \"Pre-B\":              \"#2DBD67\",\n",
    "    \"Immature B\":         \"#91FF7B\",\n",
    "    \"Plasma\":             \"#9DC012\",\n",
    "    \"cDC1\":               \"#76A7CB\",\n",
    "    \"cDC2\":               \"#16D2E3\",\n",
    "    \"pDC\":                \"#69FFCB\",\n",
    "    \"NK CD56 bright\":     \"#F3AC1F\",\n",
    "    \"NK CD56 dim\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_BY_DEPTH = {\n",
    "    \"Broad\": PALETTE_BROAD,\n",
    "    \"Simplified\": PALETTE_SIMPLIFIED,\n",
    "    \"Detailed\": PALETTE_DETAILED,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True\n",
    "EXPORT_DEV     = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Detailed\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "EXCLUDE_CLASSES = {}\n",
    "\n",
    "custom_palette = PALETTE_BY_DEPTH.get(name_target_class, {})\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DIAGNOSTICS / ROBUSTIFICATION SWITCHES (PATCH)\n",
    "# -----------------------------------------------------------------------------\n",
    "DEBUG_DIAGNOSTICS = True\n",
    "HARD_TRACEBACKS   = True   # if True: prints stack traces when plot/SHAP fails\n",
    "SHAP_TRAIN_SUBSAMPLE_MAX_N = 5000  # set None to disable subsampling\n",
    "SAFE_SINGLE_THREAD = False  # set True if you see Numba/TBB fork/thread warnings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "EMBEDDING_OBSM_KEY = \"X_mofaumap\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "Triana_root = Path(models_output)\n",
    "\n",
    "dev_root     = Triana_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir       = models_root / \"heads\"\n",
    "metrics_dir     = reports_root / \"metrics\"\n",
    "probs_dir       = reports_root / \"probabilities\"\n",
    "fig_percls      = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root    = Triana_root / \"Release\"\n",
    "release_models  = release_root / name_target_class / \"Models\"\n",
    "release_reports = release_root / name_target_class / \"Reports\"\n",
    "release_metrics = release_reports / \"Metrics\"\n",
    "release_probs   = release_reports / \"Probabilities\"\n",
    "release_imps    = release_reports / \"Importances\"\n",
    "release_figs    = release_root / name_target_class / \"Figures\"\n",
    "release_single  = release_figs / \"Single_classes\"\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "if DEBUG_DIAGNOSTICS:\n",
    "    print(f\"[DEBUG] HAS_SHAP={HAS_SHAP} EXPORT_RELEASE={EXPORT_RELEASE} EXPORT_DEV={EXPORT_DEV}\")\n",
    "    print(f\"[DEBUG] release_single={release_single}\")\n",
    "    print(f\"[DEBUG] release_imps={release_imps}\")\n",
    "    print(f\"[DEBUG] SAFE_SINGLE_THREAD={SAFE_SINGLE_THREAD} SHAP_SUBSAMPLE_MAX_N={SHAP_TRAIN_SUBSAMPLE_MAX_N}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "Triana_data_Train = MLTraining.attach_celltype(Triana_data_Train, Triana_dataset_Train, consensus_field)\n",
    "Triana_data_Test  = MLTraining.attach_celltype(Triana_data_Test,  Triana_dataset_Test,  consensus_field)\n",
    "Triana_data_Cal   = MLTraining.attach_celltype(Triana_data_Cal,   Triana_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Triana_data_Train = panel_intersection(rename_data_to_panel(Triana_data_Train))\n",
    "Triana_data_Test  = panel_intersection(rename_data_to_panel(Triana_data_Test))\n",
    "Triana_data_Cal   = panel_intersection(rename_data_to_panel(Triana_data_Cal))\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS (WITH CAL/TEST ROW FILTERING)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Triana_data_Cal_lbl = Triana_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Cal.columns]\n",
    "\n",
    "Triana_data_Train_Sub = Triana_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Triana_data_Test_Sub  = Triana_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Triana_data_Cal_Sub   = Triana_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Triana_data_Train_Sub.columns)\n",
    "if list(Triana_data_Test_Sub.columns) != cols_train or list(Triana_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Triana_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Triana_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Triana_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "# classes learned from TRAIN, excluding user-specified\n",
    "all_classes = sorted(pd.Series(Triana_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if str(c) not in EXCLUDE_CLASSES]\n",
    "\n",
    "excluded_present = sorted(set(all_classes).intersection(EXCLUDE_CLASSES))\n",
    "if excluded_present:\n",
    "    print(f\"  [INFO] Excluding {len(excluded_present)} classes: {excluded_present}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes after exclusions\")\n",
    "\n",
    "# ---- critical: filter CAL/TEST rows to those classes ----\n",
    "keep_set = set(map(str, class_names))\n",
    "\n",
    "cal_keep_mask  = Triana_data_Cal_lbl[\"Celltype\"].astype(str).isin(keep_set)\n",
    "test_keep_mask = Triana_data_Test[\"Celltype\"].astype(str).isin(keep_set)\n",
    "\n",
    "n_cal_drop  = int((~cal_keep_mask).sum())\n",
    "n_test_drop = int((~test_keep_mask).sum())\n",
    "\n",
    "if n_cal_drop > 0:\n",
    "    dropped = sorted(Triana_data_Cal_lbl.loc[~cal_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_cal_drop} CAL rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "if n_test_drop > 0:\n",
    "    dropped = sorted(Triana_data_Test.loc[~test_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_test_drop} TEST rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "# filtered label frames\n",
    "Triana_data_Cal_lbl_f  = Triana_data_Cal_lbl.loc[cal_keep_mask].copy()\n",
    "Triana_data_Test_lbl_f = Triana_data_Test.loc[test_keep_mask, [\"Celltype\"]].copy()\n",
    "\n",
    "# filtered feature frames (must align by index)\n",
    "X_cal_all_df = Triana_data_Cal_Sub.loc[Triana_data_Cal_lbl_f.index].copy()\n",
    "X_te_all_df  = Triana_data_Test_Sub.loc[Triana_data_Test_lbl_f.index].copy()\n",
    "test_index   = X_te_all_df.index\n",
    "\n",
    "# map filtered labels\n",
    "s_cal = Triana_data_Cal_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Triana_data_Cal_lbl_f.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in CAL after filtering: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Triana_data_Test_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Triana_data_Test_lbl_f.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in TEST after filtering: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# probability matrices sized to filtered CAL/TEST\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "xgb_shap_rows      = []\n",
    "lr_contrib_rows    = []\n",
    "platt_metrics_rows = []\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Triana/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Triana_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Triana_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN embedding (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=custom_palette,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Triana_dataset_Train,\n",
    "            train_df=Triana_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Triana/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Triana_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Triana_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST (filtered) for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Triana_data_Test.loc[X_te_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # CAL split (filtered) for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Triana_data_Cal.loc[X_cal_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(scaler.transform(df.values), index=df.index, columns=cols_train)\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Triana\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions (unchanged)\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Triana\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "    shap_df = shap_df[\n",
    "        [\"depth\", \"class_name\", \"dataset\", \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\", \"rank_within_class\"]\n",
    "    ]\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Triana_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "    probs_dev.to_csv(probs_dir / \"probabilities_before_after_TEST.csv\", index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Triana_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "    probs_release.to_csv(release_probs / \"Multiclass_models_probabilities_on_test.csv\", index=True)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report_df = pd.DataFrame(\n",
    "    classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    ").T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(cm_mc, index=pd.Index(class_names, name=\"true\"), columns=pd.Index(class_names, name=\"pred\"))\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(y_true_bin, score_raw, f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(y_true_bin, score_cal, f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw); m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw); per_class_rows.append(m_raw)\n",
    "    m_cal = _metrics_from_cm(cm_cal); m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal); per_class_rows.append(m_cal)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    per_class_df.to_csv(metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\", index=False)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\n✅ DETAILED PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luecken Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "os.makedirs(data_path + \"/Luecken\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Luecken/Dev\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Luecken/Release\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Luecken/Dev/Models\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Luecken/Release/Models\", exist_ok=True)\n",
    "\n",
    "models_output = data_path + \"/Luecken\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luecken_Models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broad annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# NOTE:\n",
    "#   Many helper functions are now provided by MLTraining.py.\n",
    "#   This script should primarily orchestrate: data prep -> model training loop -> exports.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\n",
    "    'Immature': \"#0079ea\", \n",
    "    'Mature': \"#AF3434\"\n",
    "}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    'HSC_MPP':            '#0079ea',\n",
    "    'LMPP':               \"#17BECF\",\n",
    "    'GMP':                \"#C5E4FF\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'Monocyte':           \"#D27CE3\",\n",
    "    'CD14 Mono':         \"#D27CE3\",\n",
    "    'CD16 Mono':         \"#8D43CD\",\n",
    "    'Erythroblast':      \"#F30A1A\",\n",
    "    'ErP':               \"#D1235A\",\n",
    "    'MEP':               \"#E364B0\",\n",
    "    'CD4 T Naive':       \"#C99546\",\n",
    "    'CD4 T Memory':      \"#C1AF93\",\n",
    "    'CD8 T Naive':       \"#4D382E\",\n",
    "    'CD8 T Memory':      \"#6B3317\",\n",
    "    'Other_T':           \"#EDB416\",\n",
    "    'Treg':              \"#6E6C37\",\n",
    "    'B Naive':          '#1C511D',\n",
    "    'B Memory':         \"#68D827\",\n",
    "    'Pro-B':            \"#66BB6A\",\n",
    "    'Pre-B':            \"#2DBD67\",\n",
    "    'Immature B':      \"#91FF7B\",\n",
    "    'Plasma':           \"#9DC012\",\n",
    "    'cDC1':             \"#76A7CB\",\n",
    "    'cDC2':             \"#16D2E3\",\n",
    "    'pDC':              \"#69FFCB\",\n",
    "    'NK CD56 bright':  \"#F3AC1F\",\n",
    "    'NK CD56 dim':     \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True    # set False to disable Release outputs\n",
    "EXPORT_DEV     = False   # set True to enable Dev outputs\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Broad\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Choose where to read the 2D embedding from.\n",
    "# Supported:\n",
    "#   - \"adata_obsm\": read from adata_train.obsm[obsm_key]\n",
    "#   - \"adata_obs\":  read from adata_train.obs[[obs_x, obs_y]]\n",
    "#   - \"train_df\":   read from train_df[[df_x, df_y]] (e.g., Luecken_data_Train has UMAP columns)\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"adata_obsm\"\n",
    "EMBEDDING_OBSM_KEY = \"X_umap\"     # e.g. \"X_umap\", \"X_pca\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"adata_obs\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "\n",
    "# If EMBEDDING_SOURCE == \"train_df\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "Luecken_root = Path(models_output)\n",
    "\n",
    "dev_root     = Luecken_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir    = models_root / \"heads\"\n",
    "metrics_dir  = reports_root / \"metrics\"\n",
    "probs_dir    = reports_root / \"probabilities\"\n",
    "fig_percls   = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root     = Luecken_root / \"Release\"\n",
    "release_models   = release_root / name_target_class / \"Models\"\n",
    "release_reports  = release_root / name_target_class / \"Reports\"\n",
    "release_metrics  = release_reports / \"Metrics\"\n",
    "release_probs    = release_reports / \"Probabilities\"\n",
    "release_imps     = release_reports / \"Importances\"\n",
    "release_figs     = release_root / name_target_class / \"Figures\"\n",
    "release_single   = release_figs / \"Single_classes\"\n",
    "\n",
    "# Create directories conditionally\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] DEV Models:  {models_root}\")\n",
    "    print(f\"[INFO] DEV Reports: {reports_root}\")\n",
    "    print(f\"[INFO] DEV Figures: {fig_root}\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Luecken_data_Train = MLTraining.attach_celltype(Luecken_data_Train, Luecken_dataset_Train, consensus_field)\n",
    "Luecken_data_Test  = MLTraining.attach_celltype(Luecken_data_Test,  Luecken_dataset_Test,  consensus_field)\n",
    "Luecken_data_Cal   = MLTraining.attach_celltype(Luecken_data_Cal,   Luecken_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Luecken_data_Train = panel_intersection(rename_data_to_panel(Luecken_data_Train))\n",
    "Luecken_data_Test  = panel_intersection(rename_data_to_panel(Luecken_data_Test))\n",
    "Luecken_data_Cal   = panel_intersection(rename_data_to_panel(Luecken_data_Cal))\n",
    "\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Luecken_data_Cal_lbl = Luecken_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Cal.columns]\n",
    "\n",
    "Luecken_data_Train_Sub = Luecken_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Luecken_data_Test_Sub  = Luecken_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Luecken_data_Cal_Sub   = Luecken_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Luecken_data_Train_Sub.columns)\n",
    "if list(Luecken_data_Test_Sub.columns) != cols_train or list(Luecken_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Luecken_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Luecken_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Luecken_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "class_names  = sorted(pd.Series(Luecken_data_Train[\"Celltype\"]).dropna().unique())\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes\")\n",
    "\n",
    "s_cal = Luecken_data_Cal_lbl[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Luecken_data_Cal_lbl.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Luecken_data_Test[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Luecken_data_Test.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "X_cal_all_df = Luecken_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Luecken_data_Test_Sub.copy()\n",
    "test_index   = Luecken_data_Test_Sub.index\n",
    "\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "# Importances collectors\n",
    "xgb_shap_rows = []       # mean_abs + corr (later filtered top10/class)\n",
    "lr_contrib_rows = []     # LR base learner contributions (from stacker_raw)\n",
    "platt_metrics_rows = []  # per-class logloss/brier pre vs post platt\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Luecken/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Luecken_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Luecken_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN UMAP (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=PALETTE_BROAD,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Luecken_dataset_Train,\n",
    "            train_df=Luecken_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=(str(celltype) == \"Mature\"),\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Luecken/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Luecken_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Luecken_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Luecken_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # CAL split for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Luecken_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Luecken\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions: keep your existing helper for now if not moved\n",
    "    # If you have moved this helper into MLTraining.py, replace call accordingly.\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)  # existing in notebook\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Luecken\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "# SHAP export (Top10/class; keep corr_feature_value_vs_shap)\n",
    "shap_df = None\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "\n",
    "    keep_cols = [\n",
    "        \"depth\", \"class_name\", \"dataset\",\n",
    "        \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\",\n",
    "        \"rank_within_class\",\n",
    "    ]\n",
    "    shap_df = shap_df[keep_cols]\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "# LR export\n",
    "lr_df = None\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Luecken_data_Test[\"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "\n",
    "    probs_dev_path = probs_dir / \"probabilities_before_after_TEST.csv\"\n",
    "    probs_dev.to_csv(probs_dev_path, index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Luecken_data_Test[\"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "\n",
    "    release_probs_path = release_probs / \"Multiclass_models_probabilities_on_test.csv\"\n",
    "    probs_release.to_csv(release_probs_path, index=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    "report_df = pd.DataFrame(report).T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(\n",
    "    cm_mc,\n",
    "    index=pd.Index(class_names, name=\"true\"),\n",
    "    columns=pd.Index(class_names, name=\"pred\"),\n",
    ")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        idx = pd.Index([\"True=Other\", f\"True={cls}\"], name=\"true\")\n",
    "        cols = pd.Index([\"Pred=Other\", f\"Pred={cls}\"], name=\"pred\")\n",
    "        pd.DataFrame(cm_raw, index=idx, columns=cols).to_csv(metrics_dir / f\"{cls_safe}_binary_confmat_TEST_ARGMAX_RAW.csv\")\n",
    "        pd.DataFrame(cm_cal, index=idx, columns=cols).to_csv(metrics_dir / f\"{cls_safe}_binary_confmat_TEST_ARGMAX_CAL.csv\")\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(\n",
    "        y_true_bin,\n",
    "        score_raw,\n",
    "        f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\",\n",
    "        dev_out,\n",
    "        rel_out,\n",
    "    )\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(\n",
    "        y_true_bin,\n",
    "        score_cal,\n",
    "        f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\",\n",
    "        dev_out,\n",
    "        rel_out,\n",
    "    )\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw)\n",
    "    m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw)\n",
    "    per_class_rows.append(m_raw)\n",
    "\n",
    "    m_cal = _metrics_from_cm(cm_cal)\n",
    "    m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal)\n",
    "    per_class_rows.append(m_cal)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    print(f\"  ✓ Saved per-class plots (DEV) → {fig_percls}\")\n",
    "if EXPORT_RELEASE:\n",
    "    print(f\"  ✓ Saved per-class plots (RELEASE) → {release_single}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    dev_metrics_path = metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\"\n",
    "    per_class_df.to_csv(dev_metrics_path, index=False)\n",
    "    print(f\"  ✓ Saved DEV per-class metrics → {dev_metrics_path}\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "    print(f\"  ✓ Saved RELEASE per-class metrics → {out_single}\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "    print(f\"  ✓ Appended DEV binary-head metrics → {dev_root / 'stacker_metrics.csv'}\")\n",
    "\n",
    "print(\"\\n✅ BROAD PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# PATCHES ADDED (to address “plots missing / skipped” symptoms):\n",
    "#   (A) Optional DEBUG_DIAGNOSTICS: prints output paths + CAL class balance + confirms file writes.\n",
    "#   (B) Hard traceback on failures (instead of silent warnings) to surface root cause.\n",
    "#   (C) SHAP beeswarm robustification: optional subsample of TRAIN to avoid memory/time failures.\n",
    "#   (D) Optional SAFE_SINGLE_THREAD: mitigates fork/thread/numba/TBB instability during SHAP/plotting.\n",
    "#   (E) Explicit existence checks after savefig (so “saved but not where expected” is obvious).\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 0: IMPORTS + CONFIG\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\"Immature\": \"#0079ea\", \"Mature\": \"#AF3434\"}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    \"HSC_MPP\":            \"#0079ea\",\n",
    "    \"LMPP\":               \"#17BECF\",\n",
    "    \"GMP\":                \"#C5E4FF\",\n",
    "    \"Myeloid progenitor\": \"#AEC7E8\",\n",
    "    \"Monocyte\":           \"#D27CE3\",\n",
    "    \"CD14 Mono\":          \"#D27CE3\",\n",
    "    \"CD16 Mono\":          \"#8D43CD\",\n",
    "    \"Erythroblast\":       \"#F30A1A\",\n",
    "    \"ErP\":                \"#D1235A\",\n",
    "    \"MEP\":                \"#E364B0\",\n",
    "    \"CD4 T Naive\":        \"#C99546\",\n",
    "    \"CD4 T Memory\":       \"#C1AF93\",\n",
    "    \"CD8 T Naive\":        \"#4D382E\",\n",
    "    \"CD8 T Memory\":       \"#6B3317\",\n",
    "    \"Other_T\":            \"#EDB416\",\n",
    "    \"Treg\":               \"#6E6C37\",\n",
    "    \"B Naive\":            \"#1C511D\",\n",
    "    \"B Memory\":           \"#68D827\",\n",
    "    \"Pro-B\":              \"#66BB6A\",\n",
    "    \"Pre-B\":              \"#2DBD67\",\n",
    "    \"Immature B\":         \"#91FF7B\",\n",
    "    \"Plasma\":             \"#9DC012\",\n",
    "    \"cDC1\":               \"#76A7CB\",\n",
    "    \"cDC2\":               \"#16D2E3\",\n",
    "    \"pDC\":                \"#69FFCB\",\n",
    "    \"NK CD56 bright\":     \"#F3AC1F\",\n",
    "    \"NK CD56 dim\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_BY_DEPTH = {\n",
    "    \"Broad\": PALETTE_BROAD,\n",
    "    \"Simplified\": PALETTE_SIMPLIFIED,\n",
    "    \"Detailed\": PALETTE_DETAILED,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True\n",
    "EXPORT_DEV     = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Simplified\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "EXCLUDE_CLASSES = {}\n",
    "\n",
    "custom_palette = PALETTE_BY_DEPTH.get(name_target_class, {})\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DIAGNOSTICS / ROBUSTIFICATION SWITCHES (PATCH)\n",
    "# -----------------------------------------------------------------------------\n",
    "DEBUG_DIAGNOSTICS = True\n",
    "HARD_TRACEBACKS   = True   # if True: prints stack traces when plot/SHAP fails\n",
    "SHAP_TRAIN_SUBSAMPLE_MAX_N = 5000  # set None to disable subsampling\n",
    "SAFE_SINGLE_THREAD = False  # set True if you see Numba/TBB fork/thread warnings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "EMBEDDING_OBSM_KEY = \"X_umap\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "Luecken_root = Path(models_output)\n",
    "\n",
    "dev_root     = Luecken_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir       = models_root / \"heads\"\n",
    "metrics_dir     = reports_root / \"metrics\"\n",
    "probs_dir       = reports_root / \"probabilities\"\n",
    "fig_percls      = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root    = Luecken_root / \"Release\"\n",
    "release_models  = release_root / name_target_class / \"Models\"\n",
    "release_reports = release_root / name_target_class / \"Reports\"\n",
    "release_metrics = release_reports / \"Metrics\"\n",
    "release_probs   = release_reports / \"Probabilities\"\n",
    "release_imps    = release_reports / \"Importances\"\n",
    "release_figs    = release_root / name_target_class / \"Figures\"\n",
    "release_single  = release_figs / \"Single_classes\"\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "if DEBUG_DIAGNOSTICS:\n",
    "    print(f\"[DEBUG] HAS_SHAP={HAS_SHAP} EXPORT_RELEASE={EXPORT_RELEASE} EXPORT_DEV={EXPORT_DEV}\")\n",
    "    print(f\"[DEBUG] release_single={release_single}\")\n",
    "    print(f\"[DEBUG] release_imps={release_imps}\")\n",
    "    print(f\"[DEBUG] SAFE_SINGLE_THREAD={SAFE_SINGLE_THREAD} SHAP_SUBSAMPLE_MAX_N={SHAP_TRAIN_SUBSAMPLE_MAX_N}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "Luecken_data_Train = MLTraining.attach_celltype(Luecken_data_Train, Luecken_dataset_Train, consensus_field)\n",
    "Luecken_data_Test  = MLTraining.attach_celltype(Luecken_data_Test,  Luecken_dataset_Test,  consensus_field)\n",
    "Luecken_data_Cal   = MLTraining.attach_celltype(Luecken_data_Cal,   Luecken_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Luecken_data_Train = panel_intersection(rename_data_to_panel(Luecken_data_Train))\n",
    "Luecken_data_Test  = panel_intersection(rename_data_to_panel(Luecken_data_Test))\n",
    "Luecken_data_Cal   = panel_intersection(rename_data_to_panel(Luecken_data_Cal))\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS (WITH CAL/TEST ROW FILTERING)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Luecken_data_Cal_lbl = Luecken_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Cal.columns]\n",
    "\n",
    "Luecken_data_Train_Sub = Luecken_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Luecken_data_Test_Sub  = Luecken_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Luecken_data_Cal_Sub   = Luecken_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Luecken_data_Train_Sub.columns)\n",
    "if list(Luecken_data_Test_Sub.columns) != cols_train or list(Luecken_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Luecken_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Luecken_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Luecken_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "# classes learned from TRAIN, excluding user-specified\n",
    "all_classes = sorted(pd.Series(Luecken_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if str(c) not in EXCLUDE_CLASSES]\n",
    "\n",
    "excluded_present = sorted(set(all_classes).intersection(EXCLUDE_CLASSES))\n",
    "if excluded_present:\n",
    "    print(f\"  [INFO] Excluding {len(excluded_present)} classes: {excluded_present}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes after exclusions\")\n",
    "\n",
    "# ---- critical: filter CAL/TEST rows to those classes ----\n",
    "keep_set = set(map(str, class_names))\n",
    "\n",
    "cal_keep_mask  = Luecken_data_Cal_lbl[\"Celltype\"].astype(str).isin(keep_set)\n",
    "test_keep_mask = Luecken_data_Test[\"Celltype\"].astype(str).isin(keep_set)\n",
    "\n",
    "n_cal_drop  = int((~cal_keep_mask).sum())\n",
    "n_test_drop = int((~test_keep_mask).sum())\n",
    "\n",
    "if n_cal_drop > 0:\n",
    "    dropped = sorted(Luecken_data_Cal_lbl.loc[~cal_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_cal_drop} CAL rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "if n_test_drop > 0:\n",
    "    dropped = sorted(Luecken_data_Test.loc[~test_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_test_drop} TEST rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "# filtered label frames\n",
    "Luecken_data_Cal_lbl_f  = Luecken_data_Cal_lbl.loc[cal_keep_mask].copy()\n",
    "Luecken_data_Test_lbl_f = Luecken_data_Test.loc[test_keep_mask, [\"Celltype\"]].copy()\n",
    "\n",
    "# filtered feature frames (must align by index)\n",
    "X_cal_all_df = Luecken_data_Cal_Sub.loc[Luecken_data_Cal_lbl_f.index].copy()\n",
    "X_te_all_df  = Luecken_data_Test_Sub.loc[Luecken_data_Test_lbl_f.index].copy()\n",
    "test_index   = X_te_all_df.index\n",
    "\n",
    "# map filtered labels\n",
    "s_cal = Luecken_data_Cal_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Luecken_data_Cal_lbl_f.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in CAL after filtering: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Luecken_data_Test_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Luecken_data_Test_lbl_f.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in TEST after filtering: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# probability matrices sized to filtered CAL/TEST\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "xgb_shap_rows      = []\n",
    "lr_contrib_rows    = []\n",
    "platt_metrics_rows = []\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Luecken/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Luecken_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Luecken_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN embedding (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=custom_palette,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Luecken_dataset_Train,\n",
    "            train_df=Luecken_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Luecken/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Luecken_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Luecken_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST (filtered) for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Luecken_data_Test.loc[X_te_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # CAL split (filtered) for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Luecken_data_Cal.loc[X_cal_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(scaler.transform(df.values), index=df.index, columns=cols_train)\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Luecken\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions (unchanged)\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Luecken\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "    shap_df = shap_df[\n",
    "        [\"depth\", \"class_name\", \"dataset\", \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\", \"rank_within_class\"]\n",
    "    ]\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Luecken_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "    probs_dev.to_csv(probs_dir / \"probabilities_before_after_TEST.csv\", index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Luecken_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "    probs_release.to_csv(release_probs / \"Multiclass_models_probabilities_on_test.csv\", index=True)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report_df = pd.DataFrame(\n",
    "    classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    ").T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(cm_mc, index=pd.Index(class_names, name=\"true\"), columns=pd.Index(class_names, name=\"pred\"))\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(y_true_bin, score_raw, f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(y_true_bin, score_cal, f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw); m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw); per_class_rows.append(m_raw)\n",
    "    m_cal = _metrics_from_cm(cm_cal); m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal); per_class_rows.append(m_cal)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    per_class_df.to_csv(metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\", index=False)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\n✅ SIMPLIFIED PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# MODEL TRAINING PIPELINE (LEAN MAIN SCRIPT)\n",
    "#   - RAW vs PLATT vs TEMP-SCALED\n",
    "#   - DEV/RELEASE exports\n",
    "#   - Importances: XGB SHAP mean_abs + corr (Top10) + LR meta-learner contributions\n",
    "#   - Platt calibration plots (Ideal -> RAW -> Platt on top) with TEST LogLoss/Brier in legend\n",
    "#   - Per-class pre/post Platt metrics exported to CSV\n",
    "#   - Per-class TRAIN UMAP (pos vs rest) + legend PNG\n",
    "#\n",
    "# PATCHES ADDED (to address “plots missing / skipped” symptoms):\n",
    "#   (A) Optional DEBUG_DIAGNOSTICS: prints output paths + CAL class balance + confirms file writes.\n",
    "#   (B) Hard traceback on failures (instead of silent warnings) to surface root cause.\n",
    "#   (C) SHAP beeswarm robustification: optional subsample of TRAIN to avoid memory/time failures.\n",
    "#   (D) Optional SAFE_SINGLE_THREAD: mitigates fork/thread/numba/TBB instability during SHAP/plotting.\n",
    "#   (E) Explicit existence checks after savefig (so “saved but not where expected” is obvious).\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 0: IMPORTS + CONFIG\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc,\n",
    ")\n",
    "\n",
    "import MLTraining  # uses MLTraining.py helpers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Palettes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PALETTE_BROAD = {\"Immature\": \"#0079ea\", \"Mature\": \"#AF3434\"}\n",
    "\n",
    "PALETTE_SIMPLIFIED = {\n",
    "    \"HSPC\":      \"#0079ea\",\n",
    "    \"Erythroid\": \"#c11212\",\n",
    "    \"pDC\":       \"#62E6B8\",\n",
    "    \"Monocyte\":  \"#D27CE3\",\n",
    "    \"Myeloid\":   \"#8D43CD\",\n",
    "    \"CD4_T\":     \"#C99546\",\n",
    "    \"CD8_T\":     \"#6B3317\",\n",
    "    \"B\":         \"#68D827\",\n",
    "    \"cDC\":       \"#16D2E3\",\n",
    "    \"Other_T\":   \"#EDB416\",\n",
    "    \"NK\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_DETAILED = {\n",
    "    \"HSC_MPP\":            \"#0079ea\",\n",
    "    \"LMPP\":               \"#17BECF\",\n",
    "    \"GMP\":                \"#C5E4FF\",\n",
    "    \"Myeloid progenitor\": \"#AEC7E8\",\n",
    "    \"Monocyte\":           \"#D27CE3\",\n",
    "    \"CD14 Mono\":          \"#D27CE3\",\n",
    "    \"CD16 Mono\":          \"#8D43CD\",\n",
    "    \"Erythroblast\":       \"#F30A1A\",\n",
    "    \"ErP\":                \"#D1235A\",\n",
    "    \"MEP\":                \"#E364B0\",\n",
    "    \"CD4 T Naive\":        \"#C99546\",\n",
    "    \"CD4 T Memory\":       \"#C1AF93\",\n",
    "    \"CD8 T Naive\":        \"#4D382E\",\n",
    "    \"CD8 T Memory\":       \"#6B3317\",\n",
    "    \"Other_T\":            \"#EDB416\",\n",
    "    \"Treg\":               \"#6E6C37\",\n",
    "    \"B Naive\":            \"#1C511D\",\n",
    "    \"B Memory\":           \"#68D827\",\n",
    "    \"Pro-B\":              \"#66BB6A\",\n",
    "    \"Pre-B\":              \"#2DBD67\",\n",
    "    \"Immature B\":         \"#91FF7B\",\n",
    "    \"Plasma\":             \"#9DC012\",\n",
    "    \"cDC1\":               \"#76A7CB\",\n",
    "    \"cDC2\":               \"#16D2E3\",\n",
    "    \"pDC\":                \"#69FFCB\",\n",
    "    \"NK CD56 bright\":     \"#F3AC1F\",\n",
    "    \"NK CD56 dim\":        \"#FBEF0D\",\n",
    "}\n",
    "\n",
    "PALETTE_BY_DEPTH = {\n",
    "    \"Broad\": PALETTE_BROAD,\n",
    "    \"Simplified\": PALETTE_SIMPLIFIED,\n",
    "    \"Detailed\": PALETTE_DETAILED,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTIONAL: SHAP dependency\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXPORT SWITCHES\n",
    "# -----------------------------------------------------------------------------\n",
    "EXPORT_RELEASE = True\n",
    "EXPORT_DEV     = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "name_target_class = \"Detailed\"  # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "EXCLUDE_CLASSES = {}\n",
    "\n",
    "custom_palette = PALETTE_BY_DEPTH.get(name_target_class, {})\n",
    "kf          = MLTraining.CV\n",
    "num_cores   = -1\n",
    "metrics_log = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DIAGNOSTICS / ROBUSTIFICATION SWITCHES (PATCH)\n",
    "# -----------------------------------------------------------------------------\n",
    "DEBUG_DIAGNOSTICS = True\n",
    "HARD_TRACEBACKS   = True   # if True: prints stack traces when plot/SHAP fails\n",
    "SHAP_TRAIN_SUBSAMPLE_MAX_N = 5000  # set None to disable subsampling\n",
    "SAFE_SINGLE_THREAD = False  # set True if you see Numba/TBB fork/thread warnings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDING CONFIG (for Class_Train_data.png)\n",
    "# -----------------------------------------------------------------------------\n",
    "EMBEDDING_SOURCE = \"adata_obsm\"   # \"adata_obsm\" | \"adata_obs\" | \"train_df\"\n",
    "EMBEDDING_OBSM_KEY = \"X_umap\"\n",
    "EMBEDDING_OBS_X = \"UMAP_1\"\n",
    "EMBEDDING_OBS_Y = \"UMAP_2\"\n",
    "EMBEDDING_DF_X = \"UMAP_1\"\n",
    "EMBEDDING_DF_Y = \"UMAP_2\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "Luecken_root = Path(models_output)\n",
    "\n",
    "dev_root     = Luecken_root / \"Dev\"\n",
    "models_root  = dev_root / name_target_class / \"Models\"  / name_target_class\n",
    "reports_root = dev_root / name_target_class / \"Reports\" / name_target_class\n",
    "fig_root     = dev_root / name_target_class / \"Figures\" / name_target_class\n",
    "\n",
    "heads_dir       = models_root / \"heads\"\n",
    "metrics_dir     = reports_root / \"metrics\"\n",
    "probs_dir       = reports_root / \"probabilities\"\n",
    "fig_percls      = fig_root / \"per_class\"\n",
    "dev_importances = reports_root / \"Importances\"\n",
    "\n",
    "release_root    = Luecken_root / \"Release\"\n",
    "release_models  = release_root / name_target_class / \"Models\"\n",
    "release_reports = release_root / name_target_class / \"Reports\"\n",
    "release_metrics = release_reports / \"Metrics\"\n",
    "release_probs   = release_reports / \"Probabilities\"\n",
    "release_imps    = release_reports / \"Importances\"\n",
    "release_figs    = release_root / name_target_class / \"Figures\"\n",
    "release_single  = release_figs / \"Single_classes\"\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    for p in (models_root, heads_dir, reports_root, metrics_dir, probs_dir, fig_root, fig_percls, dev_importances):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    for p in (release_models, release_reports, release_metrics, release_probs, release_imps, release_figs, release_single):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] RELEASE Root:    {release_root}\")\n",
    "    print(f\"[INFO] RELEASE Models:  {release_models}\")\n",
    "    print(f\"[INFO] RELEASE Reports: {release_reports}\")\n",
    "    print(f\"[INFO] RELEASE Figures: {release_figs}\")\n",
    "\n",
    "if DEBUG_DIAGNOSTICS:\n",
    "    print(f\"[DEBUG] HAS_SHAP={HAS_SHAP} EXPORT_RELEASE={EXPORT_RELEASE} EXPORT_DEV={EXPORT_DEV}\")\n",
    "    print(f\"[DEBUG] release_single={release_single}\")\n",
    "    print(f\"[DEBUG] release_imps={release_imps}\")\n",
    "    print(f\"[DEBUG] SAFE_SINGLE_THREAD={SAFE_SINGLE_THREAD} SHAP_SUBSAMPLE_MAX_N={SHAP_TRAIN_SUBSAMPLE_MAX_N}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: ATTACH CELL-TYPE LABELS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Attaching cell-type labels from AnnData.obs...\")\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "Luecken_data_Train = MLTraining.attach_celltype(Luecken_data_Train, Luecken_dataset_Train, consensus_field)\n",
    "Luecken_data_Test  = MLTraining.attach_celltype(Luecken_data_Test,  Luecken_dataset_Test,  consensus_field)\n",
    "Luecken_data_Cal   = MLTraining.attach_celltype(Luecken_data_Cal,   Luecken_dataset_Cal,   consensus_field)\n",
    "\n",
    "print(f\"  ✓ Attached '{consensus_field}' to Train/Test/Cal splits\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: ALIGN DATA COLUMNS TO REFERENCE PANEL\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Aligning data columns to reference panel (exact names preserved)...\")\n",
    "\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "panel_keys = MLTraining.norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Adjust MLTraining.norm_feats rules.\")\n",
    "\n",
    "def rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = MLTraining.norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new)\n",
    "            safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"  [WARN] Dropping {len(drops)} duplicated-mapped columns (sample: {drops[:5]})\")\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "    print(f\"  ✓ Matched {len(safe_map)}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "def panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Luecken_data_Train = panel_intersection(rename_data_to_panel(Luecken_data_Train))\n",
    "Luecken_data_Test  = panel_intersection(rename_data_to_panel(Luecken_data_Test))\n",
    "Luecken_data_Cal   = panel_intersection(rename_data_to_panel(Luecken_data_Cal))\n",
    "print(\"  ✓ Data columns now aligned to panel (panel order preserved)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: PREPARE FEATURES & LABELS (WITH CAL/TEST ROW FILTERING)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Extracting features and labels...\")\n",
    "\n",
    "Luecken_data_Cal_lbl = Luecken_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Cal.columns]\n",
    "\n",
    "Luecken_data_Train_Sub = Luecken_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Luecken_data_Test_Sub  = Luecken_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Luecken_data_Cal_Sub   = Luecken_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "cols_train = list(Luecken_data_Train_Sub.columns)\n",
    "if list(Luecken_data_Test_Sub.columns) != cols_train or list(Luecken_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "MLTraining.check_finite(Luecken_data_Train_Sub, \"TRAIN\")\n",
    "MLTraining.check_finite(Luecken_data_Test_Sub,  \"TEST\")\n",
    "MLTraining.check_finite(Luecken_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"  ✓ Using {len(cols_train)} panel-intersected features (exact panel names)\")\n",
    "print(f\"    Sample: {cols_train[:5]}...\")\n",
    "\n",
    "# classes learned from TRAIN, excluding user-specified\n",
    "all_classes = sorted(pd.Series(Luecken_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if str(c) not in EXCLUDE_CLASSES]\n",
    "\n",
    "excluded_present = sorted(set(all_classes).intersection(EXCLUDE_CLASSES))\n",
    "if excluded_present:\n",
    "    print(f\"  [INFO] Excluding {len(excluded_present)} classes: {excluded_present}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "print(f\"  ✓ Found {K} classes after exclusions\")\n",
    "\n",
    "# ---- critical: filter CAL/TEST rows to those classes ----\n",
    "keep_set = set(map(str, class_names))\n",
    "\n",
    "cal_keep_mask  = Luecken_data_Cal_lbl[\"Celltype\"].astype(str).isin(keep_set)\n",
    "test_keep_mask = Luecken_data_Test[\"Celltype\"].astype(str).isin(keep_set)\n",
    "\n",
    "n_cal_drop  = int((~cal_keep_mask).sum())\n",
    "n_test_drop = int((~test_keep_mask).sum())\n",
    "\n",
    "if n_cal_drop > 0:\n",
    "    dropped = sorted(Luecken_data_Cal_lbl.loc[~cal_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_cal_drop} CAL rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "if n_test_drop > 0:\n",
    "    dropped = sorted(Luecken_data_Test.loc[~test_keep_mask, \"Celltype\"].astype(str).unique().tolist())\n",
    "    print(f\"  [INFO] Dropping {n_test_drop} TEST rows with excluded/unknown labels: {dropped}\")\n",
    "\n",
    "# filtered label frames\n",
    "Luecken_data_Cal_lbl_f  = Luecken_data_Cal_lbl.loc[cal_keep_mask].copy()\n",
    "Luecken_data_Test_lbl_f = Luecken_data_Test.loc[test_keep_mask, [\"Celltype\"]].copy()\n",
    "\n",
    "# filtered feature frames (must align by index)\n",
    "X_cal_all_df = Luecken_data_Cal_Sub.loc[Luecken_data_Cal_lbl_f.index].copy()\n",
    "X_te_all_df  = Luecken_data_Test_Sub.loc[Luecken_data_Test_lbl_f.index].copy()\n",
    "test_index   = X_te_all_df.index\n",
    "\n",
    "# map filtered labels\n",
    "s_cal = Luecken_data_Cal_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Luecken_data_Cal_lbl_f.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in CAL after filtering: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Luecken_data_Test_lbl_f[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Luecken_data_Test_lbl_f.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Still-unmapped labels in TEST after filtering: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# probability matrices sized to filtered CAL/TEST\n",
    "P_cal_raw   = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_cal_platt = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "\n",
    "P_te_raw    = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "P_te_platt  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "heads_mem = {}\n",
    "\n",
    "xgb_shap_rows      = []\n",
    "lr_contrib_rows    = []\n",
    "platt_metrics_rows = []\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: TRAIN OvR BINARY HEADS (+ Platt on CAL)\n",
    "# =============================================================================\n",
    "print(f\"\\n[STEP 4] Training {K} binary OvR classifiers...\\n\")\n",
    "\n",
    "TOP_N = 10\n",
    "base_order = [\"NB\", \"XGB\", \"KNN\", \"MLP\"]\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    cls_safe = MLTraining.safe_name(celltype)\n",
    "    print(f\"▸ Processing {cls_safe} (class {k+1}/{K})\")\n",
    "\n",
    "    # 4.1 Load TRAIN barcodes for this class\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Luecken/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_training_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Luecken_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Luecken_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"  [SKIP] Empty or single-class train (pos={y_tr.sum()}, neg={len(y_tr)-y_tr.sum()})\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4.1b TRAIN embedding (pos vs rest) + legend\n",
    "    try:\n",
    "        MLTraining.save_class_train_umap_pngs(\n",
    "            celltype=str(celltype),\n",
    "            cls_safe=cls_safe,\n",
    "            barcodes=found_train_barcodes,\n",
    "            y_bin=y_tr,\n",
    "            custom_palette=custom_palette,\n",
    "            out_dir_dev=fig_percls if EXPORT_DEV else None,\n",
    "            out_dir_rel=release_single if EXPORT_RELEASE else None,\n",
    "            adata_train=Luecken_dataset_Train,\n",
    "            train_df=Luecken_data_Train,\n",
    "            embedding_source=EMBEDDING_SOURCE,\n",
    "            obsm_key=EMBEDDING_OBSM_KEY,\n",
    "            obs_x=EMBEDDING_OBS_X,\n",
    "            obs_y=EMBEDDING_OBS_Y,\n",
    "            df_x=EMBEDDING_DF_X,\n",
    "            df_y=EMBEDDING_DF_Y,\n",
    "            neg_color=\"#A3A3A3\",\n",
    "            outline=(5, 0.05),\n",
    "            debug=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"UMAP train plot failed for '{celltype}': {e}\")\n",
    "\n",
    "    # 4.2 Load TEST barcodes for class-specific metrics (optional)\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Luecken/Consensus_annotation_{name_target_class.lower()}_final/Barcodes_testing_class_{cls_safe}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Luecken_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Luecken_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # Full TEST (filtered) for head probabilities / calibration plot eval\n",
    "    X_te_all_local = X_te_all_df\n",
    "    y_te_all = (Luecken_data_Test.loc[X_te_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # CAL split (filtered) for Platt fitting\n",
    "    X_cal_df  = X_cal_all_df\n",
    "    y_cal_bin = (Luecken_data_Cal.loc[X_cal_all_df.index, \"Celltype\"].astype(str).values == str(celltype)).astype(int)\n",
    "\n",
    "    # 4.3 Fit scaler on TRAIN; transform all splits\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame(scaler.transform(df.values), index=df.index, columns=cols_train)\n",
    "\n",
    "    X_tr_sc_df      = _sc(X_tr_df)\n",
    "    X_te_sc_df      = _sc(X_te_df)\n",
    "    X_te_all_sc_df  = _sc(X_te_all_local)\n",
    "    X_cal_sc_df     = _sc(X_cal_df)\n",
    "\n",
    "    # 4.4 Train base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=cls_safe)\n",
    "\n",
    "    # 4.5 Stacking RAW head\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # 4.6 Platt calibration (fit on CAL only)\n",
    "    pos_cal   = int(y_cal_bin.sum())\n",
    "    n_cal_bin = int(len(y_cal_bin))\n",
    "    has_both  = (0 < pos_cal < n_cal_bin)\n",
    "\n",
    "    stacker_platt = None\n",
    "    if has_both:\n",
    "        stacker_platt = MLTraining.calibrate_prefit(stacker_raw, X_cal_sc_df, y_cal_bin, method=\"sigmoid\")\n",
    "    else:\n",
    "        print(\"    [WARN] Skipped Platt calibration (single-class CAL)\")\n",
    "\n",
    "    # 4.7 Platt evaluation curve on TEST (Ideal -> RAW -> Platt) + metrics row\n",
    "    try:\n",
    "        p_test_raw   = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        p_test_platt = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1] if stacker_platt is not None else None\n",
    "\n",
    "        dev_platt = (fig_percls / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_DEV else None\n",
    "        rel_platt = (release_single / f\"{cls_safe}_Platt_calibration_evaluation_on_test.png\") if EXPORT_RELEASE else None\n",
    "\n",
    "        ll_raw, br_raw, ll_pl, br_pl, pl_avail = MLTraining.plot_platt_calibration_on_test(\n",
    "            y_true_bin=y_te_all.astype(int),\n",
    "            p_raw=p_test_raw,\n",
    "            p_platt=p_test_platt,\n",
    "            title=f\"{name_target_class} – {celltype}: Platt calibration evaluation on TEST\",\n",
    "            out_png_dev=dev_platt,\n",
    "            out_png_rel=rel_platt,\n",
    "            n_bins=15,\n",
    "        )\n",
    "\n",
    "        platt_metrics_rows.append({\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"n_test_samples\": int(len(y_te_all)),\n",
    "            \"n_test_positive\": int(y_te_all.sum()),\n",
    "            \"logloss_raw\": ll_raw,\n",
    "            \"brier_raw\": br_raw,\n",
    "            \"logloss_platt\": ll_pl,\n",
    "            \"brier_platt\": br_pl,\n",
    "            \"platt_available\": bool(pl_avail),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Platt calibration plot failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.8 Save per-class head bundle + keep in-memory for package\n",
    "    head_bundle = {\n",
    "        \"atlas\": \"Luecken\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": str(celltype),\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "        \"columns\": cols_train,\n",
    "        \"scaler\": scaler,\n",
    "        \"model_raw\": stacker_raw,\n",
    "        \"model_platt\": stacker_platt,\n",
    "    }\n",
    "    heads_mem[str(celltype)] = head_bundle\n",
    "\n",
    "    if EXPORT_DEV:\n",
    "        joblib.dump(head_bundle, heads_dir / f\"{cls_safe}.joblib\")\n",
    "\n",
    "    # 4.9 Optional per-head metrics logging (class-specific TEST subset)\n",
    "    try:\n",
    "        model_for_eval = stacker_platt if stacker_platt is not None else stacker_raw\n",
    "        m = MLTraining.evaluate_classifier(model_for_eval, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=str(celltype), used_platt=bool(stacker_platt is not None))\n",
    "        metrics_log.append(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4.10 OvR probability matrices (RAW + PLATT) for multiclass downstream\n",
    "    P_cal_raw[:, k] = stacker_raw.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te_raw[:,  k] = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "    if stacker_platt is not None:\n",
    "        P_cal_platt[:, k] = stacker_platt.predict_proba(X_cal_sc_df)[:, 1]\n",
    "        P_te_platt[:,  k] = stacker_platt.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "    else:\n",
    "        P_cal_platt[:, k] = P_cal_raw[:, k]\n",
    "        P_te_platt[:,  k] = P_te_raw[:,  k]\n",
    "\n",
    "    # 4.11 SHAP: mean_abs + corr on TEST; beeswarm TRAIN only\n",
    "    if HAS_SHAP:\n",
    "        try:\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            shap_sum_test = MLTraining.xgb_shap_mean_abs_and_corr(XGB_model, X_te_all_sc_df, class_index=1)\n",
    "            shap_sum_test[\"depth\"] = name_target_class\n",
    "            shap_sum_test[\"class_name\"] = str(celltype)\n",
    "            shap_sum_test[\"dataset\"] = \"TEST\"\n",
    "            xgb_shap_rows.extend(shap_sum_test.to_dict(orient=\"records\"))\n",
    "\n",
    "            # Beeswarm on TRAIN only\n",
    "            if EXPORT_DEV:\n",
    "                outp = fig_percls / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "            if EXPORT_RELEASE:\n",
    "                outp = release_single / f\"{cls_safe}_SHAP_beeswarm_TRAIN.png\"\n",
    "                sv, _ = MLTraining.xgb_shap_values(XGB_model, X_tr_sc_df, class_index=1)\n",
    "                MLTraining.plot_xgb_shap_beeswarm(\n",
    "                    sv, X_tr_sc_df,\n",
    "                    title=f\"{name_target_class} – {celltype}: SHAP importance\",\n",
    "                    max_display=TOP_N,\n",
    "                    out_path=outp,\n",
    "                    figsize=(6, 7),\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"SHAP failed for class '{celltype}': {e}\")\n",
    "\n",
    "    # 4.12 LR meta-learner contributions (unchanged)\n",
    "    try:\n",
    "        contrib = _lr_baselearner_contributions(stacker_raw, X_te_all_sc_df, base_order=base_order)\n",
    "        row = {\n",
    "            \"depth\": name_target_class,\n",
    "            \"class_name\": str(celltype),\n",
    "            \"dataset\": \"TEST\",\n",
    "            \"n_meta_features\": contrib[\"n_meta_features\"],\n",
    "            \"per_estimator_meta_cols\": contrib[\"per_estimator_meta_cols\"],\n",
    "        }\n",
    "        for b in base_order:\n",
    "            row[f\"{b}_mean_abs_contribution\"] = contrib[\"per_base\"].get(b, {}).get(\"mean_abs_contribution\", 0.0)\n",
    "            row[f\"{b}_coef_l1\"]               = contrib[\"per_base\"].get(b, {}).get(\"coef_l1\", 0.0)\n",
    "            row[f\"{b}_n_meta_cols\"]           = contrib[\"per_base\"].get(b, {}).get(\"n_cols\", 0)\n",
    "        lr_contrib_rows.append(row)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LR contribution extraction failed for class '{celltype}': {e}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT: Per-class LogLoss & Brier (pre vs post Platt) on TEST\n",
    "# =============================================================================\n",
    "print(\"\\n[EXPORT] Per-class calibration metrics (RAW vs Platt on TEST)...\")\n",
    "\n",
    "_ = MLTraining.export_platt_metrics_csv(\n",
    "    platt_metrics_rows,\n",
    "    out_dev=metrics_dir if EXPORT_DEV else None,\n",
    "    out_rel=release_metrics if EXPORT_RELEASE else None,\n",
    "    filename=\"Single_classes_metrics_pre_and_post_platt_calibration.csv\",\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: MULTICLASS TEMPERATURE SCALING (fit on CAL using PLATT matrix)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Multiclass Temperature Scaling on CAL (using Platt OvR probabilities)...\")\n",
    "\n",
    "def _check_probs(P: np.ndarray, name: str):\n",
    "    if np.isnan(P).any() or np.isinf(P).any():\n",
    "        raise ValueError(f\"{name} contains NaN/Inf\")\n",
    "    if (P < 0).any() or (P > 1).any():\n",
    "        raise ValueError(f\"{name} contains values outside [0,1]\")\n",
    "\n",
    "_check_probs(P_cal_platt, \"P_cal_platt\")\n",
    "_check_probs(P_te_platt,  \"P_te_platt\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal_platt, y_cal_multiclass)\n",
    "P_te_cal = ts_cal.transform(P_te_platt)\n",
    "\n",
    "P_te_cal = np.asarray(P_te_cal)\n",
    "if P_te_cal.ndim == 1:\n",
    "    P_te_cal = P_te_cal.reshape(-1, 1)\n",
    "\n",
    "if P_te_cal.shape[1] == 1 and K == 2:\n",
    "    P_te_cal = np.hstack([1.0 - P_te_cal, P_te_cal])\n",
    "elif P_te_cal.shape[1] != K:\n",
    "    row_sums = P_te_platt.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_cal = P_te_platt / row_sums\n",
    "    print(f\"  [WARN] TemperatureScaling returned shape {P_te_cal.shape}; fell back to sum-normalized OvR probs\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(ts_cal, models_root / \"temp_scaler.joblib\")\n",
    "    pd.Series(class_names, name=\"class_name\").to_csv(models_root / \"class_names.csv\", index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5b: SAVE DEPLOYABLE PACKAGE(S)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5b] Saving deployable package(s)...\")\n",
    "\n",
    "package = {\n",
    "    \"atlas\": \"Luecken\",\n",
    "    \"depth\": name_target_class,\n",
    "    \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    \"class_names\": class_names,\n",
    "    \"heads\": heads_mem,\n",
    "    \"temp_scaler\": ts_cal,\n",
    "}\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    joblib.dump(package, models_root / \"package.joblib\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    joblib.dump(package, release_models / \"Multiclass_models.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5c: EXPORT IMPORTANCES (Top10 per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5c] Exporting importances (Top 10 per class; SHAP mean_abs + corr + LR)...\")\n",
    "\n",
    "if len(xgb_shap_rows) > 0:\n",
    "    shap_df = pd.DataFrame(xgb_shap_rows)\n",
    "    shap_df = (\n",
    "        shap_df.sort_values([\"depth\", \"class_name\", \"mean_abs_shap\"], ascending=[True, True, False])\n",
    "               .groupby([\"depth\", \"class_name\"], as_index=False)\n",
    "               .head(TOP_N)\n",
    "    )\n",
    "    shap_df[\"rank_within_class\"] = (\n",
    "        shap_df.groupby([\"depth\", \"class_name\"])[\"mean_abs_shap\"]\n",
    "               .rank(ascending=False, method=\"first\")\n",
    "               .astype(int)\n",
    "    )\n",
    "    shap_df = shap_df[\n",
    "        [\"depth\", \"class_name\", \"dataset\", \"feature\", \"mean_abs_shap\", \"corr_feature_value_vs_shap\", \"rank_within_class\"]\n",
    "    ]\n",
    "    if EXPORT_DEV:\n",
    "        shap_df.to_csv(dev_importances / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        shap_df.to_csv(release_imps / \"SHAP_XGB_Feature_importances.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No SHAP rows collected (or SHAP not installed).\")\n",
    "\n",
    "if len(lr_contrib_rows) > 0:\n",
    "    lr_df = pd.DataFrame(lr_contrib_rows)\n",
    "    if EXPORT_DEV:\n",
    "        lr_df.to_csv(dev_importances / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "    if EXPORT_RELEASE:\n",
    "        lr_df.to_csv(release_imps / \"LR_MetaLearner_BaseLearner_contributions.csv\", index=False)\n",
    "else:\n",
    "    print(\"  [INFO] No LR contribution rows collected.\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: SAVE PROBABILITIES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Saving probability outputs...\")\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    probs_raw_df   = pd.DataFrame(P_te_raw,   index=test_index, columns=[f\"raw_{c}\"   for c in class_names])\n",
    "    probs_platt_df = pd.DataFrame(P_te_platt, index=test_index, columns=[f\"platt_{c}\" for c in class_names])\n",
    "    probs_cal_df   = pd.DataFrame(P_te_cal,   index=test_index, columns=[f\"cal_{c}\"   for c in class_names])\n",
    "\n",
    "    probs_dev = pd.concat([probs_raw_df, probs_platt_df, probs_cal_df], axis=1)\n",
    "    probs_dev[\"true_label\"] = Luecken_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_dev[\"pred_raw\"]   = P_te_raw.argmax(axis=1)\n",
    "    probs_dev[\"pred_cal\"]   = P_te_cal.argmax(axis=1)\n",
    "    probs_dev[\"pred_raw_name\"] = [class_names[i] for i in probs_dev[\"pred_raw\"].values]\n",
    "    probs_dev[\"pred_cal_name\"] = [class_names[i] for i in probs_dev[\"pred_cal\"].values]\n",
    "    probs_dev.to_csv(probs_dir / \"probabilities_before_after_TEST.csv\", index=True)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    probs_cal_df = pd.DataFrame(P_te_cal, index=test_index, columns=[f\"cal_{c}\" for c in class_names])\n",
    "    probs_release = probs_cal_df.copy()\n",
    "    probs_release[\"true_label\"]    = Luecken_data_Test.loc[test_index, \"Celltype\"].values\n",
    "    probs_release[\"pred_cal\"]      = P_te_cal.argmax(axis=1)\n",
    "    probs_release[\"pred_cal_name\"] = [class_names[i] for i in probs_release[\"pred_cal\"].values]\n",
    "    probs_release[\"max_cal_prob\"]  = probs_cal_df.max(axis=1).values\n",
    "    probs_release.to_csv(release_probs / \"Multiclass_models_probabilities_on_test.csv\", index=True)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: MULTICLASS EVALUATION (TEST) — using CAL probabilities\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 7] Multiclass evaluation (TEST; using CAL probs)...\\n\")\n",
    "\n",
    "y_pred_cal = P_te_cal.argmax(axis=1)\n",
    "report_txt = classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, digits=3)\n",
    "print(\"Multiclass Classification Report (TEST):\")\n",
    "print(report_txt)\n",
    "\n",
    "cm_mc = confusion_matrix(y_test_multiclass, y_pred_cal, labels=range(K))\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_mc)\n",
    "\n",
    "report_df = pd.DataFrame(\n",
    "    classification_report(y_test_multiclass, y_pred_cal, target_names=class_names, output_dict=True)\n",
    ").T\n",
    "\n",
    "cm_mc_df = pd.DataFrame(cm_mc, index=pd.Index(class_names, name=\"true\"), columns=pd.Index(class_names, name=\"pred\"))\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    report_df.to_csv(metrics_dir / \"multiclass_classification_report_TEST.csv\")\n",
    "    cm_mc_df.to_csv(metrics_dir / \"multiclass_confusion_matrix_TEST.csv\")\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    report_df.to_csv(release_metrics / \"Multiclass_models_metrics_on_test.csv\")\n",
    "    cm_mc_df.to_csv(release_metrics / \"Multiclass_models_confusion_matrix_on_test.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: FIGURES (MULTICLASS CM + PER-CLASS CONF & ROC)\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Saving plots...\")\n",
    "\n",
    "def _save_multiclass_cm_png(out_path: Path):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mc, display_labels=class_names)\n",
    "    disp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{name_target_class} – Multiclass Confusion Matrix (on TEST)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    _save_multiclass_cm_png(fig_root / \"multiclass_confusion_matrix_TEST.png\")\n",
    "if EXPORT_RELEASE:\n",
    "    _save_multiclass_cm_png(release_figs / \"Multiclass_models_confusion_matrix_on_test.png\")\n",
    "\n",
    "per_class_rows = []\n",
    "y_pred_raw = P_te_raw.argmax(axis=1)\n",
    "\n",
    "def _metrics_from_cm(cm2x2):\n",
    "    tn, fp, fn, tp = cm2x2.ravel()\n",
    "    support = tp + fn\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return dict(TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn),\n",
    "                support=int(support), precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def _save_cm_fig(cm2x2, cls_label, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fig = plt.figure(figsize=(5.5, 5.0))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm2x2, display_labels=[\"Other\", cls_label]).plot(\n",
    "        values_format=\"d\", cmap=\"Blues\", colorbar=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_roc(y_true, y_score, title, out_dev: Path | None, out_rel: Path | None):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6.0, 5.5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1, color=\"gray\")\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} AUC={a:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    if out_dev is not None:\n",
    "        plt.savefig(out_dev, dpi=300)\n",
    "    if out_rel is not None:\n",
    "        plt.savefig(out_rel, dpi=300)\n",
    "    plt.close(fig)\n",
    "    return a\n",
    "\n",
    "for k, cls in enumerate(class_names):\n",
    "    cls_safe = MLTraining.safe_name(cls)\n",
    "    y_true_bin = (y_test_multiclass == k).astype(int)\n",
    "\n",
    "    score_raw = P_te_raw[:, k]\n",
    "    score_cal = P_te_cal[:, k]\n",
    "\n",
    "    y_pred_raw_bin = (y_pred_raw == k).astype(int)\n",
    "    y_pred_cal_bin = (y_pred_cal == k).astype(int)\n",
    "\n",
    "    cm_raw = confusion_matrix(y_true_bin, y_pred_raw_bin, labels=[0, 1])\n",
    "    cm_cal = confusion_matrix(y_true_bin, y_pred_cal_bin, labels=[0, 1])\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_raw, cls, f\"{name_target_class} – {cls}: Confusion Matrix (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_confusion_matrix_on_test.png\") if EXPORT_RELEASE else None\n",
    "    _save_cm_fig(cm_cal, cls, f\"{name_target_class} – {cls}: Confusion Matrix (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_RAW_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_raw = _save_roc(y_true_bin, score_raw, f\"{name_target_class} – {cls}: ROC (RAW; pre-Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    dev_out = (fig_percls / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_DEV else None\n",
    "    rel_out = (release_single / f\"{cls_safe}_CAL_ROC_on_test.png\") if EXPORT_RELEASE else None\n",
    "    auc_cal = _save_roc(y_true_bin, score_cal, f\"{name_target_class} – {cls}: ROC (CAL; Platt & Temp)\", dev_out, rel_out)\n",
    "\n",
    "    m_raw = _metrics_from_cm(cm_raw); m_raw.update(model=\"RAW\", class_name=cls, auc=auc_raw); per_class_rows.append(m_raw)\n",
    "    m_cal = _metrics_from_cm(cm_cal); m_cal.update(model=\"CAL\", class_name=cls, auc=auc_cal); per_class_rows.append(m_cal)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SAVE METRICS TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Saving metrics tables...\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_rows)[\n",
    "    [\"class_name\", \"model\", \"TP\", \"FP\", \"TN\", \"FN\", \"support\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "].sort_values([\"class_name\", \"model\"])\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    per_class_df.to_csv(metrics_dir / \"per_class_argmax_metrics_TEST_included.csv\", index=False)\n",
    "\n",
    "if EXPORT_RELEASE:\n",
    "    out_single = release_metrics / \"Single_classes_metrics_and_confusion_matrix_on_test.csv\"\n",
    "    per_class_df.to_csv(out_single, index=False)\n",
    "\n",
    "if EXPORT_DEV:\n",
    "    metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "    MLTraining.append_metrics_csv(metrics_df, csv_path=dev_root / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\n✅ DETAILED PIPELINE COMPLETE. Exports saved according to EXPORT_DEV / EXPORT_RELEASE.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# ---------- INPUT ----------\n",
    "cm_csv_path = Path(\n",
    "    \"/Users/kgurashi/GitHub/2024__EspressoPro_Manuscript/Data/Pre_trained_models/\"\n",
    "    \"TotalSeqD_Heme_Oncology_CAT399906/Release/Hao/Reports/Detailed/metrics/\"\n",
    "    \"multiclass_confusion_matrix_TEST_included.csv\"\n",
    ")\n",
    "\n",
    "name_target_class = \"Simplified\"  # Broad | Simplified | Detailed\n",
    "\n",
    "NORMALIZE  = \"row\"   # None | \"row\" | \"col\"\n",
    "CBAR_LABEL = \"% Agreement\"\n",
    "\n",
    "authors_to_do = [\"Hao\", \"Luecken\", \"Zhang\", \"Triana\"]\n",
    "\n",
    "def render_heatmap(csv_path: Path, normalize: str = \"row\", cbar_label: str = \"% Agreement\"):\n",
    "    cm_df = pd.read_csv(csv_path, index_col=0)\n",
    "    labels_true = list(cm_df.index)\n",
    "    labels_pred = list(cm_df.columns)\n",
    "    cm = cm_df.values.astype(float)\n",
    "\n",
    "    # ---------- NORMALISE ----------\n",
    "    if normalize is None:\n",
    "        data = cm\n",
    "    else:\n",
    "        if normalize == \"row\":\n",
    "            denom = cm.sum(axis=1, keepdims=True)\n",
    "        elif normalize == \"col\":\n",
    "            denom = cm.sum(axis=0, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(\"normalize must be None | 'row' | 'col'\")\n",
    "        denom[denom == 0] = 1.0\n",
    "        data = (cm / denom) * 100.0\n",
    "\n",
    "    # ---------- OUTPUT PATHS ----------\n",
    "    # csv_path: .../Release/<Author>/Reports/<Depth>/metrics/<file.csv>\n",
    "    depth_root = csv_path.parents[2]      # .../Release/<Author>/Reports/<Depth>\n",
    "    figures_dir = depth_root / \"Figures\"  # .../Release/<Author>/Reports/<Depth>/Figures\n",
    "    figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_base = figures_dir / \"Multiclass_models_confusion_matrix_on_test_with_percentage_agreement\"\n",
    "    png_path = out_base.with_suffix(\".png\")\n",
    "    pdf_path = out_base.with_suffix(\".pdf\")\n",
    "\n",
    "    # ---------- PLOT ----------\n",
    "    n_cols = len(labels_pred)\n",
    "    n_rows = len(labels_true)\n",
    "\n",
    "    fig_w = max(5, 0.6 * n_cols + 2)\n",
    "    fig_h = max(5, 0.6 * n_rows + 2)\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h), constrained_layout=False)\n",
    "\n",
    "    im = ax.imshow(data, cmap=\"magma\", aspect=\"auto\", interpolation=\"nearest\")\n",
    "\n",
    "    ax.set_xticks(np.arange(n_cols))\n",
    "    ax.set_yticks(np.arange(n_rows))\n",
    "    ax.set_xticklabels(labels_pred, rotation=45, ha=\"right\", fontsize=20)\n",
    "    ax.set_yticklabels(labels_true, fontsize=20)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\n",
    "    ax.set_xlim(-0.5, n_cols - 0.5)\n",
    "    ax.set_ylim(n_rows - 0.5, -0.5)\n",
    "\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    ax.set_xticks(np.arange(-0.5, n_cols, 1.0), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, n_rows, 1.0), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"#373737\", linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"2.5%\", pad=0.02)\n",
    "    cb = plt.colorbar(im, cax=cax)\n",
    "    cb.set_label(cbar_label, fontsize=20)\n",
    "    cb.ax.tick_params(labelsize=20, length=3, width=0.6)\n",
    "    for spine in cax.spines.values():\n",
    "        spine.set_linewidth(0.6)\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(\"#000000\")\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(png_path, dpi=300)\n",
    "    plt.savefig(pdf_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"[SAVE] {png_path}\")\n",
    "    print(f\"[SAVE] {pdf_path}\")\n",
    "\n",
    "# Infer assay root\n",
    "# cm_csv_path: .../<Assay>/Release/Hao/Reports/Detailed/metrics/<file.csv>\n",
    "# assay_root should be: .../<Assay>/Release\n",
    "assay_root = cm_csv_path.parents[5]\n",
    "\n",
    "for author in authors_to_do:\n",
    "    csv = (\n",
    "        assay_root\n",
    "        / author\n",
    "        / \"Release\"\n",
    "        / name_target_class\n",
    "        / \"Reports\"\n",
    "        / \"metrics\"\n",
    "        / \"Multiclass_models_confusion_matrix_on_test.csv\"\n",
    "    )\n",
    "    if not csv.exists():\n",
    "        print(f\"[SKIP] Missing: {csv}\")\n",
    "        continue\n",
    "    render_heatmap(csv, normalize=NORMALIZE, cbar_label=CBAR_LABEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# ---------- INPUT ----------\n",
    "cm_csv_path = Path(\n",
    "    \"/Users/kgurashi/GitHub/2024__EspressoPro_Manuscript/Data/Pre_trained_models/\"\n",
    "    \"TotalSeqD_Heme_Oncology_CAT399906/Release/Hao/Reports/Detailed/metrics/\"\n",
    "    \"multiclass_confusion_matrix_TEST_included.csv\"\n",
    ")\n",
    "\n",
    "name_target_class = \"Detailed\"  # Broad | Simplified | Detailed\n",
    "\n",
    "NORMALIZE  = \"row\"   # None | \"row\" | \"col\"\n",
    "CBAR_LABEL = \"% Agreement\"\n",
    "\n",
    "authors_to_do = [\"Hao\", \"Luecken\", \"Zhang\", \"Triana\"]\n",
    "\n",
    "def render_heatmap(csv_path: Path, normalize: str = \"row\", cbar_label: str = \"% Agreement\"):\n",
    "    cm_df = pd.read_csv(csv_path, index_col=0)\n",
    "    labels_true = list(cm_df.index)\n",
    "    labels_pred = list(cm_df.columns)\n",
    "    cm = cm_df.values.astype(float)\n",
    "\n",
    "    # ---------- NORMALISE ----------\n",
    "    if normalize is None:\n",
    "        data = cm\n",
    "    else:\n",
    "        if normalize == \"row\":\n",
    "            denom = cm.sum(axis=1, keepdims=True)\n",
    "        elif normalize == \"col\":\n",
    "            denom = cm.sum(axis=0, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(\"normalize must be None | 'row' | 'col'\")\n",
    "        denom[denom == 0] = 1.0\n",
    "        data = (cm / denom) * 100.0\n",
    "\n",
    "    # ---------- OUTPUT PATHS ----------\n",
    "    # csv_path: .../Release/<Author>/Reports/<Depth>/metrics/<file.csv>\n",
    "    depth_root = csv_path.parents[2]      # .../Release/<Author>/Reports/<Depth>\n",
    "    figures_dir = depth_root / \"Figures\"  # .../Release/<Author>/Reports/<Depth>/Figures\n",
    "    figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_base = figures_dir / \"Multiclass_models_confusion_matrix_on_test_with_percentage_agreement\"\n",
    "    png_path = out_base.with_suffix(\".png\")\n",
    "    pdf_path = out_base.with_suffix(\".pdf\")\n",
    "\n",
    "    # ---------- PLOT ----------\n",
    "    n_cols = len(labels_pred)\n",
    "    n_rows = len(labels_true)\n",
    "\n",
    "    fig_w = max(5, 0.6 * n_cols + 2)\n",
    "    fig_h = max(5, 0.6 * n_rows + 2)\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h), constrained_layout=False)\n",
    "\n",
    "    im = ax.imshow(data, cmap=\"magma\", aspect=\"auto\", interpolation=\"nearest\")\n",
    "\n",
    "    ax.set_xticks(np.arange(n_cols))\n",
    "    ax.set_yticks(np.arange(n_rows))\n",
    "    ax.set_xticklabels(labels_pred, rotation=45, ha=\"right\", fontsize=20)\n",
    "    ax.set_yticklabels(labels_true, fontsize=20)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\n",
    "    ax.set_xlim(-0.5, n_cols - 0.5)\n",
    "    ax.set_ylim(n_rows - 0.5, -0.5)\n",
    "\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    ax.set_xticks(np.arange(-0.5, n_cols, 1.0), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, n_rows, 1.0), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"#373737\", linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"2.5%\", pad=0.02)\n",
    "    cb = plt.colorbar(im, cax=cax)\n",
    "    cb.set_label(cbar_label, fontsize=20)\n",
    "    cb.ax.tick_params(labelsize=20, length=3, width=0.6)\n",
    "    for spine in cax.spines.values():\n",
    "        spine.set_linewidth(0.6)\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(\"#000000\")\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(png_path, dpi=300)\n",
    "    plt.savefig(pdf_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"[SAVE] {png_path}\")\n",
    "    print(f\"[SAVE] {pdf_path}\")\n",
    "\n",
    "# Infer assay root\n",
    "# cm_csv_path: .../<Assay>/Release/Hao/Reports/Detailed/metrics/<file.csv>\n",
    "# assay_root should be: .../<Assay>/Release\n",
    "assay_root = cm_csv_path.parents[5]\n",
    "\n",
    "for author in authors_to_do:\n",
    "    csv = (\n",
    "        assay_root\n",
    "        / author\n",
    "        / \"Release\"\n",
    "        / name_target_class\n",
    "        / \"Reports\"\n",
    "        / \"metrics\"\n",
    "        / \"Multiclass_models_confusion_matrix_on_test.csv\"\n",
    "    )\n",
    "    if not csv.exists():\n",
    "        print(f\"[SKIP] Missing: {csv}\")\n",
    "        continue\n",
    "    render_heatmap(csv, normalize=NORMALIZE, cbar_label=CBAR_LABEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mosaic_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
