{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "script_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "parent_dir = os.path.dirname(script_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import anndata\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import harmonypy as hm\n",
    "import seaborn as sns\n",
    "import espressopro as ep\n",
    "\n",
    "##\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "##\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "# Get the number of CPU cores\n",
    "num_cores = multiprocessing.cpu_count()-2\n",
    "\n",
    "print(f\"Total CPU cores to be used: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading custom scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(parent_dir + '/Scripts/SingleCellProcessing')\n",
    "\n",
    "import SCUtils\n",
    "\n",
    "import sys\n",
    "sys.path.append(parent_dir + '/Scripts/ModelTraining')\n",
    "\n",
    "import MLTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(dataset, reduction, n_neighbors, label_input, label_output, frequency_threshold):\n",
    "    # Compute the neighborhood graph\n",
    "    sc.pp.neighbors(dataset, use_rep=reduction, n_neighbors=n_neighbors)\n",
    "\n",
    "    # Perform the clustering\n",
    "    sc.tl.leiden(dataset, key_added='clusters', resolution=10)\n",
    "\n",
    "    # Initialize the new column with the existing labels\n",
    "    dataset.obs[label_output] = dataset.obs[label_input]\n",
    "\n",
    "    # For each cluster, find the most frequent label and assign it to all cells in the cluster\n",
    "    for cluster in dataset.obs['clusters'].unique():\n",
    "        cluster_labels = dataset.obs.loc[dataset.obs['clusters'] == cluster, label_input]\n",
    "        most_frequent_label = cluster_labels.mode()[0]\n",
    "        frequency = (cluster_labels == most_frequent_label).mean()\n",
    "\n",
    "        if frequency > frequency_threshold:\n",
    "            dataset.obs.loc[dataset.obs['clusters'] == cluster, label_output] = most_frequent_label\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTHONHASHSEED was set as envinronmental variable to 0 as follows:\n",
    "    \n",
    "    conda env config vars set PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_pythonhashseed(seed=0):\n",
    "    current_seed = os.environ.get(\"PYTHONHASHSEED\")\n",
    "\n",
    "    seed = str(seed)\n",
    "    if current_seed is None or current_seed != seed:\n",
    "        print(f'Setting PYTHONHASHSEED=\"{seed}\"')\n",
    "        os.environ[\"PYTHONHASHSEED\"] = seed\n",
    "        # restart the current process\n",
    "        os.execl(sys.executable, sys.executable, *sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "hash = random.getrandbits(128)\n",
    "\n",
    "print(\"hash value: %032x\" % hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path\n",
    "data_path = parent_dir + \"/Data\"\n",
    "figures_path = parent_dir + \"/Figures\"\n",
    "\n",
    "# Create the folder\n",
    "os.makedirs(data_path + \"/Pre_trained_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_barcodes_path = data_path + \"/Training_barcodes\"\n",
    "train_barcodes_path\n",
    "test_barcodes_path = data_path + \"/Testing_barcodes\"\n",
    "test_barcodes_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Hao Y. et al. (2021) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_dataset_Train = sc.read_h5ad(data_path + \"/References/Hao\" + \"/228AB_healthy_donors_PBMNCs_annotated_Train.h5ad\")\n",
    "Hao_dataset_Test = sc.read_h5ad(data_path + \"/References/Hao\" + \"/228AB_healthy_donors_PBMNCs_annotated_Test.h5ad\")\n",
    "Hao_dataset_Cal = sc.read_h5ad(data_path + \"/References/Hao\" + \"/228AB_healthy_donors_PBMNCs_annotated_Cal.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_dataset_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc_context\n",
    "import scanpy as sc\n",
    "\n",
    "# --- Config ---\n",
    "label_key = 'Consensus_annotation_detailed_final'\n",
    "basis_key = 'X_wnn.umap'\n",
    "color_key = f'{label_key}_colors'\n",
    "\n",
    "# Your custom palette (label -> hex)\n",
    "custom_palette = {\n",
    "    'B Memory': \"#68D827\",\n",
    "    'B Naive': '#1C511D',\n",
    "    'CD14 Mono': \"#D27CE3\",\n",
    "    'CD16 Mono': \"#8D43CD\",\n",
    "    'CD4 T Memory': \"#C1AF93\",\n",
    "    'CD4 T Naive': \"#C99546\",\n",
    "    'CD8 T Memory': \"#6B3317\",\n",
    "    'CD8 T Naive': \"#4D382E\",\n",
    "    'ErP': \"#D1235A\",\n",
    "    'Erythroblast': \"#F30A1A\",\n",
    "    'GMP': \"#C5E4FF\",\n",
    "    'HSC_MPP': '#0079ea',\n",
    "    'Immature B': \"#91FF7B\",\n",
    "    'LMPP': \"#17BECF\",\n",
    "    'MAIT': \"#BCBD22\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'NK CD56 bright': \"#F3AC1F\",\n",
    "    'NK CD56 dim': \"#FBEF0D\",\n",
    "    'Plasma': \"#9DC012\",\n",
    "    'Pro-B': \"#66BB6A\",\n",
    "    'Small': \"#292929\",\n",
    "    'cDC1': \"#76A7CB\",\n",
    "    'cDC2': \"#16D2E3\",\n",
    "    'gdT': \"#EDB416\",\n",
    "    'pDC': \"#69FFCB\",\n",
    "    'CD4 CTL': \"#D7D2CB\",\n",
    "    'MEP': \"#E364B0\",\n",
    "    'Pre-B': \"#2DBD67\",\n",
    "    'Pre-Pro-B': '#92AC8E',\n",
    "    'EoBaMaP': \"#728245\",\n",
    "    'MkP': \"#69424D\",\n",
    "    'Stroma': \"#727272\",\n",
    "    'Macrophage': \"#5F4761\",\n",
    "    'ILC': \"#F7CF94\",\n",
    "    'dnT': \"#504423\",\n",
    "    'Treg': \"#6E6C37\",\n",
    "    'Platelet': \"#FF39A6\",\n",
    "}\n",
    "\n",
    "# --- Ensure categorical dtype ---\n",
    "if not pd.api.types.is_categorical_dtype(Hao_dataset_Train.obs[label_key]):\n",
    "    Hao_dataset_Train.obs[label_key] = Hao_dataset_Train.obs[label_key].astype('category')\n",
    "\n",
    "cats = list(Hao_dataset_Train.obs[label_key].cat.categories)\n",
    "\n",
    "# --- Sanity checks: missing/extra labels ---\n",
    "labels_in_palette = set(custom_palette.keys())\n",
    "labels_in_data = set(cats)\n",
    "\n",
    "missing_in_palette = [c for c in cats if c not in labels_in_palette]\n",
    "extra_in_palette   = [c for c in custom_palette.keys() if c not in labels_in_data]\n",
    "\n",
    "if missing_in_palette:\n",
    "    print(\"[WARN] Missing colors for:\", missing_in_palette, \"-> will use light grey (#cccccc).\")\n",
    "if extra_in_palette:\n",
    "    print(\"[INFO] Palette has unused entries:\", extra_in_palette)\n",
    "\n",
    "# --- Build palette list in *category order* ---\n",
    "fallback = '#cccccc'\n",
    "palette_list = [custom_palette.get(c, fallback) for c in cats]\n",
    "\n",
    "# --- Save onto .uns so Scanpy uses it consistently elsewhere ---\n",
    "Hao_dataset_Train.uns[color_key] = palette_list\n",
    "\n",
    "# --- Plot with Scanpy using built-in outlines (no clustering) ---\n",
    "with rc_context({\"figure.figsize\": (5.2, 4.5)}):\n",
    "    sc.pl.embedding(\n",
    "        Hao_dataset_Train,\n",
    "        basis=basis_key,\n",
    "        color=label_key,\n",
    "        palette=palette_list,\n",
    "        legend_loc='on data',\n",
    "        legend_fontsize=10,\n",
    "        legend_fontoutline=1.5,\n",
    "        size=10,\n",
    "        add_outline=True,   # built-in group outlines\n",
    "        frameon=True,\n",
    "        title='Hao',\n",
    "        show=False,\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_dataset_Train.X = ep.Normalise_protein_data(Hao_dataset_Train.X)\n",
    "Hao_dataset_Test.X = ep.Normalise_protein_data(Hao_dataset_Test.X)\n",
    "Hao_dataset_Cal.X = ep.Normalise_protein_data(Hao_dataset_Cal.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(Hao_dataset_Train, 'Consensus_annotation_detailed_final', method='wilcoxon')\n",
    "sc.pl.rank_genes_groups(Hao_dataset_Train, n_genes=10, sharey=False, ncols = 3, fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.rank_genes_groups_matrixplot(Hao_dataset_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_dataset_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Hao_dataset_Train, keys='CD56', groupby='celltype.l2', rotation=90, use_raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Hao_dataset_Train, keys='CD56', groupby='Consensus_annotation_detailed_final', rotation=90, use_raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_data_Train = pd.DataFrame(Hao_dataset_Train.X, index=Hao_dataset_Train.obs_names, columns=Hao_dataset_Train.var_names)\n",
    "Hao_data_Test = pd.DataFrame(Hao_dataset_Test.X, index=Hao_dataset_Test.obs_names, columns=Hao_dataset_Test.var_names)\n",
    "Hao_data_Cal = pd.DataFrame(Hao_dataset_Cal.X, index=Hao_dataset_Cal.obs_names, columns=Hao_dataset_Cal.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these are the columns to be dropped\n",
    "columns_to_drop = [\"IgD\",\"IgM\", \"Rag-IgG2c\"]\n",
    "Hao_data_Train = Hao_data_Train.drop(columns=columns_to_drop, errors='ignore')\n",
    "Hao_data_Test = Hao_data_Test.drop(columns=columns_to_drop, errors='ignore')\n",
    "Hao_data_Cal = Hao_data_Cal.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Zhang X. et al. (2024) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zhang_dataset_Train = sc.read_h5ad(data_path + \"/References/Zhang\" + \"/Zhang_adata_annotated_Train.h5ad\")\n",
    "Zhang_dataset_Test = sc.read_h5ad(data_path + \"/References/Zhang\" + \"/Zhang_adata_annotated_Test.h5ad\")\n",
    "Zhang_dataset_Cal = sc.read_h5ad(data_path + \"/References/Zhang\" + \"/Zhang_adata_annotated_Cal.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zhang_dataset_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc_context\n",
    "import scanpy as sc\n",
    "\n",
    "# --- Config ---\n",
    "label_key = 'Consensus_annotation_detailed_final'\n",
    "basis_key = 'X_umap'\n",
    "color_key = f'{label_key}_colors'\n",
    "\n",
    "# Your custom palette (label -> hex)\n",
    "custom_palette = {\n",
    "    'B Memory': \"#68D827\",\n",
    "    'B Naive': '#1C511D',\n",
    "    'CD14 Mono': \"#D27CE3\",\n",
    "    'CD16 Mono': \"#8D43CD\",\n",
    "    'CD4 T Memory': \"#C1AF93\",\n",
    "    'CD4 T Naive': \"#C99546\",\n",
    "    'CD8 T Memory': \"#6B3317\",\n",
    "    'CD8 T Naive': \"#4D382E\",\n",
    "    'ErP': \"#D1235A\",\n",
    "    'Erythroblast': \"#F30A1A\",\n",
    "    'GMP': \"#C5E4FF\",\n",
    "    'HSC_MPP': '#0079ea',\n",
    "    'Immature B': \"#91FF7B\",\n",
    "    'LMPP': \"#17BECF\",\n",
    "    'MAIT': \"#BCBD22\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'NK CD56 bright': \"#F3AC1F\",\n",
    "    'NK CD56 dim': \"#FBEF0D\",\n",
    "    'Plasma': \"#9DC012\",\n",
    "    'Pro-B': \"#66BB6A\",\n",
    "    'Small': \"#292929\",\n",
    "    'cDC1': \"#76A7CB\",\n",
    "    'cDC2': \"#16D2E3\",\n",
    "    'gdT': \"#EDB416\",\n",
    "    'pDC': \"#69FFCB\",\n",
    "    'CD4 CTL': \"#D7D2CB\",\n",
    "    'MEP': \"#E364B0\",\n",
    "    'Pre-B': \"#2DBD67\",\n",
    "    'Pre-Pro-B': '#92AC8E',\n",
    "    'EoBaMaP': \"#728245\",\n",
    "    'MkP': \"#69424D\",\n",
    "    'Stroma': \"#727272\",\n",
    "    'Macrophage': \"#5F4761\",\n",
    "    'ILC': \"#F7CF94\",\n",
    "    'dnT': \"#504423\",\n",
    "}\n",
    "\n",
    "# --- Ensure categorical dtype ---\n",
    "if not pd.api.types.is_categorical_dtype(Zhang_dataset_Train.obs[label_key]):\n",
    "    Zhang_dataset_Train.obs[label_key] = Zhang_dataset_Train.obs[label_key].astype('category')\n",
    "\n",
    "cats = list(Zhang_dataset_Train.obs[label_key].cat.categories)\n",
    "\n",
    "# --- Sanity checks: missing/extra labels ---\n",
    "labels_in_palette = set(custom_palette.keys())\n",
    "labels_in_data = set(cats)\n",
    "\n",
    "missing_in_palette = [c for c in cats if c not in labels_in_palette]\n",
    "extra_in_palette   = [c for c in custom_palette.keys() if c not in labels_in_data]\n",
    "\n",
    "if missing_in_palette:\n",
    "    print(\"[WARN] Missing colors for:\", missing_in_palette, \"-> will use light grey (#cccccc).\")\n",
    "if extra_in_palette:\n",
    "    print(\"[INFO] Palette has unused entries:\", extra_in_palette)\n",
    "\n",
    "# --- Build palette list in *category order* ---\n",
    "fallback = '#cccccc'\n",
    "palette_list = [custom_palette.get(c, fallback) for c in cats]\n",
    "\n",
    "# --- Save onto .uns so Scanpy uses it consistently elsewhere ---\n",
    "Zhang_dataset_Train.uns[color_key] = palette_list\n",
    "\n",
    "# --- Plot with Scanpy using built-in outlines (no clustering) ---\n",
    "with rc_context({\"figure.figsize\": (5.5, 4.5)}):\n",
    "    sc.pl.embedding(\n",
    "        Zhang_dataset_Train,\n",
    "        basis=basis_key,\n",
    "        color=label_key,\n",
    "        palette=palette_list,\n",
    "        legend_loc='on data',\n",
    "        legend_fontsize=10,\n",
    "        legend_fontoutline=1.5,\n",
    "        size=10,\n",
    "        add_outline=True,   # built-in group outlines\n",
    "        frameon=True,\n",
    "        title='Zhang',\n",
    "        show=False,\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zhang_dataset_Train.X = ep.Normalise_protein_data(Zhang_dataset_Train.X)\n",
    "Zhang_dataset_Test.X = ep.Normalise_protein_data(Zhang_dataset_Test.X)\n",
    "Zhang_dataset_Cal.X = ep.Normalise_protein_data(Zhang_dataset_Cal.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(Zhang_dataset_Train, 'Consensus_annotation_detailed_final', method='wilcoxon')\n",
    "sc.pl.rank_genes_groups(Zhang_dataset_Train, n_genes=10, sharey=False, ncols = 3, fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Zhang_dataset_Train, keys='CD123', groupby='Consensus_annotation_broad_final', rotation=90, use_raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zhang_data_Train = pd.DataFrame(Zhang_dataset_Train.X, index=Zhang_dataset_Train.obs_names, columns=Zhang_dataset_Train.var_names)\n",
    "Zhang_data_Test = pd.DataFrame(Zhang_dataset_Test.X, index=Zhang_dataset_Test.obs_names, columns=Zhang_dataset_Test.var_names)\n",
    "Zhang_data_Cal = pd.DataFrame(Zhang_dataset_Cal.X, index=Zhang_dataset_Cal.obs_names, columns=Zhang_dataset_Cal.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these are the columns to be dropped\n",
    "columns_to_drop = [\"IgG.Fc\", \"Isotype_G0114F7\", \"Isotype_HTK888\",\n",
    "                   \"Isotype_MOPC.173\", \"Isotype_MOPC.21\", \"Isotype_MPC.11\",\n",
    "                   \"Isotype_RTK2071\", \"Isotype_RTK2758\", \"Isotype_RTK4174\",\n",
    "                   \"Isotype_RTK4530\"]\n",
    "Zhang_data_Train = Zhang_data_Train.drop(columns=columns_to_drop, errors='ignore')\n",
    "Zhang_data_Test = Zhang_data_Test.drop(columns=columns_to_drop, errors='ignore')\n",
    "Zhang_data_Cal = Zhang_data_Cal.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Triana S. et al. (2021) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triana_dataset_Train = sc.read_h5ad(data_path + \"/References/Triana\" + \"/97AB_young_and_old_adult_healthy_donor_BMMNCs_annotated_Train.h5ad\")\n",
    "Triana_dataset_Test = sc.read_h5ad(data_path + \"/References/Triana\" + \"/97AB_young_and_old_adult_healthy_donor_BMMNCs_annotated_Test.h5ad\")\n",
    "Triana_dataset_Cal = sc.read_h5ad(data_path + \"/References/Triana\" + \"/97AB_young_and_old_adult_healthy_donor_BMMNCs_annotated_Cal.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triana_dataset_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc_context\n",
    "import scanpy as sc\n",
    "\n",
    "# --- Config ---\n",
    "label_key = 'Consensus_annotation_detailed_final'\n",
    "basis_key = 'X_mofaumap'\n",
    "color_key = f'{label_key}_colors'\n",
    "\n",
    "# Your custom palette (label -> hex)\n",
    "custom_palette = {\n",
    "    'B Memory': \"#68D827\",\n",
    "    'B Naive': '#1C511D',\n",
    "    'CD14 Mono': \"#D27CE3\",\n",
    "    'CD16 Mono': \"#8D43CD\",\n",
    "    'CD4 T Memory': \"#C1AF93\",\n",
    "    'CD4 T Naive': \"#C99546\",\n",
    "    'CD8 T Memory': \"#6B3317\",\n",
    "    'CD8 T Naive': \"#4D382E\",\n",
    "    'ErP': \"#D1235A\",\n",
    "    'Erythroblast': \"#F30A1A\",\n",
    "    'GMP': \"#C5E4FF\",\n",
    "    'HSC_MPP': '#0079ea',\n",
    "    'Immature B': \"#91FF7B\",\n",
    "    'LMPP': \"#17BECF\",\n",
    "    'MAIT': \"#BCBD22\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'NK CD56 bright': \"#F3AC1F\",\n",
    "    'NK CD56 dim': \"#FBEF0D\",\n",
    "    'Plasma': \"#9DC012\",\n",
    "    'Pro-B': \"#66BB6A\",\n",
    "    'Small': \"#292929\",\n",
    "    'cDC1': \"#76A7CB\",\n",
    "    'cDC2': \"#16D2E3\",\n",
    "    'gdT': \"#EDB416\",\n",
    "    'pDC': \"#69FFCB\",\n",
    "    'CD4 CTL': \"#D7D2CB\",\n",
    "    'MEP': \"#E364B0\",\n",
    "    'Pre-B': \"#2DBD67\",\n",
    "    'Pre-Pro-B': '#92AC8E',\n",
    "    'EoBaMaP': \"#728245\",\n",
    "    'MkP': \"#69424D\",\n",
    "    'Stroma': \"#727272\",\n",
    "    'Macrophage': \"#5F4761\",\n",
    "    'ILC': \"#F7CF94\",\n",
    "    'dnT': \"#504423\",\n",
    "    'Treg': \"#6E6C37\",\n",
    "    'Platelet': \"#FF39A6\",\n",
    "}\n",
    "\n",
    "# --- Ensure categorical dtype ---\n",
    "if not pd.api.types.is_categorical_dtype(Triana_dataset_Train.obs[label_key]):\n",
    "    Triana_dataset_Train.obs[label_key] = Triana_dataset_Train.obs[label_key].astype('category')\n",
    "\n",
    "cats = list(Triana_dataset_Train.obs[label_key].cat.categories)\n",
    "\n",
    "# --- Sanity checks: missing/extra labels ---\n",
    "labels_in_palette = set(custom_palette.keys())\n",
    "labels_in_data = set(cats)\n",
    "\n",
    "missing_in_palette = [c for c in cats if c not in labels_in_palette]\n",
    "extra_in_palette   = [c for c in custom_palette.keys() if c not in labels_in_data]\n",
    "\n",
    "if missing_in_palette:\n",
    "    print(\"[WARN] Missing colors for:\", missing_in_palette, \"-> will use light grey (#cccccc).\")\n",
    "if extra_in_palette:\n",
    "    print(\"[INFO] Palette has unused entries:\", extra_in_palette)\n",
    "\n",
    "# --- Build palette list in *category order* ---\n",
    "fallback = '#cccccc'\n",
    "palette_list = [custom_palette.get(c, fallback) for c in cats]\n",
    "\n",
    "# --- Save onto .uns so Scanpy uses it consistently elsewhere ---\n",
    "Triana_dataset_Train.uns[color_key] = palette_list\n",
    "\n",
    "# --- Plot with Scanpy using built-in outlines (no clustering) ---\n",
    "with rc_context({\"figure.figsize\": (5.25, 4.5)}):\n",
    "    sc.pl.embedding(\n",
    "        Triana_dataset_Train,\n",
    "        basis=basis_key,\n",
    "        color=label_key,\n",
    "        palette=palette_list,\n",
    "        legend_loc='on data',\n",
    "        legend_fontsize=10,\n",
    "        legend_fontoutline=1.5,\n",
    "        size=10,\n",
    "        add_outline=True,   # built-in group outlines\n",
    "        frameon=True,\n",
    "        title='Triana',\n",
    "        show=False,\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triana_dataset_Train.X = ep.Normalise_protein_data(Triana_dataset_Train.X)\n",
    "Triana_dataset_Test.X = ep.Normalise_protein_data(Triana_dataset_Test.X)\n",
    "Triana_dataset_Cal.X = ep.Normalise_protein_data(Triana_dataset_Cal.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(Triana_dataset_Train, 'Consensus_annotation_simplified_final', method='wilcoxon')\n",
    "sc.pl.rank_genes_groups(Triana_dataset_Train, n_genes=10, sharey=False, ncols = 3, fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Triana_dataset_Train, keys='CD133', groupby='Consensus_annotation_detailed_final', rotation=90, use_raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triana_data_Train = pd.DataFrame(Triana_dataset_Train.X, index=Triana_dataset_Train.obs_names, columns=Triana_dataset_Train.var_names)\n",
    "Triana_data_Test = pd.DataFrame(Triana_dataset_Test.X, index=Triana_dataset_Test.obs_names, columns=Triana_dataset_Test.var_names)\n",
    "Triana_data_Cal = pd.DataFrame(Triana_dataset_Cal.X, index=Triana_dataset_Cal.obs_names, columns=Triana_dataset_Cal.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these are the columns to be dropped\n",
    "columns_to_drop = [\"IgG\", \"IgD\"]\n",
    "Triana_data_Train = Triana_data_Train.drop(columns=columns_to_drop, errors='ignore')\n",
    "Triana_data_Test = Triana_data_Test.drop(columns=columns_to_drop, errors='ignore')\n",
    "Triana_data_Cal = Triana_data_Cal.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Luecken M.D. et al. (2021) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luecken_dataset_Train = sc.read_h5ad(data_path + \"/References/Luecken\" + \"/140AB_adult_healthy_donor_BMMNCs_annotated_Train.h5ad\")\n",
    "Luecken_dataset_Test = sc.read_h5ad(data_path + \"/References/Luecken\" + \"/140AB_adult_healthy_donor_BMMNCs_annotated_Test.h5ad\")\n",
    "Luecken_dataset_Cal = sc.read_h5ad(data_path + \"/References/Luecken\" + \"/140AB_adult_healthy_donor_BMMNCs_annotated_Cal.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luecken_dataset_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc_context\n",
    "import scanpy as sc\n",
    "\n",
    "# --- Config ---\n",
    "label_key = 'Consensus_annotation_detailed_final'\n",
    "basis_key = 'X_umap'\n",
    "color_key = f'{label_key}_colors'\n",
    "\n",
    "# Your custom palette (label -> hex)\n",
    "custom_palette = {\n",
    "    'B Memory': \"#68D827\",\n",
    "    'B Naive': '#1C511D',\n",
    "    'CD14 Mono': \"#D27CE3\",\n",
    "    'CD16 Mono': \"#8D43CD\",\n",
    "    'CD4 T Memory': \"#C1AF93\",\n",
    "    'CD4 T Naive': \"#C99546\",\n",
    "    'CD8 T Memory': \"#6B3317\",\n",
    "    'CD8 T Naive': \"#4D382E\",\n",
    "    'ErP': \"#D1235A\",\n",
    "    'Erythroblast': \"#F30A1A\",\n",
    "    'GMP': \"#C5E4FF\",\n",
    "    'HSC_MPP': '#0079ea',\n",
    "    'Immature B': \"#91FF7B\",\n",
    "    'LMPP': \"#17BECF\",\n",
    "    'MAIT': \"#BCBD22\",\n",
    "    'Myeloid progenitor': \"#AEC7E8\",\n",
    "    'NK CD56 bright': \"#F3AC1F\",\n",
    "    'NK CD56 dim': \"#FBEF0D\",\n",
    "    'Plasma': \"#9DC012\",\n",
    "    'Pro-B': \"#66BB6A\",\n",
    "    'Small': \"#292929\",\n",
    "    'cDC1': \"#76A7CB\",\n",
    "    'cDC2': \"#16D2E3\",\n",
    "    'gdT': \"#EDB416\",\n",
    "    'pDC': \"#69FFCB\",\n",
    "    'CD4 CTL': \"#D7D2CB\",\n",
    "    'MEP': \"#E364B0\",\n",
    "    'Pre-B': \"#2DBD67\",\n",
    "    'Pre-Pro-B': '#92AC8E',\n",
    "    'EoBaMaP': \"#728245\",\n",
    "    'MkP': \"#69424D\",\n",
    "    'Stroma': \"#727272\",\n",
    "    'Macrophage': \"#5F4761\",\n",
    "    'ILC': \"#F7CF94\",\n",
    "    'dnT': \"#504423\",\n",
    "    'Treg': \"#6E6C37\",\n",
    "    'Platelet': \"#FF39A6\",\n",
    "}\n",
    "\n",
    "# --- Ensure categorical dtype ---\n",
    "if not pd.api.types.is_categorical_dtype(Luecken_dataset_Train.obs[label_key]):\n",
    "    Luecken_dataset_Train.obs[label_key] = Luecken_dataset_Train.obs[label_key].astype('category')\n",
    "\n",
    "cats = list(Luecken_dataset_Train.obs[label_key].cat.categories)\n",
    "\n",
    "# --- Sanity checks: missing/extra labels ---\n",
    "labels_in_palette = set(custom_palette.keys())\n",
    "labels_in_data = set(cats)\n",
    "\n",
    "missing_in_palette = [c for c in cats if c not in labels_in_palette]\n",
    "extra_in_palette   = [c for c in custom_palette.keys() if c not in labels_in_data]\n",
    "\n",
    "if missing_in_palette:\n",
    "    print(\"[WARN] Missing colors for:\", missing_in_palette, \"-> will use light grey (#cccccc).\")\n",
    "if extra_in_palette:\n",
    "    print(\"[INFO] Palette has unused entries:\", extra_in_palette)\n",
    "\n",
    "# --- Build palette list in *category order* ---\n",
    "fallback = '#cccccc'\n",
    "palette_list = [custom_palette.get(c, fallback) for c in cats]\n",
    "\n",
    "# --- Save onto .uns so Scanpy uses it consistently elsewhere ---\n",
    "Luecken_dataset_Train.uns[color_key] = palette_list\n",
    "\n",
    "# --- Plot with Scanpy using built-in outlines (no clustering) ---\n",
    "with rc_context({\"figure.figsize\": (5, 4.5)}):\n",
    "    sc.pl.embedding(\n",
    "        Luecken_dataset_Train,\n",
    "        basis=basis_key,\n",
    "        color=label_key,\n",
    "        palette=palette_list,\n",
    "        legend_loc='on data',\n",
    "        legend_fontsize=10,\n",
    "        legend_fontoutline=1.5,\n",
    "        size=10,\n",
    "        add_outline=True,   # built-in group outlines\n",
    "        frameon=True,\n",
    "        title='Luecken',\n",
    "        show=False,\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luecken_dataset_Train.X = ep.Normalise_protein_data(Luecken_dataset_Train.X)\n",
    "Luecken_dataset_Test.X = ep.Normalise_protein_data(Luecken_dataset_Test.X)\n",
    "Luecken_dataset_Cal.X = ep.Normalise_protein_data(Luecken_dataset_Cal.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(Luecken_dataset_Train, 'Consensus_annotation_detailed_final', method='wilcoxon')\n",
    "sc.pl.rank_genes_groups(Luecken_dataset_Train, n_genes=10, sharey=False, ncols = 3, fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Luecken_dataset_Train, keys='CD49b', groupby='Consensus_annotation_detailed_final', rotation=90, use_raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luecken_data_Train = pd.DataFrame(Luecken_dataset_Train.X, index=Luecken_dataset_Train.obs_names, columns=Luecken_dataset_Train.var_names)\n",
    "Luecken_data_Test = pd.DataFrame(Luecken_dataset_Test.X, index=Luecken_dataset_Test.obs_names, columns=Luecken_dataset_Test.var_names)\n",
    "Luecken_data_Cal = pd.DataFrame(Luecken_dataset_Cal.X, index=Luecken_dataset_Cal.obs_names, columns=Luecken_dataset_Cal.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these are the columns to be dropped\n",
    "columns_to_drop = [\"IgG\", \"IgM\", \"IgD\"]\n",
    "Luecken_data_Train = Luecken_data_Train.drop(columns=columns_to_drop, errors='ignore')\n",
    "Luecken_data_Test = Luecken_data_Test.drop(columns=columns_to_drop, errors='ignore')\n",
    "Luecken_data_Cal = Luecken_data_Cal.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading antibodies panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalSeqD_Heme_Oncology_CAT399906 = pd.read_csv(data_path + \"/Antibodies_panels/TotalSeqD_Heme_Oncology_CAT399906.csv\", index_col=0).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TotalSeqD Heme Oncology CAT399906 Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_path + '/Pre_trained_models/TotalSeqD_Heme_Oncology_CAT399906'\n",
    "os.makedirs(data_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hao Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "os.makedirs(data_path + \"/Hao\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Hao/Models\", exist_ok=True)\n",
    "\n",
    "models_output = data_path + \"/Hao\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hao_Models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broad annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Hao_data_Train, Hao_data_Test, Hao_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Hao_dataset_Train, Hao_dataset_Test, Hao_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Broad\"   # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Hao_data_Train = attach_celltype(Hao_data_Train, Hao_dataset_Train, consensus_field)\n",
    "Hao_data_Test  = attach_celltype(Hao_data_Test,  Hao_dataset_Test,  consensus_field)\n",
    "Hao_data_Cal   = attach_celltype(Hao_data_Cal,   Hao_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Hao_data_Train = _rename_data_to_panel(Hao_data_Train)\n",
    "Hao_data_Test  = _rename_data_to_panel(Hao_data_Test)\n",
    "Hao_data_Cal   = _rename_data_to_panel(Hao_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Hao_data_Train = _panel_intersection(Hao_data_Train)\n",
    "Hao_data_Test  = _panel_intersection(Hao_data_Test)\n",
    "Hao_data_Cal   = _panel_intersection(Hao_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Hao_data_Cal_lbl = Hao_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Cal.columns]\n",
    "\n",
    "Hao_data_Train_Sub = Hao_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Hao_data_Test_Sub  = Hao_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Hao_data_Cal_Sub   = Hao_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Hao_data_Train_Sub.columns)\n",
    "if list(Hao_data_Test_Sub.columns) != cols_train or list(Hao_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Hao_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Hao_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Hao_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# Consistent class order\n",
    "class_names  = sorted(pd.Series(Hao_data_Train[\"Celltype\"]).dropna().unique())\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# Multiclass labels arrays\n",
    "s_cal = Hao_data_Cal_lbl[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Hao_data_Cal_lbl.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Hao_data_Test[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Hao_data_Test.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Hao_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Hao_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Hao_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Hao/Consensus_annotation_broad_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Hao_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Hao_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Hao/Consensus_annotation_broad_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Hao_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Hao_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Hao_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Hao_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Hao\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal, y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te,    index=test_index, columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc, index=test_index, columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Hao_data_Test[\"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te.argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc.argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS):\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation\n",
    "y_pred_mc = P_te_mc.argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Hao_data_Train, Hao_data_Test, Hao_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Hao_dataset_Train, Hao_dataset_Test, Hao_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Simplified\"   # \"simplified\" | \"Simplified\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Hao_data_Train = attach_celltype(Hao_data_Train, Hao_dataset_Train, consensus_field)\n",
    "Hao_data_Test  = attach_celltype(Hao_data_Test,  Hao_dataset_Test,  consensus_field)\n",
    "Hao_data_Cal   = attach_celltype(Hao_data_Cal,   Hao_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Hao_data_Train = _rename_data_to_panel(Hao_data_Train)\n",
    "Hao_data_Test  = _rename_data_to_panel(Hao_data_Test)\n",
    "Hao_data_Cal   = _rename_data_to_panel(Hao_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Hao_data_Train = _panel_intersection(Hao_data_Train)\n",
    "Hao_data_Test  = _panel_intersection(Hao_data_Test)\n",
    "Hao_data_Cal   = _panel_intersection(Hao_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Hao_data_Cal_lbl = Hao_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Cal.columns]\n",
    "\n",
    "Hao_data_Train_Sub = Hao_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Hao_data_Test_Sub  = Hao_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Hao_data_Cal_Sub   = Hao_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Hao_data_Train_Sub.columns)\n",
    "if list(Hao_data_Test_Sub.columns) != cols_train or list(Hao_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Hao_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Hao_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Hao_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# ===== Exclude specific classes from the multiclass set and per-class loop =====\n",
    "EXCLUDE_CLASSES = {\"Macrophage\", \"ILC\", \"Stroma\"}\n",
    "\n",
    "all_classes = sorted(pd.Series(Hao_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if c not in EXCLUDE_CLASSES]\n",
    "if not class_names:\n",
    "    raise ValueError(\"After exclusions, class_names is empty.\")\n",
    "print(f\"[classes] Included ({len(class_names)}): {class_names}\")\n",
    "if missing := [c for c in all_classes if c in EXCLUDE_CLASSES]:\n",
    "    print(f\"[classes] Excluded: {missing}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# --- Multiclass labels (MASKED to included classes) ---\n",
    "# CAL\n",
    "mask_cal_mc = Hao_data_Cal_lbl[\"Celltype\"].isin(class_names)\n",
    "s_cal = Hao_data_Cal_lbl.loc[mask_cal_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Hao_data_Cal_lbl.loc[mask_cal_mc & s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "# TEST\n",
    "mask_test_mc = Hao_data_Test[\"Celltype\"].isin(class_names)\n",
    "s_te = Hao_data_Test.loc[mask_test_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Hao_data_Test.loc[mask_test_mc & s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Hao_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Hao_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats (only for included classes)\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Hao_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Hao/Consensus_annotation_simplified_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Hao_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Hao_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Hao/Consensus_annotation_simplified_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Hao_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Hao_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Hao_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Hao_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing  - labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Hao\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration (columns order = class_names)\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split (excluded classes masked out)...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "# Fit only on CAL rows whose true label is one of the included classes\n",
    "ts_cal.fit(P_cal[mask_cal_mc.values, :], y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + INCLUDED class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "# Evaluate & save on TEST rows whose true label is an INCLUDED class\n",
    "test_index_masked = Hao_data_Test_Sub.index[mask_test_mc.values]\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te[mask_test_mc.values, :],    index=test_index_masked,\n",
    "                            columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc[mask_test_mc.values, :], index=test_index_masked,\n",
    "                            columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Hao_data_Test.loc[mask_test_mc, \"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS) [included classes only]:\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST_included.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation on the masked subset\n",
    "y_pred_mc = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST, excluded classes removed):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Hao_data_Train, Hao_data_Test, Hao_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Hao_dataset_Train, Hao_dataset_Test, Hao_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Detailed\"   # \"detailed\" | \"detailed\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Hao_data_Train = attach_celltype(Hao_data_Train, Hao_dataset_Train, consensus_field)\n",
    "Hao_data_Test  = attach_celltype(Hao_data_Test,  Hao_dataset_Test,  consensus_field)\n",
    "Hao_data_Cal   = attach_celltype(Hao_data_Cal,   Hao_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Hao_data_Train = _rename_data_to_panel(Hao_data_Train)\n",
    "Hao_data_Test  = _rename_data_to_panel(Hao_data_Test)\n",
    "Hao_data_Cal   = _rename_data_to_panel(Hao_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Hao_data_Train = _panel_intersection(Hao_data_Train)\n",
    "Hao_data_Test  = _panel_intersection(Hao_data_Test)\n",
    "Hao_data_Cal   = _panel_intersection(Hao_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Hao_data_Cal_lbl = Hao_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Hao_data_Cal.columns]\n",
    "\n",
    "Hao_data_Train_Sub = Hao_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Hao_data_Test_Sub  = Hao_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Hao_data_Cal_Sub   = Hao_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Hao_data_Train_Sub.columns)\n",
    "if list(Hao_data_Test_Sub.columns) != cols_train or list(Hao_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Hao_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Hao_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Hao_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# ===== Exclude specific classes from the multiclass set and per-class loop =====\n",
    "EXCLUDE_CLASSES = {\"Macrophage\", \"ILC\", \"Stroma\", \"dnT\"}\n",
    "\n",
    "all_classes = sorted(pd.Series(Hao_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if c not in EXCLUDE_CLASSES]\n",
    "if not class_names:\n",
    "    raise ValueError(\"After exclusions, class_names is empty.\")\n",
    "print(f\"[classes] Included ({len(class_names)}): {class_names}\")\n",
    "if missing := [c for c in all_classes if c in EXCLUDE_CLASSES]:\n",
    "    print(f\"[classes] Excluded: {missing}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# --- Multiclass labels (MASKED to included classes) ---\n",
    "# CAL\n",
    "mask_cal_mc = Hao_data_Cal_lbl[\"Celltype\"].isin(class_names)\n",
    "s_cal = Hao_data_Cal_lbl.loc[mask_cal_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Hao_data_Cal_lbl.loc[mask_cal_mc & s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "# TEST\n",
    "mask_test_mc = Hao_data_Test[\"Celltype\"].isin(class_names)\n",
    "s_te = Hao_data_Test.loc[mask_test_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Hao_data_Test.loc[mask_test_mc & s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Hao_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Hao_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats (only for included classes)\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Hao_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Hao/Consensus_annotation_detailed_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Hao_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Hao_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Hao/Consensus_annotation_detailed_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Hao_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Hao_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Hao_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Hao_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing  - labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Hao\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration (columns order = class_names)\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split (excluded classes masked out)...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "# Fit only on CAL rows whose true label is one of the included classes\n",
    "ts_cal.fit(P_cal[mask_cal_mc.values, :], y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + INCLUDED class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "# Evaluate & save on TEST rows whose true label is an INCLUDED class\n",
    "test_index_masked = Hao_data_Test_Sub.index[mask_test_mc.values]\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te[mask_test_mc.values, :],    index=test_index_masked,\n",
    "                            columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc[mask_test_mc.values, :], index=test_index_masked,\n",
    "                            columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Hao_data_Test.loc[mask_test_mc, \"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS) [included classes only]:\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST_included.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation on the masked subset\n",
    "y_pred_mc = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST, excluded classes removed):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zhang Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "os.makedirs(data_path + \"/Zhang\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Zhang/Models\", exist_ok=True)\n",
    "\n",
    "models_output = data_path + \"/Zhang/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zhang_Models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broad annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Zhang_data_Train, Zhang_data_Test, Zhang_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Zhang_dataset_Train, Zhang_dataset_Test, Zhang_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Broad\"   # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Zhang_data_Train = attach_celltype(Zhang_data_Train, Zhang_dataset_Train, consensus_field)\n",
    "Zhang_data_Test  = attach_celltype(Zhang_data_Test,  Zhang_dataset_Test,  consensus_field)\n",
    "Zhang_data_Cal   = attach_celltype(Zhang_data_Cal,   Zhang_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Zhang_data_Train = _rename_data_to_panel(Zhang_data_Train)\n",
    "Zhang_data_Test  = _rename_data_to_panel(Zhang_data_Test)\n",
    "Zhang_data_Cal   = _rename_data_to_panel(Zhang_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Zhang_data_Train = _panel_intersection(Zhang_data_Train)\n",
    "Zhang_data_Test  = _panel_intersection(Zhang_data_Test)\n",
    "Zhang_data_Cal   = _panel_intersection(Zhang_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Zhang_data_Cal_lbl = Zhang_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Cal.columns]\n",
    "\n",
    "Zhang_data_Train_Sub = Zhang_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Zhang_data_Test_Sub  = Zhang_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Zhang_data_Cal_Sub   = Zhang_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Zhang_data_Train_Sub.columns)\n",
    "if list(Zhang_data_Test_Sub.columns) != cols_train or list(Zhang_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Zhang_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Zhang_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Zhang_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# Consistent class order\n",
    "class_names  = sorted(pd.Series(Zhang_data_Train[\"Celltype\"]).dropna().unique())\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# Multiclass labels arrays\n",
    "s_cal = Zhang_data_Cal_lbl[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Zhang_data_Cal_lbl.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Zhang_data_Test[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Zhang_data_Test.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Zhang_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Zhang_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Zhang_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Zhang/Consensus_annotation_broad_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Zhang_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Zhang_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Zhang/Consensus_annotation_broad_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Zhang_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Zhang_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Zhang_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Zhang_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Zhang\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal, y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te,    index=test_index, columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc, index=test_index, columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Zhang_data_Test[\"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te.argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc.argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS):\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation\n",
    "y_pred_mc = P_te_mc.argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Zhang_data_Train, Zhang_data_Test, Zhang_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Zhang_dataset_Train, Zhang_dataset_Test, Zhang_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Simplified\"   # \"simplified\" | \"Simplified\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Zhang_data_Train = attach_celltype(Zhang_data_Train, Zhang_dataset_Train, consensus_field)\n",
    "Zhang_data_Test  = attach_celltype(Zhang_data_Test,  Zhang_dataset_Test,  consensus_field)\n",
    "Zhang_data_Cal   = attach_celltype(Zhang_data_Cal,   Zhang_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Zhang_data_Train = _rename_data_to_panel(Zhang_data_Train)\n",
    "Zhang_data_Test  = _rename_data_to_panel(Zhang_data_Test)\n",
    "Zhang_data_Cal   = _rename_data_to_panel(Zhang_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Zhang_data_Train = _panel_intersection(Zhang_data_Train)\n",
    "Zhang_data_Test  = _panel_intersection(Zhang_data_Test)\n",
    "Zhang_data_Cal   = _panel_intersection(Zhang_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Zhang_data_Cal_lbl = Zhang_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Cal.columns]\n",
    "\n",
    "Zhang_data_Train_Sub = Zhang_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Zhang_data_Test_Sub  = Zhang_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Zhang_data_Cal_Sub   = Zhang_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Zhang_data_Train_Sub.columns)\n",
    "if list(Zhang_data_Test_Sub.columns) != cols_train or list(Zhang_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Zhang_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Zhang_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Zhang_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# ===== Exclude specific classes from the multiclass set and per-class loop =====\n",
    "EXCLUDE_CLASSES = {\"Macrophage\", \"ILC\", \"Stroma\"}\n",
    "\n",
    "all_classes = sorted(pd.Series(Zhang_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if c not in EXCLUDE_CLASSES]\n",
    "if not class_names:\n",
    "    raise ValueError(\"After exclusions, class_names is empty.\")\n",
    "print(f\"[classes] Included ({len(class_names)}): {class_names}\")\n",
    "if missing := [c for c in all_classes if c in EXCLUDE_CLASSES]:\n",
    "    print(f\"[classes] Excluded: {missing}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# --- Multiclass labels (MASKED to included classes) ---\n",
    "# CAL\n",
    "mask_cal_mc = Zhang_data_Cal_lbl[\"Celltype\"].isin(class_names)\n",
    "s_cal = Zhang_data_Cal_lbl.loc[mask_cal_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Zhang_data_Cal_lbl.loc[mask_cal_mc & s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "# TEST\n",
    "mask_test_mc = Zhang_data_Test[\"Celltype\"].isin(class_names)\n",
    "s_te = Zhang_data_Test.loc[mask_test_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Zhang_data_Test.loc[mask_test_mc & s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Zhang_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Zhang_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats (only for included classes)\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Zhang_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Zhang/Consensus_annotation_simplified_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Zhang_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Zhang_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Zhang/Consensus_annotation_simplified_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Zhang_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Zhang_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Zhang_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Zhang_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing  - labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Zhang\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration (columns order = class_names)\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split (excluded classes masked out)...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "# Fit only on CAL rows whose true label is one of the included classes\n",
    "ts_cal.fit(P_cal[mask_cal_mc.values, :], y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + INCLUDED class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "# Evaluate & save on TEST rows whose true label is an INCLUDED class\n",
    "test_index_masked = Zhang_data_Test_Sub.index[mask_test_mc.values]\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te[mask_test_mc.values, :],    index=test_index_masked,\n",
    "                            columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc[mask_test_mc.values, :], index=test_index_masked,\n",
    "                            columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Zhang_data_Test.loc[mask_test_mc, \"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS) [included classes only]:\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST_included.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation on the masked subset\n",
    "y_pred_mc = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST, excluded classes removed):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Zhang_data_Train, Zhang_data_Test, Zhang_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Zhang_dataset_Train, Zhang_dataset_Test, Zhang_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Detailed\"   # \"detailed\" | \"detailed\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Zhang_data_Train = attach_celltype(Zhang_data_Train, Zhang_dataset_Train, consensus_field)\n",
    "Zhang_data_Test  = attach_celltype(Zhang_data_Test,  Zhang_dataset_Test,  consensus_field)\n",
    "Zhang_data_Cal   = attach_celltype(Zhang_data_Cal,   Zhang_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Zhang_data_Train = _rename_data_to_panel(Zhang_data_Train)\n",
    "Zhang_data_Test  = _rename_data_to_panel(Zhang_data_Test)\n",
    "Zhang_data_Cal   = _rename_data_to_panel(Zhang_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Zhang_data_Train = _panel_intersection(Zhang_data_Train)\n",
    "Zhang_data_Test  = _panel_intersection(Zhang_data_Test)\n",
    "Zhang_data_Cal   = _panel_intersection(Zhang_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Zhang_data_Cal_lbl = Zhang_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Zhang_data_Cal.columns]\n",
    "\n",
    "Zhang_data_Train_Sub = Zhang_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Zhang_data_Test_Sub  = Zhang_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Zhang_data_Cal_Sub   = Zhang_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Zhang_data_Train_Sub.columns)\n",
    "if list(Zhang_data_Test_Sub.columns) != cols_train or list(Zhang_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Zhang_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Zhang_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Zhang_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# ===== Exclude specific classes from the multiclass set and per-class loop =====\n",
    "EXCLUDE_CLASSES = {\"Macrophage\", \"ILC\", \"Stroma\", \"dnT\"}\n",
    "\n",
    "all_classes = sorted(pd.Series(Zhang_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if c not in EXCLUDE_CLASSES]\n",
    "if not class_names:\n",
    "    raise ValueError(\"After exclusions, class_names is empty.\")\n",
    "print(f\"[classes] Included ({len(class_names)}): {class_names}\")\n",
    "if missing := [c for c in all_classes if c in EXCLUDE_CLASSES]:\n",
    "    print(f\"[classes] Excluded: {missing}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# --- Multiclass labels (MASKED to included classes) ---\n",
    "# CAL\n",
    "mask_cal_mc = Zhang_data_Cal_lbl[\"Celltype\"].isin(class_names)\n",
    "s_cal = Zhang_data_Cal_lbl.loc[mask_cal_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Zhang_data_Cal_lbl.loc[mask_cal_mc & s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "# TEST\n",
    "mask_test_mc = Zhang_data_Test[\"Celltype\"].isin(class_names)\n",
    "s_te = Zhang_data_Test.loc[mask_test_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Zhang_data_Test.loc[mask_test_mc & s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Zhang_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Zhang_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats (only for included classes)\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Zhang_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Zhang/Consensus_annotation_detailed_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Zhang_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Zhang_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Zhang/Consensus_annotation_detailed_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Zhang_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Zhang_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Zhang_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Zhang_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing  - labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Zhang\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration (columns order = class_names)\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split (excluded classes masked out)...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "# Fit only on CAL rows whose true label is one of the included classes\n",
    "ts_cal.fit(P_cal[mask_cal_mc.values, :], y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + INCLUDED class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "# Evaluate & save on TEST rows whose true label is an INCLUDED class\n",
    "test_index_masked = Zhang_data_Test_Sub.index[mask_test_mc.values]\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te[mask_test_mc.values, :],    index=test_index_masked,\n",
    "                            columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc[mask_test_mc.values, :], index=test_index_masked,\n",
    "                            columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Zhang_data_Test.loc[mask_test_mc, \"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS) [included classes only]:\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST_included.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation on the masked subset\n",
    "y_pred_mc = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST, excluded classes removed):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triana Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "os.makedirs(data_path + \"/Triana\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Triana/Models\", exist_ok=True)\n",
    "\n",
    "models_output = data_path + \"/Triana/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triana_Models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broad annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Triana_data_Train, Triana_data_Test, Triana_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Triana_dataset_Train, Triana_dataset_Test, Triana_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Broad\"   # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Triana_data_Train = attach_celltype(Triana_data_Train, Triana_dataset_Train, consensus_field)\n",
    "Triana_data_Test  = attach_celltype(Triana_data_Test,  Triana_dataset_Test,  consensus_field)\n",
    "Triana_data_Cal   = attach_celltype(Triana_data_Cal,   Triana_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Triana_data_Train = _rename_data_to_panel(Triana_data_Train)\n",
    "Triana_data_Test  = _rename_data_to_panel(Triana_data_Test)\n",
    "Triana_data_Cal   = _rename_data_to_panel(Triana_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Triana_data_Train = _panel_intersection(Triana_data_Train)\n",
    "Triana_data_Test  = _panel_intersection(Triana_data_Test)\n",
    "Triana_data_Cal   = _panel_intersection(Triana_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Triana_data_Cal_lbl = Triana_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Cal.columns]\n",
    "\n",
    "Triana_data_Train_Sub = Triana_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Triana_data_Test_Sub  = Triana_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Triana_data_Cal_Sub   = Triana_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Triana_data_Train_Sub.columns)\n",
    "if list(Triana_data_Test_Sub.columns) != cols_train or list(Triana_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Triana_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Triana_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Triana_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# Consistent class order\n",
    "class_names  = sorted(pd.Series(Triana_data_Train[\"Celltype\"]).dropna().unique())\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# Multiclass labels arrays\n",
    "s_cal = Triana_data_Cal_lbl[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Triana_data_Cal_lbl.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Triana_data_Test[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Triana_data_Test.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Triana_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Triana_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Triana_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Triana/Consensus_annotation_broad_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Triana_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Triana_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Triana/Consensus_annotation_broad_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Triana_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Triana_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Triana_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Triana_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Triana\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal, y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te,    index=test_index, columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc, index=test_index, columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Triana_data_Test[\"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te.argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc.argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS):\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation\n",
    "y_pred_mc = P_te_mc.argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Triana_data_Train, Triana_data_Test, Triana_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Triana_dataset_Train, Triana_dataset_Test, Triana_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Simplified\"   # \"simplified\" | \"Simplified\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Triana_data_Train = attach_celltype(Triana_data_Train, Triana_dataset_Train, consensus_field)\n",
    "Triana_data_Test  = attach_celltype(Triana_data_Test,  Triana_dataset_Test,  consensus_field)\n",
    "Triana_data_Cal   = attach_celltype(Triana_data_Cal,   Triana_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Triana_data_Train = _rename_data_to_panel(Triana_data_Train)\n",
    "Triana_data_Test  = _rename_data_to_panel(Triana_data_Test)\n",
    "Triana_data_Cal   = _rename_data_to_panel(Triana_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Triana_data_Train = _panel_intersection(Triana_data_Train)\n",
    "Triana_data_Test  = _panel_intersection(Triana_data_Test)\n",
    "Triana_data_Cal   = _panel_intersection(Triana_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Triana_data_Cal_lbl = Triana_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Cal.columns]\n",
    "\n",
    "Triana_data_Train_Sub = Triana_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Triana_data_Test_Sub  = Triana_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Triana_data_Cal_Sub   = Triana_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Triana_data_Train_Sub.columns)\n",
    "if list(Triana_data_Test_Sub.columns) != cols_train or list(Triana_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Triana_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Triana_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Triana_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# ===== Exclude specific classes from the multiclass set and per-class loop =====\n",
    "EXCLUDE_CLASSES = {\"Macrophage\", \"ILC\", \"Stroma\"}\n",
    "\n",
    "all_classes = sorted(pd.Series(Triana_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if c not in EXCLUDE_CLASSES]\n",
    "if not class_names:\n",
    "    raise ValueError(\"After exclusions, class_names is empty.\")\n",
    "print(f\"[classes] Included ({len(class_names)}): {class_names}\")\n",
    "if missing := [c for c in all_classes if c in EXCLUDE_CLASSES]:\n",
    "    print(f\"[classes] Excluded: {missing}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# --- Multiclass labels (MASKED to included classes) ---\n",
    "# CAL\n",
    "mask_cal_mc = Triana_data_Cal_lbl[\"Celltype\"].isin(class_names)\n",
    "s_cal = Triana_data_Cal_lbl.loc[mask_cal_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Triana_data_Cal_lbl.loc[mask_cal_mc & s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "# TEST\n",
    "mask_test_mc = Triana_data_Test[\"Celltype\"].isin(class_names)\n",
    "s_te = Triana_data_Test.loc[mask_test_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Triana_data_Test.loc[mask_test_mc & s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Triana_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Triana_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats (only for included classes)\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Triana_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Triana/Consensus_annotation_simplified_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Triana_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Triana_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Triana/Consensus_annotation_simplified_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Triana_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Triana_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Triana_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Triana_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing  - labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Triana\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration (columns order = class_names)\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split (excluded classes masked out)...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "# Fit only on CAL rows whose true label is one of the included classes\n",
    "ts_cal.fit(P_cal[mask_cal_mc.values, :], y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + INCLUDED class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "# Evaluate & save on TEST rows whose true label is an INCLUDED class\n",
    "test_index_masked = Triana_data_Test_Sub.index[mask_test_mc.values]\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te[mask_test_mc.values, :],    index=test_index_masked,\n",
    "                            columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc[mask_test_mc.values, :], index=test_index_masked,\n",
    "                            columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Triana_data_Test.loc[mask_test_mc, \"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS) [included classes only]:\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST_included.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation on the masked subset\n",
    "y_pred_mc = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST, excluded classes removed):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc_context\n",
    "\n",
    "# --------- 1) Collect training barcodes (union across classes or a single class) ---------\n",
    "def collect_training_barcodes(train_barcodes_path: str | Path,\n",
    "                              atlas: str = \"Triana\",\n",
    "                              depth: str = \"simplified\",\n",
    "                              class_name: str | None = None) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Reads CSVs like:\n",
    "      {train_barcodes_path}/{atlas}/Consensus_annotation_{depth}_final/Barcodes_training_class_{Class}.csv\n",
    "    and returns the union of 'Positive' and 'Negative' barcodes.\n",
    "    \"\"\"\n",
    "    base = Path(train_barcodes_path) / atlas / f\"Consensus_annotation_{depth.lower()}_final\"\n",
    "\n",
    "    if class_name is None:\n",
    "        files = sorted(base.glob(\"Barcodes_training_class_*.csv\"))\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No training CSVs found in: {base}\")\n",
    "    else:\n",
    "        files = [base / f\"Barcodes_training_class_{str(class_name).replace(' ', '_')}.csv\"]\n",
    "        if not files[0].exists():\n",
    "            raise FileNotFoundError(f\"Training CSV not found: {files[0]}\")\n",
    "\n",
    "    barcodes = []\n",
    "    for fp in files:\n",
    "        df = pd.read_csv(fp, index_col=0)\n",
    "        for col in (\"Positive\", \"Negative\"):\n",
    "            if col in df.columns:\n",
    "                barcodes.extend(df[col].dropna().astype(str).tolist())\n",
    "\n",
    "    return pd.Index(pd.unique(barcodes))\n",
    "\n",
    "\n",
    "# --------- 2) Plot UMAP for the subset of training barcodes ---------\n",
    "def plot_training_subset_umap(\n",
    "    adata,                                 # Triana_dataset_Train (AnnData)\n",
    "    train_barcodes_path: str | Path,\n",
    "    *,\n",
    "    atlas: str = \"Triana\",\n",
    "    depth: str = \"simplified\",             # matches your CSV folder name\n",
    "    class_name: str | None = None,         # None -> all classes; or \"B Memory\", etc.\n",
    "    label_key: str = \"Consensus_annotation_detailed_final\",\n",
    "    basis_key: str = \"X_mofaumap\",\n",
    "    custom_palette: dict | None = None,    # your dict {label: \"#hex\"}\n",
    "    point_size: float = 10.0,\n",
    "    title: str | None = None,\n",
    "):\n",
    "    # 1) get training barcodes\n",
    "    keep_barcodes = collect_training_barcodes(train_barcodes_path, atlas=atlas, depth=depth, class_name=class_name)\n",
    "\n",
    "    # 2) subset AnnData\n",
    "    n_before = adata.n_obs\n",
    "    mask = adata.obs_names.isin(keep_barcodes)\n",
    "    n_after = int(mask.sum())\n",
    "    if n_after == 0:\n",
    "        raise ValueError(\"No training barcodes overlapped with adata.obs_names.\")\n",
    "    ad_sub = adata[mask].copy()\n",
    "    print(f\"[subset] kept {n_after}/{n_before} cells for plotting \"\n",
    "          f\"({'ALL classes' if class_name is None else f'class={class_name}'})\")\n",
    "\n",
    "    # 3) ensure categorical for coloring\n",
    "    if not pd.api.types.is_categorical_dtype(ad_sub.obs[label_key]):\n",
    "        ad_sub.obs[label_key] = ad_sub.obs[label_key].astype(\"category\")\n",
    "    cats = list(ad_sub.obs[label_key].cat.categories)\n",
    "\n",
    "    # 4) palette handling (use your custom palette where available; fallback to grey)\n",
    "    fallback = \"#cccccc\"\n",
    "    if custom_palette is None:\n",
    "        palette_list = None  # let scanpy pick\n",
    "    else:\n",
    "        labels_in_palette = set(custom_palette.keys())\n",
    "        labels_in_data = set(cats)\n",
    "        missing_in_palette = [c for c in cats if c not in labels_in_palette]\n",
    "        extra_in_palette   = [c for c in custom_palette.keys() if c not in labels_in_data]\n",
    "        if missing_in_palette:\n",
    "            print(\"[WARN] Missing colors for:\", missing_in_palette, \"-> using light grey (#cccccc).\")\n",
    "        if extra_in_palette:\n",
    "            print(\"[INFO] Palette has unused entries:\", extra_in_palette)\n",
    "\n",
    "        palette_list = [custom_palette.get(c, fallback) for c in cats]\n",
    "        # stash into .uns so scanpy reuses it\n",
    "        ad_sub.uns[f\"{label_key}_colors\"] = palette_list\n",
    "\n",
    "    # 5) plot\n",
    "    with rc_context({\"figure.figsize\": (5.75, 4.75)}):\n",
    "        sc.pl.embedding(\n",
    "            ad_sub,\n",
    "            basis=basis_key,\n",
    "            color=label_key,\n",
    "            palette=palette_list,\n",
    "            legend_loc=\"on data\",\n",
    "            legend_fontsize=10,\n",
    "            legend_fontoutline=1.5,\n",
    "            size=point_size,\n",
    "            add_outline=True,\n",
    "            frameon=True,\n",
    "            title=title or f\"Triana â€” training subset ({'all classes' if class_name is None else class_name})\",\n",
    "            show=True,\n",
    "        )\n",
    "\n",
    "\n",
    "# ---------------------- EXAMPLES ----------------------\n",
    "# 1) Plot ALL training barcodes (union of every class) with your detailed labels + palette\n",
    "# plot_training_subset_umap(\n",
    "#     Triana_dataset_Train,\n",
    "#     train_barcodes_path=train_barcodes_path,\n",
    "#     atlas=\"Triana\",\n",
    "#     depth=\"simplified\",\n",
    "#     class_name=None,\n",
    "#     label_key=\"Consensus_annotation_detailed_final\",\n",
    "#     basis_key=\"X_mofaumap\",\n",
    "#     custom_palette=custom_palette,  # pass the dict you defined\n",
    "#     point_size=10,\n",
    "#     title=\"Triana â€” all training cells\",\n",
    "# )\n",
    "\n",
    "# 2) Plot only one classâ€™s training barcodes (e.g., \"B Memory\")\n",
    "plot_training_subset_umap(\n",
    "    Triana_dataset_Train,\n",
    "    train_barcodes_path=train_barcodes_path,\n",
    "    atlas=\"Triana\",\n",
    "    depth=\"simplified\",\n",
    "    class_name=\"Erythroid\",\n",
    "    label_key=\"Consensus_annotation_detailed_final\",\n",
    "    basis_key=\"X_mofaumap\",\n",
    "    custom_palette=custom_palette,\n",
    "    point_size=10,\n",
    "    title=\"Triana â€” training cells: B Memory\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Triana_data_Train, Triana_data_Test, Triana_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Triana_dataset_Train, Triana_dataset_Test, Triana_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Simplified\"   # \"simplified\" | \"Simplified\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Triana_data_Train = attach_celltype(Triana_data_Train, Triana_dataset_Train, consensus_field)\n",
    "Triana_data_Test  = attach_celltype(Triana_data_Test,  Triana_dataset_Test,  consensus_field)\n",
    "Triana_data_Cal   = attach_celltype(Triana_data_Cal,   Triana_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Triana_data_Train = _rename_data_to_panel(Triana_data_Train)\n",
    "Triana_data_Test  = _rename_data_to_panel(Triana_data_Test)\n",
    "Triana_data_Cal   = _rename_data_to_panel(Triana_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Triana_data_Train = _panel_intersection(Triana_data_Train)\n",
    "Triana_data_Test  = _panel_intersection(Triana_data_Test)\n",
    "Triana_data_Cal   = _panel_intersection(Triana_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Triana_data_Cal_lbl = Triana_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Cal.columns]\n",
    "\n",
    "Triana_data_Train_Sub = Triana_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Triana_data_Test_Sub  = Triana_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Triana_data_Cal_Sub   = Triana_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Triana_data_Train_Sub.columns)\n",
    "if list(Triana_data_Test_Sub.columns) != cols_train or list(Triana_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Triana_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Triana_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Triana_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# Consistent class order\n",
    "class_names  = sorted(pd.Series(Triana_data_Train[\"Celltype\"]).dropna().unique())\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# Multiclass labels arrays\n",
    "s_cal = Triana_data_Cal_lbl[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Triana_data_Cal_lbl.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Triana_data_Test[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Triana_data_Test.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Triana_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Triana_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Triana_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Triana/Consensus_annotation_simplified_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Triana_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Triana_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Triana/Consensus_annotation_simplified_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Triana_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Triana_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Triana_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Triana_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Triana\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal, y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te,    index=test_index, columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc, index=test_index, columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Triana_data_Test[\"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te.argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc.argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS):\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation\n",
    "y_pred_mc = P_te_mc.argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Triana_data_Train, Triana_data_Test, Triana_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Triana_dataset_Train, Triana_dataset_Test, Triana_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Detailed\"   # \"detailed\" | \"detailed\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Triana_data_Train = attach_celltype(Triana_data_Train, Triana_dataset_Train, consensus_field)\n",
    "Triana_data_Test  = attach_celltype(Triana_data_Test,  Triana_dataset_Test,  consensus_field)\n",
    "Triana_data_Cal   = attach_celltype(Triana_data_Cal,   Triana_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Triana_data_Train = _rename_data_to_panel(Triana_data_Train)\n",
    "Triana_data_Test  = _rename_data_to_panel(Triana_data_Test)\n",
    "Triana_data_Cal   = _rename_data_to_panel(Triana_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Triana_data_Train = _panel_intersection(Triana_data_Train)\n",
    "Triana_data_Test  = _panel_intersection(Triana_data_Test)\n",
    "Triana_data_Cal   = _panel_intersection(Triana_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Triana_data_Cal_lbl = Triana_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Triana_data_Cal.columns]\n",
    "\n",
    "Triana_data_Train_Sub = Triana_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Triana_data_Test_Sub  = Triana_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Triana_data_Cal_Sub   = Triana_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Triana_data_Train_Sub.columns)\n",
    "if list(Triana_data_Test_Sub.columns) != cols_train or list(Triana_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Triana_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Triana_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Triana_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# ===== Exclude specific classes from the multiclass set and per-class loop =====\n",
    "EXCLUDE_CLASSES = {\"Macrophage\", \"ILC\", \"Stroma\", \"dnT\"}\n",
    "\n",
    "all_classes = sorted(pd.Series(Triana_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if c not in EXCLUDE_CLASSES]\n",
    "if not class_names:\n",
    "    raise ValueError(\"After exclusions, class_names is empty.\")\n",
    "print(f\"[classes] Included ({len(class_names)}): {class_names}\")\n",
    "if missing := [c for c in all_classes if c in EXCLUDE_CLASSES]:\n",
    "    print(f\"[classes] Excluded: {missing}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# --- Multiclass labels (MASKED to included classes) ---\n",
    "# CAL\n",
    "mask_cal_mc = Triana_data_Cal_lbl[\"Celltype\"].isin(class_names)\n",
    "s_cal = Triana_data_Cal_lbl.loc[mask_cal_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Triana_data_Cal_lbl.loc[mask_cal_mc & s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "# TEST\n",
    "mask_test_mc = Triana_data_Test[\"Celltype\"].isin(class_names)\n",
    "s_te = Triana_data_Test.loc[mask_test_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Triana_data_Test.loc[mask_test_mc & s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Triana_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Triana_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats (only for included classes)\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Triana_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Triana/Consensus_annotation_detailed_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Triana_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Triana_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Triana/Consensus_annotation_detailed_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Triana_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Triana_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Triana_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Triana_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing  - labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Triana\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration (columns order = class_names)\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split (excluded classes masked out)...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "# Fit only on CAL rows whose true label is one of the included classes\n",
    "ts_cal.fit(P_cal[mask_cal_mc.values, :], y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + INCLUDED class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "# Evaluate & save on TEST rows whose true label is an INCLUDED class\n",
    "test_index_masked = Triana_data_Test_Sub.index[mask_test_mc.values]\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te[mask_test_mc.values, :],    index=test_index_masked,\n",
    "                            columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc[mask_test_mc.values, :], index=test_index_masked,\n",
    "                            columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Triana_data_Test.loc[mask_test_mc, \"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS) [included classes only]:\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST_included.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation on the masked subset\n",
    "y_pred_mc = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST, excluded classes removed):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luecken Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "os.makedirs(data_path + \"/Luecken\", exist_ok=True)\n",
    "os.makedirs(data_path + \"/Luecken/Models\", exist_ok=True)\n",
    "\n",
    "models_output = data_path + \"/Luecken/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luecken_Models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broad annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Luecken_data_Train, Luecken_data_Test, Luecken_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Luecken_dataset_Train, Luecken_dataset_Test, Luecken_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Broad\"   # \"Broad\" | \"Simplified\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Luecken_data_Train = attach_celltype(Luecken_data_Train, Luecken_dataset_Train, consensus_field)\n",
    "Luecken_data_Test  = attach_celltype(Luecken_data_Test,  Luecken_dataset_Test,  consensus_field)\n",
    "Luecken_data_Cal   = attach_celltype(Luecken_data_Cal,   Luecken_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Luecken_data_Train = _rename_data_to_panel(Luecken_data_Train)\n",
    "Luecken_data_Test  = _rename_data_to_panel(Luecken_data_Test)\n",
    "Luecken_data_Cal   = _rename_data_to_panel(Luecken_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Luecken_data_Train = _panel_intersection(Luecken_data_Train)\n",
    "Luecken_data_Test  = _panel_intersection(Luecken_data_Test)\n",
    "Luecken_data_Cal   = _panel_intersection(Luecken_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Luecken_data_Cal_lbl = Luecken_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Cal.columns]\n",
    "\n",
    "Luecken_data_Train_Sub = Luecken_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Luecken_data_Test_Sub  = Luecken_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Luecken_data_Cal_Sub   = Luecken_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Luecken_data_Train_Sub.columns)\n",
    "if list(Luecken_data_Test_Sub.columns) != cols_train or list(Luecken_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Luecken_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Luecken_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Luecken_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# Consistent class order\n",
    "class_names  = sorted(pd.Series(Luecken_data_Train[\"Celltype\"]).dropna().unique())\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# Multiclass labels arrays\n",
    "s_cal = Luecken_data_Cal_lbl[\"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Luecken_data_Cal_lbl.loc[s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "s_te = Luecken_data_Test[\"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Luecken_data_Test.loc[s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Luecken_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Luecken_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Luecken_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Luecken/Consensus_annotation_broad_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Luecken_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Luecken_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Luecken/Consensus_annotation_broad_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Luecken_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Luecken_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Luecken_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Luecken_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Luecken\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "ts_cal.fit(P_cal, y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te,    index=test_index, columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc, index=test_index, columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Luecken_data_Test[\"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te.argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc.argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS):\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation\n",
    "y_pred_mc = P_te_mc.argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Luecken_data_Train, Luecken_data_Test, Luecken_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Luecken_dataset_Train, Luecken_dataset_Test, Luecken_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Simplified\"   # \"simplified\" | \"Simplified\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Luecken_data_Train = attach_celltype(Luecken_data_Train, Luecken_dataset_Train, consensus_field)\n",
    "Luecken_data_Test  = attach_celltype(Luecken_data_Test,  Luecken_dataset_Test,  consensus_field)\n",
    "Luecken_data_Cal   = attach_celltype(Luecken_data_Cal,   Luecken_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Luecken_data_Train = _rename_data_to_panel(Luecken_data_Train)\n",
    "Luecken_data_Test  = _rename_data_to_panel(Luecken_data_Test)\n",
    "Luecken_data_Cal   = _rename_data_to_panel(Luecken_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Luecken_data_Train = _panel_intersection(Luecken_data_Train)\n",
    "Luecken_data_Test  = _panel_intersection(Luecken_data_Test)\n",
    "Luecken_data_Cal   = _panel_intersection(Luecken_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Luecken_data_Cal_lbl = Luecken_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Cal.columns]\n",
    "\n",
    "Luecken_data_Train_Sub = Luecken_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Luecken_data_Test_Sub  = Luecken_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Luecken_data_Cal_Sub   = Luecken_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Luecken_data_Train_Sub.columns)\n",
    "if list(Luecken_data_Test_Sub.columns) != cols_train or list(Luecken_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Luecken_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Luecken_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Luecken_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# ===== Exclude specific classes from the multiclass set and per-class loop =====\n",
    "EXCLUDE_CLASSES = {\"Macrophage\", \"ILC\", \"Stroma\"}\n",
    "\n",
    "all_classes = sorted(pd.Series(Luecken_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if c not in EXCLUDE_CLASSES]\n",
    "if not class_names:\n",
    "    raise ValueError(\"After exclusions, class_names is empty.\")\n",
    "print(f\"[classes] Included ({len(class_names)}): {class_names}\")\n",
    "if missing := [c for c in all_classes if c in EXCLUDE_CLASSES]:\n",
    "    print(f\"[classes] Excluded: {missing}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# --- Multiclass labels (MASKED to included classes) ---\n",
    "# CAL\n",
    "mask_cal_mc = Luecken_data_Cal_lbl[\"Celltype\"].isin(class_names)\n",
    "s_cal = Luecken_data_Cal_lbl.loc[mask_cal_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Luecken_data_Cal_lbl.loc[mask_cal_mc & s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "# TEST\n",
    "mask_test_mc = Luecken_data_Test[\"Celltype\"].isin(class_names)\n",
    "s_te = Luecken_data_Test.loc[mask_test_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Luecken_data_Test.loc[mask_test_mc & s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Luecken_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Luecken_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats (only for included classes)\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Luecken_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Luecken/Consensus_annotation_simplified_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Luecken_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Luecken_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Luecken/Consensus_annotation_simplified_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Luecken_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Luecken_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Luecken_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Luecken_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing  - labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Luecken\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration (columns order = class_names)\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split (excluded classes masked out)...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "# Fit only on CAL rows whose true label is one of the included classes\n",
    "ts_cal.fit(P_cal[mask_cal_mc.values, :], y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + INCLUDED class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "# Evaluate & save on TEST rows whose true label is an INCLUDED class\n",
    "test_index_masked = Luecken_data_Test_Sub.index[mask_test_mc.values]\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te[mask_test_mc.values, :],    index=test_index_masked,\n",
    "                            columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc[mask_test_mc.values, :], index=test_index_masked,\n",
    "                            columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Luecken_data_Test.loc[mask_test_mc, \"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS) [included classes only]:\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST_included.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation on the masked subset\n",
    "y_pred_mc = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST, excluded classes removed):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------------- CONFIG (expects these to already exist)\n",
    "#   models_output, train_barcodes_path, test_barcodes_path\n",
    "#   Luecken_data_Train, Luecken_data_Test, Luecken_data_Cal          (DataFrames indexed by barcode)\n",
    "#   Luecken_dataset_Train, Luecken_dataset_Test, Luecken_dataset_Cal (AnnData with obs labels)\n",
    "#   TotalSeqD_Heme_Oncology_CAT399906                    (iterable of feature names)\n",
    "#   MLTraining module with: CV, train_NB, train_XGB, train_KNN, train_MLP,\n",
    "#                           plot_calibration_curve, save_models, evaluate_classifier, append_metrics_csv\n",
    "\n",
    "name_target_class = \"Detailed\"   # \"detailed\" | \"detailed\" | \"Detailed\"\n",
    "fig_root   = Path(models_output) / \"Figures\"\n",
    "models_dir = Path(models_output) / \"Models\"\n",
    "fig_root.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kf         = MLTraining.CV\n",
    "num_cores  = -1\n",
    "metrics_log = []\n",
    "\n",
    "# ============================= HELPERS =============================\n",
    "\n",
    "def _norm_feats(names) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Normalizer used ONLY to construct matching keys.\n",
    "    Panel names remain untouched; data columns are normalized and then mapped BACK\n",
    "    to the exact panel names via a lookup.\n",
    "    \"\"\"\n",
    "    s = pd.Index(map(str, names))\n",
    "    s = (s.str.strip()\n",
    "           .str.lower()\n",
    "           .str.replace(r\"[ _/]+\", \"-\", regex=True)\n",
    "           .str.replace(r\"-+\", \"-\", regex=True)\n",
    "           .str.strip(\"-\"))\n",
    "    return s\n",
    "\n",
    "def attach_celltype(df: pd.DataFrame, ad: \"AnnData\", field: str) -> pd.DataFrame:\n",
    "    if field not in ad.obs:\n",
    "        raise KeyError(f\"'{field}' not found in AnnData.obs\")\n",
    "    lab = (ad.obs[field]\n",
    "             .astype(\"string\")\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    out = df.copy()\n",
    "    out[\"Celltype\"] = pd.Categorical(lab.reindex(out.index))\n",
    "    if out[\"Celltype\"].isna().any():\n",
    "        missing = int(out[\"Celltype\"].isna().sum())\n",
    "        print(f\"[WARN] {missing} rows got NaN Celltype after reindex; check barcode alignment.\")\n",
    "    return out\n",
    "\n",
    "def _check_finite(df: pd.DataFrame, tag: str):\n",
    "    arr = df.to_numpy()\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = np.where(~np.isfinite(arr))\n",
    "        raise ValueError(f\"Non-finite values found in {tag} features at positions {bad}\")\n",
    "\n",
    "def _unwrap_estimator(m):\n",
    "    return getattr(m, \"estimator\", None) or getattr(m, \"base_estimator\", None) or m\n",
    "\n",
    "def _assert_feature_counts(cell_name: str, models_dict: dict, expected: int):\n",
    "    pairs = [\n",
    "        (\"NB\",  models_dict.get(\"NB\")),\n",
    "        (\"XGB\", models_dict.get(\"XGB\")),\n",
    "        (\"KNN\", models_dict.get(\"KNN\")),\n",
    "        (\"MLP\", models_dict.get(\"MLP\")),\n",
    "        (\"Stacker\", models_dict.get(\"Stacker\")),\n",
    "    ]\n",
    "    for name, est in pairs:\n",
    "        if est is None:\n",
    "            continue\n",
    "        base = _unwrap_estimator(est)\n",
    "        nfi = getattr(base, \"n_features_in_\", None)\n",
    "        if nfi is not None and nfi != expected:\n",
    "            raise RuntimeError(f\"{cell_name}:{name} saw {nfi} features; expected {expected}\")\n",
    "\n",
    "# ============================= LABEL ATTACH =============================\n",
    "\n",
    "consensus_field = f\"Consensus_annotation_{name_target_class.lower()}_final\"\n",
    "\n",
    "Luecken_data_Train = attach_celltype(Luecken_data_Train, Luecken_dataset_Train, consensus_field)\n",
    "Luecken_data_Test  = attach_celltype(Luecken_data_Test,  Luecken_dataset_Test,  consensus_field)\n",
    "Luecken_data_Cal   = attach_celltype(Luecken_data_Cal,   Luecken_dataset_Cal,   consensus_field)\n",
    "\n",
    "# ============================= PANEL & DATA COLUMN ALIGNMENT =============================\n",
    "\n",
    "# Keep the panel EXACTLY as provided\n",
    "panel = pd.Index(map(str, TotalSeqD_Heme_Oncology_CAT399906))\n",
    "\n",
    "# Build a mapping: normalized_key -> exact panel name\n",
    "panel_keys    = _norm_feats(panel)\n",
    "norm_to_panel = dict(zip(panel_keys, panel))\n",
    "if len(norm_to_panel) != len(panel):\n",
    "    raise ValueError(\"Panel contains names that collide after normalization. Consider adjusting _norm_feats rules.\")\n",
    "\n",
    "def _rename_data_to_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename only feature columns so that after normalization they map\n",
    "    back to the exact panel column names. Keeps 'cell_barcode' and 'Celltype' intact.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat     = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "\n",
    "    feat_keys   = _norm_feats(feat)\n",
    "    mapped      = [norm_to_panel.get(k) for k in feat_keys]  # None if not in panel\n",
    "    rename_map  = {old: new for old, new in zip(feat, mapped) if new is not None}\n",
    "\n",
    "    # Handle duplicate mappings (two data columns â†’ same panel col). Keep first, drop the rest.\n",
    "    seen, safe_map, drops = set(), {}, []\n",
    "    for old, new in rename_map.items():\n",
    "        if new in seen:\n",
    "            drops.append(old)\n",
    "        else:\n",
    "            seen.add(new); safe_map[old] = new\n",
    "\n",
    "    if drops:\n",
    "        print(f\"[WARN] Dropping {len(drops)} duplicated-mapped columns (showing up to 5): {drops[:5]}\")\n",
    "\n",
    "    if drops:\n",
    "        df.drop(columns=drops, inplace=True, errors=\"ignore\")\n",
    "    df.rename(columns=safe_map, inplace=True)\n",
    "\n",
    "    matched = len(safe_map)\n",
    "    print(f\"[map] matched {matched}/{len(feat)} data columns to panel\")\n",
    "    return df\n",
    "\n",
    "# Apply: normalize/rename ONLY data splits (panel remains untouched)\n",
    "Luecken_data_Train = _rename_data_to_panel(Luecken_data_Train)\n",
    "Luecken_data_Test  = _rename_data_to_panel(Luecken_data_Test)\n",
    "Luecken_data_Cal   = _rename_data_to_panel(Luecken_data_Cal)\n",
    "\n",
    "# Intersect each split with the panel IN PANEL ORDER\n",
    "def _panel_intersection(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_feat = [c for c in [\"cell_barcode\", \"Celltype\"] if c in df.columns]\n",
    "    feat_cols = pd.Index([c for c in df.columns if c not in non_feat])\n",
    "    inter = panel.intersection(feat_cols, sort=False)\n",
    "    if inter.empty:\n",
    "        raise ValueError(\"Panel/Data intersection is empty after renaming. Check mapping rules.\")\n",
    "    return df.reindex(columns=list(inter) + non_feat)\n",
    "\n",
    "Luecken_data_Train = _panel_intersection(Luecken_data_Train)\n",
    "Luecken_data_Test  = _panel_intersection(Luecken_data_Test)\n",
    "Luecken_data_Cal   = _panel_intersection(Luecken_data_Cal)\n",
    "\n",
    "# ============================= FEATURES & LABELS =============================\n",
    "\n",
    "Luecken_data_Cal_lbl = Luecken_data_Cal[[\"Celltype\"]].copy()\n",
    "\n",
    "drop_cols_train = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Train.columns]\n",
    "drop_cols_test  = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Test.columns]\n",
    "drop_cols_cal   = [c for c in [\"cell_barcode\", \"Celltype\"] if c in Luecken_data_Cal.columns]\n",
    "\n",
    "Luecken_data_Train_Sub = Luecken_data_Train.drop(columns=drop_cols_train, errors=\"ignore\")\n",
    "Luecken_data_Test_Sub  = Luecken_data_Test.drop(columns=drop_cols_test,  errors=\"ignore\")\n",
    "Luecken_data_Cal_Sub   = Luecken_data_Cal.drop(columns=drop_cols_cal,    errors=\"ignore\")\n",
    "\n",
    "# SAFETY: shared columns & finiteness checks\n",
    "cols_train = list(Luecken_data_Train_Sub.columns)\n",
    "if list(Luecken_data_Test_Sub.columns) != cols_train or list(Luecken_data_Cal_Sub.columns) != cols_train:\n",
    "    raise ValueError(\"Train/Cal/Test feature columns differ after panel intersection!\")\n",
    "\n",
    "_check_finite(Luecken_data_Train_Sub, \"TRAIN\")\n",
    "_check_finite(Luecken_data_Test_Sub,  \"TEST\")\n",
    "_check_finite(Luecken_data_Cal_Sub,   \"CAL\")\n",
    "\n",
    "print(f\"\\n[features] Using {len(cols_train)} panel-intersected features (exact panel names):\")\n",
    "print(cols_train)\n",
    "\n",
    "# ===== Exclude specific classes from the multiclass set and per-class loop =====\n",
    "EXCLUDE_CLASSES = {\"Macrophage\", \"ILC\", \"Stroma\", \"dnT\"}\n",
    "\n",
    "all_classes = sorted(pd.Series(Luecken_data_Train[\"Celltype\"]).dropna().unique())\n",
    "class_names = [c for c in all_classes if c not in EXCLUDE_CLASSES]\n",
    "if not class_names:\n",
    "    raise ValueError(\"After exclusions, class_names is empty.\")\n",
    "print(f\"[classes] Included ({len(class_names)}): {class_names}\")\n",
    "if missing := [c for c in all_classes if c in EXCLUDE_CLASSES]:\n",
    "    print(f\"[classes] Excluded: {missing}\")\n",
    "\n",
    "K            = len(class_names)\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# --- Multiclass labels (MASKED to included classes) ---\n",
    "# CAL\n",
    "mask_cal_mc = Luecken_data_Cal_lbl[\"Celltype\"].isin(class_names)\n",
    "s_cal = Luecken_data_Cal_lbl.loc[mask_cal_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_cal.isna().any():\n",
    "    missing = Luecken_data_Cal_lbl.loc[mask_cal_mc & s_cal.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked CAL split: {missing}\")\n",
    "y_cal_multiclass = s_cal.to_numpy(dtype=np.int64)\n",
    "\n",
    "# TEST\n",
    "mask_test_mc = Luecken_data_Test[\"Celltype\"].isin(class_names)\n",
    "s_te = Luecken_data_Test.loc[mask_test_mc, \"Celltype\"].map(class_to_idx)\n",
    "if s_te.isna().any():\n",
    "    missing = Luecken_data_Test.loc[mask_test_mc & s_te.isna(), \"Celltype\"].unique()\n",
    "    raise ValueError(f\"Unknown labels in masked TEST split: {missing}\")\n",
    "y_test_multiclass = s_te.to_numpy(dtype=np.int64)\n",
    "\n",
    "# Reuse across classes\n",
    "X_cal_all_df = Luecken_data_Cal_Sub.copy()\n",
    "X_te_all_df  = Luecken_data_Test_Sub.copy()\n",
    "\n",
    "# Preallocate OvR prob mats (only for included classes)\n",
    "P_cal = np.zeros((X_cal_all_df.shape[0], K), dtype=float)\n",
    "P_te  = np.zeros((X_te_all_df.shape[0],  K), dtype=float)\n",
    "\n",
    "test_index = Luecken_data_Test_Sub.index\n",
    "\n",
    "# ============================= TRAIN PER-CLASS OVR =============================\n",
    "\n",
    "for celltype in class_names:\n",
    "    k = class_to_idx[celltype]\n",
    "    name = str(celltype).replace(\" \", \"_\")\n",
    "    print(f\"\\nProcessing {name} (class {k+1}/{K})...\")\n",
    "\n",
    "    # ---- TRAIN slice via barcode lists\n",
    "    train_barcodes_df = pd.read_csv(\n",
    "        f\"{train_barcodes_path}/Luecken/Consensus_annotation_detailed_final/Barcodes_training_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    train_positive_barcodes = train_barcodes_df[\"Positive\"].dropna().values\n",
    "    train_negative_barcodes = train_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_train_barcodes = np.concatenate([train_positive_barcodes, train_negative_barcodes])\n",
    "\n",
    "    train_mask = Luecken_data_Train_Sub.index.isin(all_train_barcodes)\n",
    "    X_tr_df = Luecken_data_Train_Sub.loc[train_mask]\n",
    "    found_train_barcodes = X_tr_df.index.values\n",
    "    y_tr = np.isin(found_train_barcodes, train_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Skip guards\n",
    "    if X_tr_df.empty or np.unique(y_tr).size < 2:\n",
    "        print(f\"[SKIP] {name}: empty or single-class train slice (pos={y_tr.sum()}, neg={(len(y_tr)-y_tr.sum())}).\")\n",
    "        continue\n",
    "\n",
    "    # ---- TEST slice via barcode lists\n",
    "    test_barcodes_df = pd.read_csv(\n",
    "        f\"{test_barcodes_path}/Luecken/Consensus_annotation_detailed_final/Barcodes_testing_class_{name}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    test_positive_barcodes = test_barcodes_df[\"Positive\"].dropna().values\n",
    "    test_negative_barcodes = test_barcodes_df[\"Negative\"].dropna().values\n",
    "    all_test_barcodes = np.concatenate([test_positive_barcodes, test_negative_barcodes])\n",
    "\n",
    "    test_mask = Luecken_data_Test_Sub.index.isin(all_test_barcodes)\n",
    "    X_te_df = Luecken_data_Test_Sub.loc[test_mask]\n",
    "    found_test_barcodes = X_te_df.index.values\n",
    "    y_te = np.isin(found_test_barcodes, test_positive_barcodes).astype(int)\n",
    "\n",
    "    # ---- Full-test & cal for this binary head\n",
    "    X_te_all_local = X_te_all_df.copy()\n",
    "    y_te_all = (Luecken_data_Test[\"Celltype\"].values == celltype).astype(int)\n",
    "    X_cal_df = X_cal_all_df.copy()\n",
    "    y_cal_bin = (Luecken_data_Cal_lbl[\"Celltype\"].values == celltype).astype(int)\n",
    "\n",
    "    # ---- Info\n",
    "    print(f\"Training - Found {X_tr_df.shape[0]} / {len(all_train_barcodes)} barcodes\")\n",
    "    print(f\"Training - Pos: {len(train_positive_barcodes)}, Neg: {len(train_negative_barcodes)}\")\n",
    "    print(f\"Training labels: {y_tr.sum()} pos, {len(y_tr)-y_tr.sum()} neg\")\n",
    "    print(f\"Testing  - Found {X_te_df.shape[0]} / {len(all_test_barcodes)} barcodes\")\n",
    "    print(f\"Testing  - Pos: {len(test_positive_barcodes)}, Neg: {len(test_negative_barcodes)}\")\n",
    "    print(f\"Testing  - labels: {y_te.sum()} pos, {len(y_te)-y_te.sum()} neg\")\n",
    "    print(f\"Calibrating - Found {X_cal_df.shape[0]} rows | Pos: {y_cal_bin.sum()}, Neg: {len(y_cal_bin)-y_cal_bin.sum()}\")\n",
    "    print(f\"All test data: {X_te_all_local.shape[0]} rows, positives for {celltype}: {y_te_all.sum()}\")\n",
    "\n",
    "    # ---- Scaling (fit on per-head TRAIN slice; transform others)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_tr_df.values)\n",
    "\n",
    "    def _sc(df):\n",
    "        return pd.DataFrame(\n",
    "            scaler.transform(df.values),\n",
    "            index=df.index,\n",
    "            columns=cols_train,\n",
    "        )\n",
    "\n",
    "    X_tr_sc_df     = _sc(X_tr_df)\n",
    "    X_te_sc_df     = _sc(X_te_df)\n",
    "    X_te_all_sc_df = _sc(X_te_all_local)\n",
    "    X_cal_sc_df    = _sc(X_cal_df)\n",
    "\n",
    "    print(f\"[scale] {name}: train mean ~ {X_tr_sc_df.values.mean():.3f}, std ~ {X_tr_sc_df.values.std():.3f}\")\n",
    "\n",
    "    # ---- Base learners\n",
    "    NB_model  = MLTraining.train_NB (X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    XGB_model = MLTraining.train_XGB(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    KNN_model = MLTraining.train_KNN(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "    MLP_model = MLTraining.train_MLP(X_tr_sc_df, y_tr, cv=kf, num_cores=num_cores, name_target_subclass=name)\n",
    "\n",
    "    # ---- Stacker (raw)\n",
    "    stacker_raw = StackingClassifier(\n",
    "        estimators=[(\"NB\", NB_model), (\"XGB\", XGB_model), (\"KNN\", KNN_model), (\"MLP\", MLP_model)],\n",
    "        final_estimator=LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr_sc_df, y_tr)\n",
    "\n",
    "    # ---- Feature count asserts (debug safety)\n",
    "    expected_feats = len(cols_train)\n",
    "    _assert_feature_counts(name, {\n",
    "        \"NB\": NB_model, \"XGB\": XGB_model, \"KNN\": KNN_model, \"MLP\": MLP_model, \"Stacker\": stacker_raw\n",
    "    }, expected_feats)\n",
    "\n",
    "    # ---- Binary calibration (Platt, guarded)\n",
    "    pos_cal    = int(y_cal_bin.sum())\n",
    "    n_cal_bin  = int(len(y_cal_bin))\n",
    "    has_both   = (0 < pos_cal < n_cal_bin)\n",
    "    print(f\"[CAL] {name}: cal positives={pos_cal}/{n_cal_bin}\")\n",
    "\n",
    "    if has_both:\n",
    "        try:\n",
    "            calibrator = CalibratedClassifierCV(estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        except TypeError:  # older sklearn\n",
    "            calibrator = CalibratedClassifierCV(base_estimator=stacker_raw, method=\"sigmoid\", cv=\"prefit\")\n",
    "        stacker = calibrator.fit(X_cal_sc_df, y_cal_bin)\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping calibration for {name}: single-class cal set.\")\n",
    "        stacker = stacker_raw\n",
    "\n",
    "    # ---- Calibration plot on all-test (optional)\n",
    "    try:\n",
    "        y_proba_uncal = stacker_raw.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        y_proba_cal   = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "        if has_both:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal, y_proba_cal],\n",
    "                clf_names=[\"Uncalibrated\", \"Calibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        else:\n",
    "            _ = MLTraining.plot_calibration_curve(\n",
    "                y_te_all, [y_proba_uncal],\n",
    "                clf_names=[\"Uncalibrated\"],\n",
    "                n_bins=15, strategy=\"quantile\",\n",
    "                title=f\"Calibration (uncal only) â€“ {name_target_class}:{name}\"\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped calibration plot for {name}: {e}\")\n",
    "\n",
    "    # ---- Save per-class bundle (model + scaler + columns)\n",
    "    save_subdir = models_dir / f\"{name_target_class}_{name}\"\n",
    "    save_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    MLTraining.save_models({\"Stacked\": stacker}, out_dir=save_subdir, tag=f\"{name_target_class}_{name}\")\n",
    "    joblib.dump(cols_train, save_subdir / \"feature_names.joblib\")\n",
    "\n",
    "    bundle = {\n",
    "        \"atlas\": \"Luecken\",\n",
    "        \"depth\": name_target_class,\n",
    "        \"label\": celltype,\n",
    "        \"model\": stacker,          # CalibratedClassifierCV(StackingClassifier) or StackingClassifier\n",
    "        \"columns\": cols_train,     # exact panel names, panel order\n",
    "        \"scaler\": scaler,          # per-head scaler\n",
    "        \"panel_name\": \"TotalSeqD_Heme_Oncology_CAT399906\",\n",
    "    }\n",
    "    bundle_path = save_subdir / f\"{name_target_class}_{name}_bundle.joblib\"\n",
    "    joblib.dump(bundle, bundle_path)\n",
    "    print(f\"[SAVE] Wrote bundle with columns+scaler to {bundle_path}\")\n",
    "\n",
    "    # ---- Binary metrics on the class-specific test slice\n",
    "    try:\n",
    "        m = MLTraining.evaluate_classifier(stacker, X_te_sc_df, y_te, plot_cm=False)\n",
    "        m.update(celltype=celltype)\n",
    "        metrics_log.append(m)\n",
    "        print(f\"\\n{celltype}\\n\", m.get(\"report\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Binary metrics for {name} skipped: {e}\")\n",
    "\n",
    "    # ---- Store OvR probs for multiclass calibration (columns order = class_names)\n",
    "    P_cal[:, k] = stacker.predict_proba(X_cal_sc_df)[:, 1]\n",
    "    P_te[:,  k] = stacker.predict_proba(X_te_all_sc_df)[:, 1]\n",
    "\n",
    "# ============================= MULTICLASS CALIBRATION =============================\n",
    "\n",
    "print(\"\\nFitting multiclass TemperatureScaling on CAL split (excluded classes masked out)...\")\n",
    "\n",
    "# Guards: ensure probs are in [0,1]\n",
    "if (P_cal < 0).any() or (P_cal > 1).any():\n",
    "    raise ValueError(\"P_cal must be probabilities in [0,1].\")\n",
    "if (P_te < 0).any() or (P_te > 1).any():\n",
    "    raise ValueError(\"P_te must be probabilities in [0,1].\")\n",
    "\n",
    "ts_cal = TemperatureScaling()\n",
    "# Fit only on CAL rows whose true label is one of the included classes\n",
    "ts_cal.fit(P_cal[mask_cal_mc.values, :], y_cal_multiclass)\n",
    "P_te_mc = ts_cal.transform(P_te)\n",
    "\n",
    "# Ensure calibrated probs shape (K)\n",
    "P_te_mc = np.asarray(P_te_mc)\n",
    "if P_te_mc.ndim == 1:\n",
    "    P_te_mc = P_te_mc.reshape(-1, 1)\n",
    "if P_te_mc.shape[1] == 1 and K == 2:\n",
    "    P_te_mc = np.hstack([1.0 - P_te_mc, P_te_mc])\n",
    "elif P_te_mc.shape[1] != K:\n",
    "    # Fallback: normalize OvR sums\n",
    "    row_sums = P_te.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    P_te_mc = P_te / row_sums\n",
    "    print(f\"[WARN] TemperatureScaling returned shape {P_te_mc.shape}; fell back to sum-normalized OvR probs.\")\n",
    "\n",
    "# Persist multiclass temp scaler + INCLUDED class names\n",
    "joblib.dump(ts_cal, models_dir / f\"{name_target_class}_multiclass_temp_scaler.joblib\")\n",
    "(pd.Series(class_names, name=\"class_name\")\n",
    "   .to_csv(models_dir / f\"{name_target_class}_class_names.csv\", index=False))\n",
    "\n",
    "# ============================= PROBS COMPARISON & METRICS =============================\n",
    "\n",
    "# Evaluate & save on TEST rows whose true label is an INCLUDED class\n",
    "test_index_masked = Luecken_data_Test_Sub.index[mask_test_mc.values]\n",
    "\n",
    "probs_raw_df = pd.DataFrame(P_te[mask_test_mc.values, :],    index=test_index_masked,\n",
    "                            columns=[f\"raw_{c}\" for c in class_names])\n",
    "probs_mc_df  = pd.DataFrame(P_te_mc[mask_test_mc.values, :], index=test_index_masked,\n",
    "                            columns=[f\"mc_{c}\"  for c in class_names])\n",
    "\n",
    "probs_compare = pd.concat([probs_raw_df, probs_mc_df], axis=1)\n",
    "probs_compare[\"true_label\"]    = Luecken_data_Test.loc[mask_test_mc, \"Celltype\"].values\n",
    "probs_compare[\"pred_raw\"]      = P_te[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_mc\"]       = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "probs_compare[\"pred_raw_name\"] = [class_names[i] for i in probs_compare[\"pred_raw\"].values]\n",
    "probs_compare[\"pred_mc_name\"]  = [class_names[i] for i in probs_compare[\"pred_mc\"].values]\n",
    "\n",
    "print(\"\\nPreview of probabilities BEFORE (raw OvR) vs AFTER (multiclass TS) [included classes only]:\")\n",
    "print(probs_compare.head(10).to_string())\n",
    "\n",
    "probs_compare_path = models_dir / f\"{name_target_class}_probabilities_before_after_TEST_included.csv\"\n",
    "probs_compare.to_csv(probs_compare_path, index=True)\n",
    "print(f\"\\nSaved probabilities comparison to: {probs_compare_path}\")\n",
    "\n",
    "# Multiclass evaluation on the masked subset\n",
    "y_pred_mc = P_te_mc[mask_test_mc.values, :].argmax(axis=1)\n",
    "print(\"\\nMulticlass classification report (TEST, excluded classes removed):\")\n",
    "print(classification_report(y_test_multiclass, y_pred_mc, target_names=class_names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test_multiclass, y_pred_mc, labels=range(K))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Per-class binary head metrics CSV\n",
    "metrics_df = pd.DataFrame.from_records(metrics_log)\n",
    "MLTraining.append_metrics_csv(metrics_df, csv_path=Path(models_output) / \"stacker_metrics.csv\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mosaic_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
